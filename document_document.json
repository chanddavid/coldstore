[
  {
    "model": "documents.createnewdocuments",
    "pk": 14,
    "fields": {
      "created_by": 25,
      "title": "weekly report(july 15 to 26)",
      "content": "<h1><strong>Works done:</strong></h1>\r\n\r\n<div>1.&nbsp;R &amp; D on aws video hosting and application to shift videos from wistia to aws s3 and cloudfront distribution</div>\r\n\r\n<div>2.&nbsp;Review and solve the issue &ldquo;400 bad request when trying to add session in admin panel&rdquo;</div>\r\n\r\n<div>3.&nbsp;Review on changing the app flow</div>\r\n\r\n<div>4.&nbsp;Review &ldquo;journal entries disappearing in android and ios&rdquo;</div>\r\n\r\n<div>5.&nbsp;Review user stats in user profile</div>\r\n\r\n<div>6.&nbsp;Review coupon redemption url in signup(<a href=\"https://www.unplug.app/redeem\">https://www.unplug.app/redeem</a>)</div>\r\n\r\n<div>7.&nbsp;Changed policy of the bucket and tested whether it worked on application or not for thumbnails</div>\r\n\r\n<div>8.&nbsp;Solved 400 bad request due to restricted access to coupon who are not in team&nbsp;</div>\r\n\r\n<div>9. Email service now shifted to mandrill&nbsp;</div>\r\n\r\n<p>&nbsp;</p>",
      "date": "2019-07-25"
    }
  },
  {
    "model": "documents.createnewdocuments",
    "pk": 15,
    "fields": {
      "created_by": 3,
      "title": "2019-(07-15 to 07-26)",
      "content": "<p><strong>Works Done:</strong></p>\r\n\r\n<p><strong>DFS</strong></p>\r\n\r\n<ul>\r\n\t<li style=\"list-style-type:disc\">\r\n\t<p><span style=\"background-color:transparent; color:#000000\">Studied thread pool executor using callable and future.</span></p>\r\n\t</li>\r\n\t<li style=\"list-style-type:disc\">\r\n\t<p><span style=\"background-color:transparent; color:#000000\">File chunking using file channel in nio socket, saved the chunked data in a file.</span></p>\r\n\t</li>\r\n\t<li style=\"list-style-type:disc\">\r\n\t<p><span style=\"background-color:transparent; color:#000000\">Sent chunked data in multiple servers to store it using thread pool executor.</span></p>\r\n\t</li>\r\n\t<li style=\"list-style-type:disc\">\r\n\t<p><span style=\"background-color:transparent; color:#000000\">Sent filename, datasize, hashed key along with data in one request.</span></p>\r\n\t</li>\r\n\t<li style=\"list-style-type:disc\">\r\n\t<p><span style=\"background-color:transparent; color:#000000\">Sent data with no loss, tested successfully.</span></p>\r\n\t</li>\r\n\t<li style=\"list-style-type:disc\">\r\n\t<p><span style=\"background-color:transparent; color:#000000\">Sent metadata of file to server</span></p>\r\n\t</li>\r\n\t<li style=\"list-style-type:disc\">\r\n\t<p><span style=\"background-color:transparent; color:#000000\">Encapsulate parent module in child dependencies which packaging in jar.</span></p>\r\n\t</li>\r\n</ul>\r\n\r\n<p><strong>Kafka</strong></p>\r\n\r\n<ul>\r\n\t<li style=\"list-style-type:disc\">\r\n\t<p><span style=\"background-color:transparent; color:#000000\">Sinked data from postgres to elasticsearch in already created index.</span></p>\r\n\t</li>\r\n\t<li style=\"list-style-type:disc\">\r\n\t<p><span style=\"background-color:transparent; color:#000000\">Implemented key for kafka topics.</span></p>\r\n\t</li>\r\n\t<li style=\"list-style-type:disc\">\r\n\t<p><span style=\"background-color:transparent; color:#000000\">Used KSQL to create stream using JSON and AVRO format.</span></p>\r\n\t</li>\r\n</ul>",
      "date": "2019-07-25"
    }
  },
  {
    "model": "documents.createnewdocuments",
    "pk": 16,
    "fields": {
      "created_by": 13,
      "title": "2019-07-28",
      "content": "<h1><strong>1) GPS Tracking Analysis</strong></h1>\r\n\r\n<p><strong>Description</strong></p>\r\n\r\n<p>Finally, POI way of analysis was selected to determine user&#39;s building and earlier code had to be implemented.</p>\r\n\r\n<p>Good aount of past 2 weeks time was spent to find dymanic POI</p>\r\n\r\n<p><strong>Problem faced</strong>: Neighbour building and lack of building data was creating problem to find dynamic POI</p>\r\n\r\n<p><strong>Time taken</strong>; 2 weeks</p>\r\n\r\n<h1><strong>2) DFS</strong></h1>\r\n\r\n<p><strong>Description</strong></p>\r\n\r\n<p>Data chunk was checksum verified and saved to file and file metadata was sent to master</p>\r\n\r\n<p><strong>Problem faced</strong>:&nbsp;</p>\r\n\r\n<p>Due to nature od socket small complete data was not being saved</p>\r\n\r\n<p><strong>Time taken</strong>: 2 weeks</p>\r\n\r\n<h1><strong>3) SHM</strong></h1>\r\n\r\n<p><strong>Description</strong></p>\r\n\r\n<p>CPU average load and disk usage per mount point had to be calculated</p>\r\n\r\n<p><strong>Problem faced</strong>: None</p>\r\n\r\n<p><strong>Time taken:</strong> 2 days</p>",
      "date": "2019-07-25"
    }
  },
  {
    "model": "documents.createnewdocuments",
    "pk": 17,
    "fields": {
      "created_by": 23,
      "title": "2019 -(07-22 to 07-26)",
      "content": "<p><strong>Works&nbsp;Done:</strong></p>\r\n\r\n<p><strong>Hadoop:</strong></p>\r\n\r\n<p>- Studied Hadoop and its components</p>\r\n\r\n<p>- Single Node installation (Stand alone mode)</p>\r\n\r\n<p>- Studied hadoop commands and implemented them to transfer files between local file system and hdfs</p>\r\n\r\n<p>- Setup passwordless SSH connection using RSA key</p>\r\n\r\n<p>- Multi Node (2-node) installation in Pseudo-Mode</p>\r\n\r\n<p>- Added one more Data node in the multi-node cluster</p>\r\n\r\n<p>&nbsp;</p>",
      "date": "2019-07-25"
    }
  },
  {
    "model": "documents.createnewdocuments",
    "pk": 18,
    "fields": {
      "created_by": 15,
      "title": "Weekly_Report-July-09-2019",
      "content": "<h1><strong>Tasks:</strong></h1>\r\n\r\n<p>Anniversary Data Generation and Visualization</p>\r\n\r\n<p>Presentation</p>\r\n\r\n<p>Promo product choice for different categories of customers.</p>\r\n\r\n<p>2-year growth and loss in each store.</p>\r\n\r\n<p>Churn Prediction</p>\r\n\r\n<p>&nbsp;</p>",
      "date": "2019-07-25"
    }
  },
  {
    "model": "documents.createnewdocuments",
    "pk": 20,
    "fields": {
      "created_by": 3,
      "title": "2019-(07-29 to 08-09)",
      "content": "<p><strong>Works Done:</strong></p>\r\n\r\n<p><strong>Kafka</strong></p>\r\n\r\n<ul>\r\n\t<li>\r\n\t<p>Code modification of Phoenix connector provided by confluent</p>\r\n\t</li>\r\n\t<li>\r\n\t<p>Implementation of source connector for oracle using oracle-11g</p>\r\n\t</li>\r\n\t<li>\r\n\t<p>Implementation of source connector for oracle using oracle-12c</p>\r\n\t</li>\r\n\t<li>\r\n\t<p>Using JDBC for phoenix sink connector.</p>\r\n\t</li>\r\n\t<li>\r\n\t<p>Added Schema registry in kafka cluster</p>\r\n\t</li>\r\n\t<li>\r\n\t<p>Used schema registry to add schema for a topic</p>\r\n\t</li>\r\n\t<li>\r\n\t<p>Implement schema registry for streams in kafka</p>\r\n\t</li>\r\n</ul>\r\n\r\n<p><strong>Unplug</strong></p>\r\n\r\n<ul>\r\n\t<li>\r\n\t<p>Fetched video_id instead of hashed_id for all categories in dashboard and userprofile</p>\r\n\t</li>\r\n</ul>\r\n\r\n<p><strong>Others</strong></p>\r\n\r\n<ul>\r\n\t<li>\r\n\t<p>Installed Oracle 11g &amp; 12c using docker</p>\r\n\t</li>\r\n\t<li>\r\n\t<p>Web socket for sending message to all subscribed clients</p>\r\n\t</li>\r\n</ul>",
      "date": "2019-07-25"
    }
  },
  {
    "model": "documents.createnewdocuments",
    "pk": 22,
    "fields": {
      "created_by": 13,
      "title": "2019-08-12",
      "content": "<h1>GPS Tracking</h1>\r\n\r\n<h2>Description</h2>\r\n\r\n<p>1) API for POI Correction</p>\r\n\r\n<p>2) Test and Correction</p>\r\n\r\n<p><strong>Problem faced: None</strong></p>\r\n\r\n<p><strong>Time taken: 2 weeks</strong></p>\r\n\r\n<h1>Phonix Connector</h1>\r\n\r\n<h2>Description</h2>\r\n\r\n<p>1) Modified Phonix Connector to be compatible with out phoenix table design</p>\r\n\r\n<p><strong>Problem faced: None</strong></p>\r\n\r\n<p><strong>Time Taken: 2 weeks</strong></p>\r\n\r\n<h1>DFS file format:</h1>\r\n\r\n<h2>Description</h2>\r\n\r\n<p>1) Follower sends file metadata to master</p>\r\n\r\n<p><strong>Problem faced; None</strong></p>\r\n\r\n<p><strong>Time taken: 4 days</strong></p>",
      "date": "2019-07-25"
    }
  },
  {
    "model": "documents.createnewdocuments",
    "pk": 23,
    "fields": {
      "created_by": 23,
      "title": "2019 -(07-29 to 08-09)",
      "content": "<p><strong>Works&nbsp;Done:</strong></p>\r\n\r\n<p><strong>Hadoop:</strong></p>\r\n\r\n<p>- Study HA&nbsp;, Zookeeper, ZKFC</p>\r\n\r\n<p>- Install ZK (multi-node)</p>\r\n\r\n<p>-&nbsp;Install Hadoop HA</p>\r\n\r\n<p>- Upload file in HDFS check and understand&nbsp;metadata (Version file)</p>\r\n\r\n<p>-&nbsp;Study replication factor and implementation.</p>\r\n\r\n<p>- Install Hadoop 3.1.2 HA in server (30-33)</p>\r\n\r\n<p>- Decommission and commission data node &amp; change existing data node to Standby namenode</p>\r\n\r\n<p>- Upgrade hadoop 2.9.1 to 3.1.2 with downtime.</p>",
      "date": "2019-07-25"
    }
  },
  {
    "model": "documents.createnewdocuments",
    "pk": 25,
    "fields": {
      "created_by": 16,
      "title": "12_23Aug_2019",
      "content": "<p><strong>Week Report</strong></p>\r\n\r\n<p><strong>Kafka:</strong></p>\r\n\r\n<p>1. Aggregation in ksql using both kstream and ktable.</p>\r\n\r\n<p>2. Extract data from API&nbsp;</p>\r\n\r\n<p>3. Create consumer as java application</p>\r\n\r\n<p>4. Scala</p>\r\n\r\n<p><strong>Result</strong></p>\r\n\r\n<p><img src=\"/media/uploads/2019/08/23/image_dR7FSbq.png\" style=\"height:193px; width:795px\" /></p>\r\n\r\n<p><strong>Connector</strong></p>\r\n\r\n<p><img src=\"/media/uploads/2019/08/23/image.png\" style=\"height:337px; width:1318px\" /></p>\r\n\r\n<p><strong>Issues</strong></p>\r\n\r\n<p>1) sink data to existing index in elasticsearch.</p>\r\n\r\n<p>solution:&nbsp;&quot;topic.index.map&quot;: &quot;hgi_classDetails:es_data&quot;</p>\r\n\r\n<p>2) Kafka cannot interprete numeric data type.</p>\r\n\r\n<p>solution:&nbsp;&quot;numeric.mapping&quot;: &quot;best_fit&quot;</p>\r\n\r\n<p>3) Key value cannot be interpreted while creating key for ktable</p>\r\n\r\n<p>solution:&nbsp;</p>\r\n\r\n<p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &quot;transforms&quot;:&quot;createKey, extractString&quot;,<br />\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;&quot;transforms.createKey.type&quot;:&quot;org.apache.kafka.connect.transforms.ValueToKey&quot;,<br />\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &quot;transforms.createKey.fields&quot;:&quot;created_date&quot;,<br />\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;&quot;transforms.extractString.type&quot;:&quot;org.apache.kafka.connect.transforms.ExtractField$Key&quot;,<br />\r\n&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&quot;transforms.extractString.field&quot;:&quot;created_date&quot;,</p>\r\n\r\n<p>4) Connector with query cannot deduce column from joined tables.</p>\r\n\r\n<p>solution:</p>\r\n\r\n<p>&quot;query&quot;: &quot;select * from (select c.\\&quot;CreatedDate\\&quot; as \\&quot;created_date\\&quot;, p.\\&quot;PolicyNumber\\&quot; as policy_no, CASE WHEN p.\\&quot;AgentDetail\\&quot; LIKE &#39;{%}&#39; THEN p.\\&quot;AgentDetail\\&quot;::json-&gt;&gt;&#39;AgentName&#39; ELSE &#39;Null&#39; END AS \\&quot;agent\\&quot;, CASE WHEN c.\\&quot;CalulationDetailJSON\\&quot; LIKE &#39;{%}&#39; THEN c.\\&quot;CalulationDetailJSON\\&quot;::json-&gt;&gt;&#39;AgentCommissonAmount&#39; ELSE &#39;Null&#39; END AS \\&quot;commission_amount\\&quot;, p.\\&quot;ExpiryDate\\&quot; as expiry_date, CAST(p.\\&quot;Transaction\\&quot; AS NUMERIC(15,4)) AS premium_amount, p.\\&quot;VATBillNo\\&quot; as vat_bill_no, p.\\&quot;EndorsementType\\&quot; as endorsement_type, CASE WHEN p.\\&quot;BranchDetail\\&quot; LIKE &#39;{%}&#39; THEN p.\\&quot;BranchDetail\\&quot;::json-&gt;&gt;&#39;name&#39; ELSE &#39;Null&#39; END AS \\&quot;branch\\&quot;, CAST(p.\\&quot;Stampduty\\&quot; AS NUMERIC(15,4)) AS stamp_duty, p.\\&quot;IssueDate\\&quot; as issue_date, CAST(p.\\&quot;VatAmount\\&quot; AS NUMERIC(15,4)) AS vat_amount, p.\\&quot;InsuredPartyName\\&quot; as insured_name, p.\\&quot;Portfolio\\&quot; as portfolio, p.\\&quot;DocumentNumber\\&quot; as document_no, p.\\&quot;EffectiveDate\\&quot; as effective_date, p.\\&quot;Class\\&quot; as class, p.\\&quot;DocumentType\\&quot; as document_type, CAST(p.\\&quot;SumInsured\\&quot; AS NUMERIC(15,4)) AS sum_insured_amount from \\&quot;PolicyIssuance\\&quot; as p left outer join \\&quot;ClassDetails\\&quot; c &nbsp;on p.\\&quot;Id\\&quot;=c.\\&quot;PolicyIssuanceId\\&quot;) as hgi_reporting&quot;,&nbsp;</p>\r\n\r\n<p>5) nested json extraction</p>\r\n\r\n<p>solution:</p>\r\n\r\n<p><u>1. Postegres query base</u></p>\r\n\r\n<p>select p.&quot;PolicyNumber&quot; as policy_no,&nbsp;<br />\r\np.&quot;AgentDetail&quot;::json-&gt;&gt;&#39;AgentName&#39; as agent,&nbsp;<br />\r\nc.&quot;CalulationDetailJSON&quot;::json-&gt;&gt;&#39;AgentCommissonAmount&#39; as commission_amount&nbsp;from &quot;PolicyIssuance&quot; as p left join &quot;ClassDetails&quot; c &nbsp;on p.&quot;Id&quot;=c.&quot;PolicyIssuanceId&quot;;</p>\r\n\r\n<p><u>2. KSQL jsonextract</u></p>\r\n\r\n<pre>\r\n CREATE STREAM recA_data WITH (VALUE_FORMAT=&#39;AVRO&#39;) AS \\\r\n SELECT EXTRACTJSONFIELD(RAFld1,&#39;$.someOtherField&#39;) AS someOtherField, \\\r\n         EXTRACTJSONFIELD(RAFld1,&#39;$.someFld&#39;)        AS someFld, \\\r\n         EXTRACTJSONFIELD(RAFld2,&#39;$.aFld&#39;)           AS aFld, \\\r\n         EXTRACTJSONFIELD(RAFld2,&#39;$.anotherFld&#39;)     AS anotherFld \\\r\n         FROM my_stream \\\r\n WHERE EXTRACTJSONFIELD(Header,&#39;$.RecType&#39;) = &#39;RecA&#39;;</pre>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p><strong>References</strong></p>\r\n\r\n<p># Extracting data from api:<br />\r\n------------------------------------<br />\r\n1) https://www.confluent.io/blog/data-wrangling-apache-kafka-ksql<br />\r\n2) https://stackoverflow.com/questions/53443768/how-to-integrate-rest-api-source-connector-with-kafka-connect</p>\r\n\r\n<p># KSQL details:<br />\r\n--------------<br />\r\n1) https://www.confluent.io/blog/data-wrangling-apache-kafka-ksql<br />\r\n2) https://stackoverflow.com/questions/53055605/ksql-is-it-possible-to-create-stream-from-multiple-topics-and-get-full-event-p</p>\r\n\r\n<p># Elasticsearch sink connector:<br />\r\n-------------------------------<br />\r\n1) https://stackoverflow.com/questions/48561197/how-to-connect-kafka-with-elasticsearch<br />\r\n2) https://speakerdeck.com/rmoff/building-streaming-data-pipelines-with-elasticsearch-apache-kafka-and-ksql?slide=40</p>\r\n\r\n<p># To handle numeric data:<br />\r\n-------------------------<br />\r\n1) https://gist.github.com/rmoff/7bb46a0b6d27982a5fb7a103bb7c95b9<br />\r\n2) https://gist.github.com/confluentgist/f58107f44741943a21c7a821c89bbf21</p>\r\n\r\n<p># Query in connector postgres:<br />\r\n------------------------------<br />\r\n1) https://stackoverflow.com/questions/32626261/how-to-parse-json-in-postgresql<br />\r\n2) https://stackoverflow.com/questions/16074375/postgresql-9-2-convert-text-json-string-to-type-json-hstore</p>",
      "date": "2019-08-19"
    }
  },
  {
    "model": "documents.createnewdocuments",
    "pk": 26,
    "fields": {
      "created_by": 13,
      "title": "2019-08-26",
      "content": "<h1><strong>Task 1&nbsp;</strong></h1>\r\n\r\n<h2>Work on new WorkFlow for GPS data processing</h2>\r\n\r\n<p><strong>Description</strong></p>\r\n\r\n<p>1) Data Storage and retrival pipeline</p>\r\n\r\n<p><strong>Time taken: 3 days</strong></p>\r\n\r\n<h1>Task 2</h1>\r\n\r\n<h2>Schema Design and simple CMS&nbsp;for HGI reporting</h2>\r\n\r\n<p><strong>Description</strong></p>\r\n\r\n<p>1)&nbsp; Nested json data were flattened and indexed into elasticsearch</p>\r\n\r\n<p>2) Simple CMS for reporting was created based on indexed data</p>\r\n\r\n<p><strong>Time taken: 2 weeks</strong></p>",
      "date": "2019-08-23"
    }
  },
  {
    "model": "documents.createnewdocuments",
    "pk": 27,
    "fields": {
      "created_by": 3,
      "title": "2019-(08-12 to 08-23)",
      "content": "<p><strong>Works Done:</strong></p>\r\n\r\n<p><strong>EKML-Pipeline</strong></p>\r\n\r\n<ul>\r\n\t<li>\r\n\t<p>Use of schema registry for phoenix sink connector and streams.</p>\r\n\t</li>\r\n\t<li>\r\n\t<p>Studying KSQL in more detail</p>\r\n\t</li>\r\n\t<li>\r\n\t<p>Elasticsearch json parsing</p>\r\n\t</li>\r\n\t<li>\r\n\t<p>Java Consumer for kafka topic, with avro format deserializer</p>\r\n\t</li>\r\n\t<li>\r\n\t<p>Topic message filtering and sending to another topic in avro format</p>\r\n\t</li>\r\n\t<li>\r\n\t<p>Started Proxy rest service for kafka topics</p>\r\n\t</li>\r\n</ul>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p><strong>HGI</strong></p>\r\n\r\n<ul>\r\n\t<li>\r\n\t<p>Discussion of HGI reporting</p>\r\n\t</li>\r\n\t<li>\r\n\t<p>KSQL to filter hgi topic message</p>\r\n\t</li>\r\n\t<li>\r\n\t<p>Using spark kafka for topic subscription</p>\r\n\t</li>\r\n</ul>",
      "date": "2019-08-23"
    }
  },
  {
    "model": "documents.createnewdocuments",
    "pk": 28,
    "fields": {
      "created_by": 23,
      "title": "2019 -(08-12 to 08-23)",
      "content": "<p><strong>Works&nbsp;Done:</strong></p>\r\n\r\n<p><strong>Hadoop and Security:</strong></p>\r\n\r\n<p>-&nbsp;Study data corruption/missing and potential problems while upgrading cluster.</p>\r\n\r\n<p>-&nbsp;Data node commission and test data corruption/missing.</p>\r\n\r\n<p>- Upgrade Hadoop 2.9 to 3.1 and rollback (downgrade), revert back to version Hadoop 2.9.</p>\r\n\r\n<p>-&nbsp;Study Hadoop Security, SSL, SASL and Kerberos.</p>\r\n\r\n<p>-&nbsp;Install Kerberos Server in a Single Node and check ticket granting service.</p>\r\n\r\n<p>-&nbsp;Configure Kerberos on multiple nodes. Create principals and keytabs.&nbsp;Issue faced: Services did not start.</p>\r\n\r\n<p>-&nbsp;Configure SSL, keystore and truststore. Install SSL certificates. (Self Sign). Issue faced: Keytab not recognized</p>\r\n\r\n<p>-&nbsp;Create and configure ssl-server.xml and ssl-client.xml files.</p>\r\n\r\n<p>-&nbsp;Study kerberos configurations and make some adjustments. Fix issue of namenode not starting. Namenode was able to start and get lock. (added web configuration for NN)</p>\r\n\r\n<p>-&nbsp;Study SSL keystore, truststore and certificates again. Remove previosly installed &quot;Self Signed&quot; certificates and install again.&nbsp;Findings: Two self sign method. Preferred method to use self sign using Certificate Authority.</p>",
      "date": "2019-08-23"
    }
  },
  {
    "model": "documents.createnewdocuments",
    "pk": 29,
    "fields": {
      "created_by": 3,
      "title": "2019-(08-26 to 09-06)",
      "content": "<h2><strong>Works Done:</strong></h2>\r\n\r\n<p><strong>EKML-Pipeline:</strong></p>\r\n\r\n<ul>\r\n\t<li>\r\n\t<p>Studying Kafka streams using java. [1]</p>\r\n\t</li>\r\n\t<li>\r\n\t<p>Implemented Video streaming with kafka streams [2]</p>\r\n\t</li>\r\n\t<li>\r\n\t<p>Added multiple consumers for a topic with different set of works.</p>\r\n\t</li>\r\n\t<li>\r\n\t<p>Video streams with Python and Kafka.</p>\r\n\t</li>\r\n</ul>\r\n\r\n<p><strong>References</strong></p>\r\n\r\n<ol>\r\n\t<li>\r\n\t<p><a href=\"https://airccj.org/CSCP/vol7/csit76320.pdf\">https://airccj.org/CSCP/vol7/csit76320.pdf</a></p>\r\n\t</li>\r\n\t<li>\r\n\t<p><a href=\"https://www.infoq.com/articles/video-stream-analytics-opencv/\">https://www.infoq.com/articles/video-stream-analytics-opencv/</a></p>\r\n\t</li>\r\n</ol>",
      "date": "2019-09-08"
    }
  },
  {
    "model": "documents.createnewdocuments",
    "pk": 30,
    "fields": {
      "created_by": 13,
      "title": "2019-sep-09",
      "content": "<h1>Task 1</h1>\r\n\r\n<h2>Data Correction For Hgi</h2>\r\n\r\n<p>Description</p>\r\n\r\n<p>1)&nbsp; Created Business Report&nbsp;</p>\r\n\r\n<p>2)&nbsp; Needed time to know data better</p>\r\n\r\n<p><strong>Time Taken: 3 days</strong></p>\r\n\r\n<p><strong>Problem Faced:</strong> Since no one knew what they wanted, I had no idea what to expect from data</p>",
      "date": "2019-09-09"
    }
  },
  {
    "model": "documents.createnewdocuments",
    "pk": 31,
    "fields": {
      "created_by": 23,
      "title": "2019 -(08-26 to 09-06)",
      "content": "<p><strong>Works&nbsp;Done:</strong></p>\r\n\r\n<p><strong>Hadoop and Security:</strong></p>\r\n\r\n<p>-&nbsp;Install SSL certificates using self sign using Certificate Authority.</p>\r\n\r\n<p>- Create proper keytabs for serives and host.</p>\r\n\r\n<p>- Test kerberos setup. (Issue faced: Namenode does not automatically restart using start-dfs.sh, have to start namenode daemon individually / Unable to view FS in web UI)</p>\r\n\r\n<p>- Study ACL for hadoop (getfacl, setfacl)</p>\r\n\r\n<p>- Configure local machine as hadoop client, created keytabs for client&nbsp;and grant access to the cluster and services.</p>\r\n\r\n<p>- Create Documentation for setting up hadoop kerberos and hadoop edge node (client).</p>\r\n\r\n<p>- Create Java api to run Hadoop services and also access kerberized high availablity cluster.</p>\r\n\r\n<p>- Study about Hbase and its alternatives in brief and compare. (Cassandra, DynamoDB, MongoDB, Accumulo)</p>",
      "date": "2019-09-09"
    }
  },
  {
    "model": "documents.createnewdocuments",
    "pk": 32,
    "fields": {
      "created_by": 15,
      "title": "Weekly Report_23-Aug-06-Sept-2019",
      "content": "<h1><strong>Task&nbsp;1:</strong></h1>\r\n\r\n<h2><strong>CHURN PREDICTION</strong></h2>\r\n\r\n<p><strong>Customer visit dataset in each month.</strong></p>\r\n\r\n<p><strong>Train and test new churn prediction model.</strong></p>\r\n\r\n<p><strong>Accuracy: 80-81%</strong></p>\r\n\r\n<p><strong>Implementing Merge in 2 Sequential models gave&nbsp;accuracy&nbsp;89-90%, which was not sufficient.&nbsp;</strong></p>\r\n\r\n<p><strong><img alt=\"\" src=\"/media/uploads/2019/09/09/multiple_inputs.png\" style=\"height:936px; width:600px\" /></strong></p>\r\n\r\n<h2><strong>Recommendation System:</strong></h2>\r\n\r\n<h3><strong>Analysis</strong></h3>\r\n\r\n<h3><strong>Feature Extraction</strong></h3>",
      "date": "2019-09-09"
    }
  },
  {
    "model": "documents.createnewdocuments",
    "pk": 34,
    "fields": {
      "created_by": 22,
      "title": "9Sept-20Sept Pema",
      "content": "<ol>\r\n\t<li>\r\n\t<h1>&nbsp;Nepali pos tagging</h1>\r\n\t</li>\r\n</ol>\r\n\r\n<p>adding mattermost notification when the server is down</p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p>Remarks</p>\r\n\r\n<ol start=\"2\">\r\n\t<li>\r\n\t<h1>Dementia</h1>\r\n\t</li>\r\n</ol>\r\n\r\n<p>getting sentence categories of all the files</p>\r\n\r\n<p>Handling the errors</p>\r\n\r\n<p>Issue: The new app build was not using our server.<br />\r\n&nbsp;</p>\r\n\r\n<p>Remarks</p>\r\n\r\n<p>Tested and sent the updated code to Naito san</p>\r\n\r\n<p>Running dementia in pm2</p>\r\n\r\n<p><img src=\"https://lh6.googleusercontent.com/YF51EGkuUiFLA-3NLB5VhZCRT6idpvD2Roc-xpkKii_R7yilEM4GQ9yPbGjqVpbc4V3YK2ipUMpLEDkGkuPCEyf6-ASHSAS5QBInQ9e1xrgFW_SyP7NdGuQS79DldyTggPmWi_M\" style=\"height:64px; width:624px\" /></p>\r\n\r\n<p><br />\r\n&nbsp;</p>\r\n\r\n<ol start=\"3\">\r\n\t<li>\r\n\t<h1>Transliteration</h1>\r\n\t</li>\r\n</ol>\r\n\r\n<p>Checking if the word transliterated is in the databse gave better result than before.</p>\r\n\r\n<p>OLD OUTPUT:</p>\r\n\r\n<p>congress party le ke garnu ? ===&gt;\u0915\u093e\u0902\u0917\u094d\u0930\u0947\u0938 \u092a\u093e\u0930\u094d\u091f\u0940 \u0932\u0947 \u0915\u0947 {&#39;garnu&#39;: [&#39;\u0917\u0930\u094d\u0928\u0941&#39;, &#39;\u0917\u0930\u0928\u0941&#39;]} ?</p>\r\n\r\n<p>ke hami sabai yestai ho ra ? ===&gt;\u0915\u0947 {&#39;hami&#39;: [&#39;\u0939\u093e\u092e\u0940&#39;, &#39;\u0939\u092e\u0940&#39;]} {&#39;sabai&#39;: [&#39;\u0938\u093e\u092c\u0948&#39;, &#39;\u0938\u092c\u093e\u0908&#39;]} {&#39;yestai&#39;: [&#39;\u092f\u0947\u0938\u094d\u0924\u0948&#39;, &#39;\u092f\u0947\u0938\u094d\u091f\u093e\u0908&#39;]} \u0939\u094b \u0930 ?</p>\r\n\r\n<p>timi ke gardai chau ? ===&gt;\u0924\u093f\u092e\u0940 \u0915\u0947 {&#39;gardai&#39;: [&#39;\u0917\u0930\u094d\u0921\u0948&#39;, &#39;\u0917\u0930\u0926\u0908&#39;]} {&#39;chau&#39;: [&#39;\u091b\u094c&#39;, &#39;\u091a\u093e\u0909&#39;]} ?</p>\r\n\r\n<p>timle khana khayo ra ? ===&gt;{&#39;timle&#39;: [&#39;\u0924\u093f\u092e\u0932\u0947&#39;, &#39;\u091f\u093f\u092e\u0932\u0947&#39;]} \u0916\u093e\u0928\u093e \u0916\u093e\u092f\u094b \u0930 ?</p>\r\n\r\n<p>why would you do that ? ===&gt;\u0935\u093e\u0908 {&#39;would&#39;: [&#39;\u0935\u094b\u0909\u0932\u094d\u0921&#39;, &#39;\u0935\u0949\u0909\u0932\u094d\u0921&#39;]} {&#39;you&#39;: [&#39;\u092f\u094b\u0909&#39;, &#39;\u092f\u0942&#39;]} \u0926\u094b {&#39;that&#39;: [&#39;\u0925\u093e\u0924&#39;, &#39;\u0924\u0924&#39;]} ?</p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p>NEW OUTPUT: checking if word in dictionary</p>\r\n\r\n<p>congress party le ke garnu sakcha ? ===&gt;\u0915\u093e\u0902\u0917\u094d\u0930\u0947\u0938 \u092a\u093e\u0930\u094d\u091f\u0940 \u0932\u0947 \u0915\u0947 \u0917\u0930\u094d\u0928\u0941 {&#39;sakcha&#39;: [&#39;\u0938\u093e\u0915\u091b\u093e&#39;, &#39;\u0938\u0915\u094d\u091a\u093e&#39;]} ?</p>\r\n\r\n<p>ke hami sabai yestai ho ra ? ===&gt;\u0915\u0947 \u0939\u093e\u092e\u0940 {&#39;sabai&#39;: [&#39;\u0938\u093e\u092c\u0948&#39;, &#39;\u0938\u092c\u093e\u0908&#39;]} {&#39;yestai&#39;: [&#39;\u092f\u0947\u0938\u094d\u0924\u0948&#39;, &#39;\u092f\u0947\u0938\u094d\u091f\u093e\u0908&#39;]} \u0939\u094b \u0930 ?</p>\r\n\r\n<p>timi ke gardai chau ? ===&gt;\u0924\u093f\u092e\u0940 \u0915\u0947 {&#39;gardai&#39;: [&#39;\u0917\u0930\u094d\u0921\u0948&#39;, &#39;\u0917\u0930\u0926\u0908&#39;]} \u091b\u094c ?</p>\r\n\r\n<p>timle khana khayo ra ? ===&gt;\u091f\u093f\u092e\u0932\u0947 \u0916\u093e\u0928\u093e \u0916\u093e\u092f\u094b \u0930 ?</p>\r\n\r\n<p>why would you do that ? ===&gt;\u0935\u093e\u0908 {&#39;would&#39;: [&#39;\u0935\u094b\u0909\u0932\u094d\u0921&#39;, &#39;\u0935\u0949\u0909\u0932\u094d\u0921&#39;]} \u092f\u0942 \u0926\u094b {&#39;that&#39;: [&#39;\u0925\u093e\u0924&#39;, &#39;\u0924\u0924&#39;]} ?</p>\r\n\r\n<p><br />\r\n&nbsp;</p>\r\n\r\n<ol start=\"4\">\r\n\t<li>\r\n\t<h1>Model to generate incorrect spell word list</h1>\r\n\t</li>\r\n</ol>\r\n\r\n<p>Building model to get incorrect word list for spelling correct)</p>\r\n\r\n<p>{&#39;\u0928\u0947\u092a\u093e\u0932\u093f&#39;: [&#39;\u0928\u0947\u092a\u093e\u0932\u0926&#39;, &#39;\u0928\u0947\u092a\u093e\u0932\u092f&#39;, &#39;\u0928\u0947\u092a\u093e\u0932&#39;, &#39;\u0928\u0947\u092a\u0932\u0907&#39;, &#39;\u0928\u0947\u092a\u093e\u0932\u0940&#39;]}</p>\r\n\r\n<p>{&#39;\u092a\u094b\u0938\u094d\u091f\u094d&#39;: [&#39;\u092a\u094b\u0938\u094d\u091f\u0930&#39;, &#39;\u092a\u094b\u0938\u094d\u091f&#39;, &#39;\u092a\u094b\u0938\u094d\u0924&#39;, &#39;\u092a\u094b\u0938\u094d\u091f&#39;]}</p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p>After adding:</p>\r\n\r\n<p>str1 = trn_to_nep.transform(str.replace(&quot;a&quot;,&quot;aa&quot;))</p>\r\n\r\n<p>str2 = trn_to_nep.transform(str.replace(&quot;e&quot;,&quot;ee&quot;))</p>\r\n\r\n<p>str3 = trn_to_nep.transform(str.replace(&quot;i&quot;, &quot;e&quot;))</p>\r\n\r\n<p>str4 = trn_to_nep.transform(str.replace(&quot;e&quot;, &quot;i&quot;))</p>\r\n\r\n<p>str5 = trn_to_nep.transform(str.replace(&quot;o&quot;, &quot;oo&quot;))</p>\r\n\r\n<p>str6 = trn_to_nep.transform(str.replace(&quot;u&quot;, &quot;o&quot;))</p>\r\n\r\n<p><br />\r\n&nbsp;</p>\r\n\r\n<p>OUTPUT:</p>\r\n\r\n<p>{&#39;\u0928\u0947\u092a\u093e\u0932\u093f&#39;: {&#39;\u0928\u093f\u092a\u093e\u0932\u0940&#39;, &#39;\u0928\u0947\u092a\u0932\u0907&#39;, &#39;\u0928\u0947\u092a\u093e\u0932\u0947&#39;, &#39;\u0928\u0947\u092a\u093e\u0932&#39;, &#39;\u0928\u0947\u092a\u093e\u0932\u0940&#39;, &#39;\u0928\u0947\u092a\u093e\u0932&ndash;&#39;, &#39;\u0928\u0940\u092a\u093e\u0932\u0940&#39;}}</p>\r\n\r\n<p>{&#39;\u092a\u094b\u0938\u094d\u091f\u094d&#39;: {&#39;\u092a\u0942\u0938\u094d\u091f&#39;, &#39;\u092a\u094b\u0938\u094d\u091f&#39;, &#39;\u092a\u094b\u0938\u094d\u0924&#39;, &#39;\u092a\u094b\u0938\u094d\u091f\u0930&#39;}}</p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p>Remarks</p>\r\n\r\n<p>This will be used for spelling correction model.</p>\r\n\r\n<ol start=\"5\">\r\n\t<li>\r\n\t<h1>&nbsp;Advertisement Network</h1>\r\n\r\n\t<ol>\r\n\t\t<li>\r\n\t\t<h1>Deployed NLP part for target ads using fastapi</h1>\r\n\t\t</li>\r\n\t</ol>\r\n\t</li>\r\n</ol>\r\n\r\n<p>URL: url:<a href=\"http://110.44.123.49:8090/\"> http://110.44.123.49:8090/</a></p>\r\n\r\n<p>data:</p>\r\n\r\n<p>{</p>\r\n\r\n<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&quot;url&quot;: &quot;<a href=\"https://baahrakhari.com/news-details/199801/2019-06-14%22\">https://baahrakhari.com/news-details/199801/2019-06-14&quot;</a>,</p>\r\n\r\n<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&quot;language&quot;: &quot;ne&quot;,</p>\r\n\r\n<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&quot;ads&quot;:[</p>\r\n\r\n<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;{&quot;id&quot;: 1, &quot;keywords&quot;: [&quot;\u092e\u0928\u094b\u0930\u0928\u094d\u091c\u0928&quot;]},</p>\r\n\r\n<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;{&quot;id&quot;: 2, &quot;keywords&quot;: [&quot;\u0916\u0947\u0932\u0915\u0941\u0926&quot;]},</p>\r\n\r\n<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;{&quot;id&quot;: 3, &quot;keywords&quot;: [&quot;\u092a\u092a\u094d\u092a\u0941&quot;,&quot;\u0915\u0928\u094d\u0938\u094d\u091f\u094d\u0930\u0915\u094d\u0938\u0928&quot;]}</p>\r\n\r\n<p>]</p>\r\n\r\n<p>}</p>\r\n\r\n<p>for this id 3 was returned</p>\r\n\r\n<ol start=\"6\">\r\n\t<li>\r\n\t<ol start=\"2\">\r\n\t\t<li>\r\n\t\t<p>Testing</p>\r\n\t\t</li>\r\n\t</ol>\r\n\t</li>\r\n</ol>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<table>\r\n\t<tbody>\r\n\t\t<tr>\r\n\t\t\t<td>&nbsp;</td>\r\n\t\t\t<td>\r\n\t\t\t<p>OUTPUT</p>\r\n\t\t\t</td>\r\n\t\t</tr>\r\n\t\t<tr>\r\n\t\t\t<td>\r\n\t\t\t<p>{</p>\r\n\r\n\t\t\t<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&quot;url&quot;: &quot;https://baahrakhari.com/news-details/216683/2019-09-19&quot;,</p>\r\n\r\n\t\t\t<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&quot;language&quot;: &quot;ne&quot;,</p>\r\n\r\n\t\t\t<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&quot;ads&quot;:[</p>\r\n\r\n\t\t\t<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;{&quot;id&quot;: 1, &quot;keywords&quot;: [&quot;\u092e\u0928\u094b\u0930\u0928\u094d\u091c\u0928&quot;]},</p>\r\n\r\n\t\t\t<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;{&quot;id&quot;: 2, &quot;keywords&quot;: [&quot;\u0916\u0947\u0932\u0915\u0941\u0926&quot;]},</p>\r\n\r\n\t\t\t<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;{&quot;id&quot;: 3, &quot;keywords&quot;: [&quot;\u0930\u093e\u091c\u0928\u0940\u0924\u093f&quot;]}</p>\r\n\r\n\t\t\t<p>]</p>\r\n\r\n\t\t\t<p>}</p>\r\n\t\t\t</td>\r\n\t\t\t<td>\r\n\t\t\t<p>{</p>\r\n\r\n\t\t\t<p>&nbsp;&nbsp;&nbsp;&nbsp;&quot;ads_id&quot;: 2</p>\r\n\r\n\t\t\t<p>}</p>\r\n\t\t\t</td>\r\n\t\t</tr>\r\n\t\t<tr>\r\n\t\t\t<td>\r\n\t\t\t<p>{</p>\r\n\r\n\t\t\t<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&quot;url&quot;: &quot;https://baahrakhari.com/news-details/216863/2019-09-20&quot;,</p>\r\n\r\n\t\t\t<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&quot;language&quot;: &quot;ne&quot;,</p>\r\n\r\n\t\t\t<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&quot;ads&quot;:[</p>\r\n\r\n\t\t\t<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;{&quot;id&quot;: 1, &quot;keywords&quot;: [&quot;\u092e\u0928\u094b\u0930\u0928\u094d\u091c\u0928&quot;]},</p>\r\n\r\n\t\t\t<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;{&quot;id&quot;: 2, &quot;keywords&quot;: [&quot;\u0916\u0947\u0932\u0915\u0941\u0926&quot;]},</p>\r\n\r\n\t\t\t<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;{&quot;id&quot;: 3, &quot;keywords&quot;: [&quot;\u0930\u093e\u091c\u0928\u0940\u0924\u093f&quot;]}</p>\r\n\r\n\t\t\t<p>]</p>\r\n\r\n\t\t\t<p>}</p>\r\n\t\t\t</td>\r\n\t\t\t<td>\r\n\t\t\t<p>{</p>\r\n\r\n\t\t\t<p>&nbsp;&nbsp;&nbsp;&nbsp;&quot;ads_id&quot;: 1</p>\r\n\r\n\t\t\t<p>}</p>\r\n\t\t\t&nbsp;\r\n\r\n\t\t\t<p>NOTE: since no match was found, id 1 was returned</p>\r\n\t\t\t</td>\r\n\t\t</tr>\r\n\t\t<tr>\r\n\t\t\t<td>\r\n\t\t\t<p>{</p>\r\n\r\n\t\t\t<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&quot;url&quot;: &quot;https://baahrakhari.com/news-details/216961/2019-09-20&quot;,</p>\r\n\r\n\t\t\t<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&quot;language&quot;: &quot;ne&quot;,</p>\r\n\r\n\t\t\t<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&quot;ads&quot;:[</p>\r\n\r\n\t\t\t<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;{&quot;id&quot;: 1, &quot;keywords&quot;: [&quot;\u092e\u0928\u094b\u0930\u0928\u094d\u091c\u0928&quot;]},</p>\r\n\r\n\t\t\t<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;{&quot;id&quot;: 2, &quot;keywords&quot;: [&quot;\u0916\u0947\u0932\u0915\u0941\u0926&quot;]},</p>\r\n\r\n\t\t\t<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;{&quot;id&quot;: 3, &quot;keywords&quot;: [&quot;\u0930\u093e\u091c\u0928\u0940\u0924\u093f&quot;]}</p>\r\n\r\n\t\t\t<p>]</p>\r\n\r\n\t\t\t<p>}</p>\r\n\t\t\t</td>\r\n\t\t\t<td>\r\n\t\t\t<p>{</p>\r\n\r\n\t\t\t<p>&nbsp;&nbsp;&nbsp;&nbsp;&quot;ads_id&quot;: 1</p>\r\n\r\n\t\t\t<p>}</p>\r\n\t\t\t</td>\r\n\t\t</tr>\r\n\t</tbody>\r\n</table>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p>Remarks:</p>\r\n\r\n<p>Need to save the URL and keywords to mongodb</p>\r\n\r\n<ol start=\"6\">\r\n\t<li>\r\n\t<h1>Others</h1>\r\n\r\n\t<ol>\r\n\t\t<li>\r\n\t\t<p>Research on NLP models: <a href=\"https://docs.google.com/document/d/11anDlKu2p90Gslusw2qxN5VZKU3FB10FWKJvbyEFOF4/edit?usp=sharing\">https://docs.google.com/document/d/11anDlKu2p90Gslusw2qxN5VZKU3FB10FWKJvbyEFOF4/edit?usp=sharing</a></p>\r\n\t\t</li>\r\n\t\t<li>\r\n\t\t<p>Companies that does NLP</p>\r\n\t\t</li>\r\n\t\t<li>\r\n\t\t<p>Basic tutorials</p>\r\n\r\n\t\t<ol>\r\n\t\t\t<li>\r\n\t\t\t<p><a href=\"https://competitions.codalab.org/competitions/\">https://competitions.codalab.org/competitions/</a></p>\r\n\t\t\t</li>\r\n\t\t\t<li>\r\n\t\t\t<p><a href=\"https://www.hackerrank.com/domains/ai?filters%5Bsubdomains%5D%5B%5D=nlp\">https://www.hackerrank.com/domains/ai?filters%5Bsubdomains%5D%5B%5D=nlp</a></p>\r\n\t\t\t</li>\r\n\t\t</ol>\r\n\t\t</li>\r\n\t\t<li>\r\n\t\t<p>Learning XLNET</p>\r\n\r\n\t\t<ol>\r\n\t\t\t<li>\r\n\t\t\t<p><a href=\"https://docs.google.com/document/d/11anDlKu2p90Gslusw2qxN5VZKU3FB10FWKJvbyEFOF4/edit\">https://docs.google.com/document/d/11anDlKu2p90Gslusw2qxN5VZKU3FB10FWKJvbyEFOF4/edit</a></p>\r\n\t\t\t</li>\r\n\t\t</ol>\r\n\t\t</li>\r\n\t\t<li>\r\n\t\t<p>NER using bert</p>\r\n\t\t</li>\r\n\t</ol>\r\n\t</li>\r\n</ol>",
      "date": "2019-09-21"
    }
  },
  {
    "model": "documents.createnewdocuments",
    "pk": 35,
    "fields": {
      "created_by": 3,
      "title": "2019-(09-09 to 09-20)",
      "content": "<p><strong>Kafka</strong></p>\r\n\r\n<p><strong>Topic Partition</strong></p>\r\n\r\n<p><span style=\"background-color:transparent; color:#000000\">Each Consumer subscribes to a/more partition in a topic. And each consumer belongs to a consumer group. Below are two scenarios:</span></p>\r\n\r\n<ol>\r\n\t<li style=\"list-style-type:decimal\">\r\n\t<p><strong>When all consumers belong to the same group</strong><span style=\"background-color:transparent; color:#000000\"> : Each consumer will try to subscribe to a different partition. In case,if there is only one partition, only one consumer will get the messages, while other consumers will be idle.</span></p>\r\n\t</li>\r\n\t<li style=\"list-style-type:decimal\">\r\n\t<p><strong>When all consumers belong to the different consumer group</strong><span style=\"background-color:transparent; color:#000000\">: Each consumer will get the messages from all partitions. Partition subscription is based on the consumer groups.</span></p>\r\n\t</li>\r\n</ol>\r\n\r\n<p><span style=\"background-color:transparent; color:#000000\">It depends on the consumer groups. Consumers within the same consumer group don&#39;t read the data again from the same partitions once the read offsets have been committed.</span></p>\r\n\r\n<p><img src=\"https://lh5.googleusercontent.com/vyyaeiHQDqgxfZO0se1lPD8zDVB8Ps8q_SnLEKPe-2KNpm53aK9OIbJSPOC8J0hwd2s1WH2Sw1WA6I6xWoYwfzZEkpQMiyQFwwLunuHfqC3FARUSumFhQG1njyJNviteOE8VBL-2\" style=\"height:357px; margin-left:0px; margin-top:0px; width:684px\" /></p>\r\n\r\n<p><strong>Problem working with Partitions</strong></p>\r\n\r\n<p><span style=\"background-color:transparent; color:#000000\">Kafka only provides a total order over messages within a partition, not between different partitions in a topic.</span></p>\r\n\r\n<p><strong>Video Stream</strong></p>\r\n\r\n<p><span style=\"background-color:transparent; color:#000000\">While creating a video stream server using Kafka, consumer was not fetching frames in order from kafka brokers. So, in order to achieve&nbsp; ordering I used a single partition for streaming a video.</span></p>\r\n\r\n<p><strong>Kafka Max Request Size</strong></p>\r\n\r\n<p><span style=\"background-color:transparent; color:#000000\">The maximum size of a request. This is also effectively a cap on the maximum record size. Note that the server has its own cap on record size which may be different from this. This setting will limit the number of record batches the producer will send in a single request to avoid sending huge requests. Default: 1048576.</span></p>\r\n\r\n<p><strong>Problem with Request Size</strong></p>\r\n\r\n<p><span style=\"background-color:transparent; color:#000000\">By default max request size is 1 MB, but the size of frames were around 3 MB which was solved by changing the settings of kafka broker, consumer and producer. In order to increase/decrease request size we need to change settings in all kafka broker, consumer and producer.</span></p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p><strong>Connection issue with IP-Camera (LZ4 compression)</strong></p>\r\n\r\n<p><span style=\"background-color:transparent; color:#000000\">We deployed video stream server with kafka in our live system (Face detection for attendance), which was working fine but due to some technical fault IP-Camera used to get disconnected and was throwing connection error in kafka consumer, although kafka producer was working fine, consumers was not receiving any new frames. Issue was with compression library which was not installed in consumer side environment (LZ4). After installing lz4 library consumer was working fine.</span></p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p><strong>Multiple Clients for Streaming</strong></p>\r\n\r\n<p><span style=\"background-color:transparent; color:#000000\">Multiple clients are accessing video stream frames, so in order to provide streams to all the clients multiple consumers were created and for flask we used thread.</span></p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p><strong>Node and React Kafka Consumer</strong></p>\r\n\r\n<p><span style=\"background-color:transparent; color:#000000\">In order to consume streams faster and show the video stream smoother we tried to use kafka consumer in node and react js, but was not successful. We will look into it someday but for now we are using flask api for showing video stream in browser.</span></p>\r\n\r\n<p><strong>Elasticsearch Transforms (Dataframe)</strong></p>\r\n\r\n<p><span style=\"background-color:transparent; color:#000000\">Transforms enable you to convert existing Elasticsearch indices into summarized indices, which provide opportunities for new insights and analytics. For example, you can use transforms to pivot your data into entity-centric indices that summarize the behavior of users or sessions or other entities in your data.</span></p>\r\n\r\n<p><span style=\"background-color:transparent; color:#000000\">You can also think of the destination index as a two-dimensional tabular data structure (known as a data frame).&nbsp;</span></p>\r\n\r\n<p><a href=\"https://www.elastic.co/guide/en/elasticsearch/reference/7.x/put-transform.html\" style=\"text-decoration:none;\"><u>https://www.elastic.co/guide/en/elasticsearch/reference/7.x/put-transform.html</u></a></p>\r\n\r\n<p>&nbsp;</p>",
      "date": "2019-09-23"
    }
  },
  {
    "model": "documents.createnewdocuments",
    "pk": 36,
    "fields": {
      "created_by": 16,
      "title": "9_20_Sept_2019",
      "content": "<p><strong>Weekly Report</strong></p>\r\n\r\n<p><u><strong>HGI:</strong></u></p>\r\n\r\n<p>- Extract data of different portfolio using Pyspark.</p>\r\n\r\n<p>- Index data into elasticsearch.</p>\r\n\r\n<p>- The portfolio that is been extracted and indexed are:</p>\r\n\r\n<p>1. PrivateVehiclePartial</p>\r\n\r\n<p>2. MotorPartial</p>\r\n\r\n<p>3. ElectricMotorcyclePartial</p>\r\n\r\n<p>4. AmbulancePartial</p>\r\n\r\n<p>5. TaxiPartial</p>\r\n\r\n<p>6. GoodsCarryingVehiclePartial</p>\r\n\r\n<p>7. PassengerCarryingVehiclePartial</p>\r\n\r\n<p>8. ConstructionEquipmentPartia</p>\r\n\r\n<p>9. ElectricVehiclePartial</p>\r\n\r\n<p>10. TankerPartia</p>\r\n\r\n<p>11. TempoPartial</p>\r\n\r\n<p>12. TractorPartial</p>\r\n\r\n<p><u><strong>Issues:</strong></u></p>\r\n\r\n<p>- Spaces in fields name resulting in duplicate column names and reindexing.</p>\r\n\r\n<p>- Conversion of datetime into unix.</p>\r\n\r\n<p>- Addition of quarter in all index.</p>\r\n\r\n<p>&nbsp;</p>",
      "date": "2019-09-23"
    }
  },
  {
    "model": "documents.createnewdocuments",
    "pk": 37,
    "fields": {
      "created_by": 15,
      "title": "Weekly Report-July-20-2019",
      "content": "<h1>Task 1:</h1>\r\n\r\n<h2>Recommendation System:</h2>\r\n\r\n<h3>Feature Extraction</h3>\r\n\r\n<h3>Customer Selection</h3>\r\n\r\n<h3>Section Recommendation</h3>\r\n\r\n<h2>Churn Prediction:</h2>\r\n\r\n<h3>New Dataset Creation</h3>\r\n\r\n<h3>Feature creation</h3>\r\n\r\n<p>Accuracy: 88.74%</p>",
      "date": "2019-09-23"
    }
  },
  {
    "model": "documents.createnewdocuments",
    "pk": 38,
    "fields": {
      "created_by": 13,
      "title": "2019-sep-23",
      "content": "<h1>Task 1</h1>\r\n\r\n<h2>HGI SCHEMA</h2>\r\n\r\n<p><strong>Description</strong></p>\r\n\r\n<p>Schema for different classes of different Portfolios were created</p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p><strong>Time taken</strong> : 5 days and going on</p>\r\n\r\n<p>Problem faced : Since the data was a nested json , flattening and indexing was a challenge</p>\r\n\r\n<p>&nbsp;</p>",
      "date": "2019-09-23"
    }
  },
  {
    "model": "documents.createnewdocuments",
    "pk": 39,
    "fields": {
      "created_by": 23,
      "title": "2019 -(09-09 to 09-20)",
      "content": "<p><strong>Works&nbsp;Done:</strong></p>\r\n\r\n<p><strong>Cassandra Using Spark</strong></p>\r\n\r\n<p>-&nbsp;&nbsp;Study about Cassandra.&nbsp;Install Cassandra Cluster and create keyspace and work with tables.</p>\r\n\r\n<p>-&nbsp;Study authentication and authorization using roles.</p>\r\n\r\n<p>- Study if Aggregations / joins possible in Cassandra? (Complex agg. not possible but supports UDF). Try some CQL aggreg. query.</p>\r\n\r\n<p>-&nbsp;Research ODBC connection Cassandra for aggregations. (X)</p>\r\n\r\n<p>- Study about Spark.&nbsp;Install Spark in localmode.</p>\r\n\r\n<p>-&nbsp;Load csv file in Spark and run some queries &amp; aggregations using SQL queries.</p>\r\n\r\n<p>-&nbsp;Study Spark connector for cassandra.&nbsp;Remove dead Cassandra node from cluster.</p>\r\n\r\n<p>-&nbsp;Setup SparkSession, read tables in cassandra cluster and perform queries (join).</p>\r\n\r\n<p>-&nbsp;Find methods of bulk load in cassandra. (SStableloader) &nbsp;</p>\r\n\r\n<p>-&nbsp;Bulk load multiple csv files (12 GB) into cassandra table using spark. (Row count 40mil. Queries take too long to execute (approx on average 10 mins using spark casssandra conenctor in a single node.)</p>",
      "date": "2019-09-23"
    }
  },
  {
    "model": "documents.createnewdocuments",
    "pk": 41,
    "fields": {
      "created_by": 23,
      "title": "2019 -(09-23 to 10-04)",
      "content": "<p><strong>Works&nbsp;Done:</strong></p>\r\n\r\n<p><strong>Cassandra Using Spark &amp; Hbase using Spark:</strong></p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p>- Study and configure Apache Phoenix</p>\r\n\r\n<p>-&nbsp;Study fat-client and thin-client</p>\r\n\r\n<p>-&nbsp;Connect both database (Hbase and Cassandra) using spark and test amout of time taken to load database&nbsp;and execute different queries. (4&nbsp;mil rows approx) (Test was exectued on a single node)</p>\r\n\r\n<p>- Create 3 node cluster&nbsp;and configure Cassandra, Hadoop, Hbase, Spark and&nbsp;Phoenix in them.</p>\r\n\r\n<p>-&nbsp;&nbsp;Connect both database (Hbase and Cassandra) using spark and test amout of time taken to load database&nbsp;and execute different queries. (11&nbsp;mil rows approx) (Test was executed on a 3 node cluster)</p>\r\n\r\n<p>-&nbsp;(Link to report Cassandra vs Hbase)&nbsp;https://office.ekbana.info/products/files/doceditor.aspx?fileid=3038&amp;doc=ek16c2hvY3BtNmd4ZFVqbGZsNVFQbTR1YzB3cXBjalZWSkdmWWgwRUVpMD0_IjMwMzgi0</p>\r\n\r\n<p>- Issues faced: Constant timeouts when running queries in hbase. Solution: Added inceased timeout parameters in hbase-site.xml&nbsp;</p>\r\n\r\n<p>Similar issue of multiple file bulk load faced with hbase. i.e no pre built tool to load &quot;multiple&quot; CSV files into database at once. However,&nbsp;solved easily&nbsp;by either writing a shell scipt or creating a spark job, opted for the latter.</p>\r\n\r\n<p>- Studied methods of snapshot in Cassandra</p>\r\n\r\n<p>- Create a&nbsp; new 3 node Cassandra test cluster to check if snapshot are restored effectively and efficiently.</p>\r\n\r\n<p>- Tried snapshot method of backing up and restoring data into a new Cluster in Cassandra.</p>\r\n\r\n<p>Issue Faced: Manual backup i.e simple copy paste of snapshot into new Cassandra Cluster did not load all the data.&nbsp;</p>\r\n\r\n<p>100% of data did not reside in one node, data was distributed among 3 nodes due to replication factor being 1 which made it harder to load data into tables accordingly.&nbsp;</p>\r\n\r\n<p>Findings: Cassandra deploys a tool (SSTableloader) with itself which reloads the snapshots into table accordingly but before, create snapshots of entire keyspaces which includes its schemas and also the schema of all the column families in the keyspace. (Need not create keyspace and column families&nbsp;manually in the new cluster, just go and deploy snapshot of keyspace and load snapshot of cf using sstableloader)</p>",
      "date": "2019-10-10"
    }
  },
  {
    "model": "documents.createnewdocuments",
    "pk": 43,
    "fields": {
      "created_by": 21,
      "title": "13th Report",
      "content": "<h1>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Weekly Report</h1>\r\n\r\n<p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Submitted by: Dipesh Shrestha</p>\r\n\r\n<p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Date: 14th Octobor, 2019</p>\r\n\r\n<h3><strong>1. Deployment of Nepali Spelling Correction beta version</strong>&nbsp;</h3>\r\n\r\n<p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;This&nbsp;&nbsp;version is successfully deployed in the server 82 with public access ip 110.44.123.96:11014. The first is simplest version to&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;be tested for the improvement and analysis of failure purpose.</p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p><strong>Task Remaining:</strong>&nbsp; The UI design and JS part is still under progress and to be completed soon. Setting up separate Mongo Database for this version is remaning.</p>\r\n\r\n<p><strong>2. Documentation of Projects</strong></p>\r\n\r\n<p><strong>&nbsp; &nbsp; &nbsp; &nbsp; </strong>The documentation of <strong>VoicePlayerPi- whereis </strong>and <strong>Nepali Spelling Correction</strong>&nbsp;is shared in only office. The possible issue and guidlines of project installation details and deployed details are included and with the web hook configuration.</p>\r\n\r\n<p>&nbsp;</p>",
      "date": "2019-10-14"
    }
  },
  {
    "model": "documents.createnewdocuments",
    "pk": 44,
    "fields": {
      "created_by": 13,
      "title": "2019-10-14",
      "content": "<h1>Task 1</h1>\r\n\r\n<h2>Description&nbsp;</h2>\r\n\r\n<p>Creating schema for HGI data. so far 42 schemas were created</p>\r\n\r\n<p><strong>Time Taken</strong> : 10 days</p>\r\n\r\n<p><strong>Problem faced</strong> : None</p>\r\n\r\n<h1>Task 2</h1>\r\n\r\n<h2>Description</h2>\r\n\r\n<p>Creating reporting based on blueprint provided by .NET team. We had to create report for monthly transaction,premium transaction along with 5 other reports.</p>\r\n\r\n<p>Started Re-insurance reporting.Almost done</p>\r\n\r\n<p><strong>Problem faced</strong> : None</p>\r\n\r\n<p><strong>Time Taken&nbsp;</strong>: 10 days</p>",
      "date": "2019-10-14"
    }
  },
  {
    "model": "documents.createnewdocuments",
    "pk": 45,
    "fields": {
      "created_by": 23,
      "title": "2019 -(10-10 to 10-18)",
      "content": "<p><strong>Works&nbsp;Done:</strong></p>\r\n\r\n<p><strong>Cassandra:</strong></p>\r\n\r\n<p>- Install Cassandra in Develepment Server (30, 31, 32, 33)</p>\r\n\r\n<p>- Check if Cassandra table needs to be truncated to load data into table incase&nbsp;table gets corrupted.&nbsp;</p>\r\n\r\n<p>(No need to truncate, backup will load using SStableloader without truncate. Incase even if we truncate, cassandra will automatically create a snapshot of the truncated table. *we can disable this auto snapshot feature if needed)</p>\r\n\r\n<p>- Study RLAC, Create report on how to set roles in Cassandra. https://office.ekbana.info/products/files/doceditor.aspx?fileid=3060&amp;doc=cGpaNFQyR3V5TkZPcWVDYnR0ckx0Y0kyYmQzbVErZlJ2UytSbkEwaklaYz0_IjMwNjAi0</p>\r\n\r\n<p>- Study Cassandra Audit Logging. Cassandra 4.0 will support audit logging. Current Cassandra version (3.11.4).</p>\r\n\r\n<p>Found a plugin to implement Audit Logging. Installed and working but need to fix configurations.</p>\r\n\r\n<p>- Create Cassandra Snapshot Docs. https://office.ekbana.info/products/files/doceditor.aspx?fileid=3059&amp;doc=QS9IaHdZa2NFMndYNWs5bFVzTWJkL2tqNmRlaUduVVcyMWErdjJMQ1d2QT0_IjMwNTki0</p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p><strong>Hbase:</strong></p>\r\n\r\n<p>- Configure yarn in HA mode and install spark cluster. Haven&#39;t tested spark on yarn yet.</p>\r\n\r\n<p>- Export to csv using spark&nbsp;resulted in data loss.</p>\r\n\r\n<p>&nbsp;</p>",
      "date": "2019-10-21"
    }
  },
  {
    "model": "documents.createnewdocuments",
    "pk": 46,
    "fields": {
      "created_by": 3,
      "title": "2019-(10-10 to 10-18)",
      "content": "<p><strong>Asseto Corsa</strong></p>\r\n\r\n<p>Completed the task required by the client. Fetched car information and best lap for each lap.</p>\r\n\r\n<p><strong>Lunch Bot</strong></p>\r\n\r\n<p>Adding lunch bot to our Django CMS. Works relating to weely lunch remaining.</p>\r\n\r\n<p><strong>Bigmart</strong></p>\r\n\r\n<p>Schema re-design.</p>\r\n\r\n<p><strong>Cassandra/Data Security</strong></p>\r\n\r\n<p>Discussion for the data security implentation in&nbsp;our bigdata platform. Studying Transparent Data encryption for cassandra.</p>\r\n\r\n<p>&nbsp;</p>",
      "date": "2019-10-18"
    }
  },
  {
    "model": "documents.createnewdocuments",
    "pk": 47,
    "fields": {
      "created_by": 3,
      "title": "2019-(10-21 to 10-31)",
      "content": "<p><strong>Cassandra</strong></p>\r\n\r\n<p>Used <strong>CassandraConnector</strong> with <strong>spark</strong> to fetch from a table with specific&nbsp;query. Using only spark and cassandra was not feasible for our application, spark retrives all the data from table and then the query is executed which will increase the throughput as well as lagtime if a simple query is sent by the user, for e.g. selecting only first 10 rows of the table from 1 million records.</p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p><strong>Kafka Source Connector</strong></p>\r\n\r\n<p>Trying to modify source connector to mask/encrypt some columns data for data security. There are two connectors that can be helpful for our requirement.</p>\r\n\r\n<p>i. JDBC Source Connector [1]</p>\r\n\r\n<p>ii. Kafka Connect Phoenix (dhananjay patkar) [2]</p>\r\n\r\n<p>During development of the JDBC connector we sometimes use a SNAPSHOT version of Kafka in order to build and test against new features. If we want to build a development version, we&nbsp;may need to build and install these dependencies to our local Maven repository in order to build the connector:</p>\r\n\r\n<p><strong>&nbsp; &nbsp; .</strong>&nbsp;Kafka</p>\r\n\r\n<p><strong>&nbsp; &nbsp; .</strong>&nbsp;Common&nbsp;</p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p><strong>Luncbot</strong></p>\r\n\r\n<p>Added some remaining features such as download and&nbsp;some external apis. Code modification as suggested by QA (shreetika)</p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p><strong>Reference</strong></p>\r\n\r\n<p>[1]&nbsp;https://github.com/confluentinc/kafka-connect-jdbc/wiki/FAQ</p>\r\n\r\n<p>[2]&nbsp;https://github.com/dhananjaypatkar/kafka-connect-phoenix</p>\r\n\r\n<p>&nbsp;</p>",
      "date": "2019-11-01"
    }
  },
  {
    "model": "documents.createnewdocuments",
    "pk": 48,
    "fields": {
      "created_by": 23,
      "title": "2019 -(10-21 to 11-01)",
      "content": "<p><strong>Works&nbsp;Done:</strong></p>\r\n\r\n<p><strong>Cassandra:</strong></p>\r\n\r\n<p>- Implement Cassandra Audit Logging with custom configurations.</p>\r\n\r\n<p>Issue Faced: Tried Using Spark,&nbsp;Audit Logs were large and messy.</p>\r\n\r\n<p>- Create Audit Logging Report. Link: https://office.ekbana.info/products/files/doceditor.aspx?fileid=3063&amp;doc=c1JoZkhyWktPOVlMREJ0QndVbytzelIvYXIyekpyY202b09vaE5Vamp3ST0_IjMwNjMi0</p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p><strong>Hbase:</strong></p>\r\n\r\n<p>- Run Spark on Yarn</p>\r\n\r\n<p>Issues Faced: Configurations issue (lack of useful documents) and memory issues (tested on own VM)</p>\r\n\r\n<p>-&nbsp;Dump Hbase tables using spark / Tried with a specific date range.&nbsp;</p>\r\n\r\n<p>- Search for alternative methods to dump Hbase tables.&nbsp;</p>\r\n\r\n<p>Finding: Hbase has a tool called export utility, but to use it, it&nbsp;needs to be configured and cluster has to undergo downtime.</p>\r\n\r\n<p>Issue Faced: Data loss /&nbsp;loss still persists even in small range of data.</p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p>&nbsp;</p>",
      "date": "2019-11-01"
    }
  },
  {
    "model": "documents.createnewdocuments",
    "pk": 49,
    "fields": {
      "created_by": 16,
      "title": "10Oct-1Nov_2019",
      "content": "<h1><strong>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Weekly Report</strong></h1>\r\n\r\n<h3><strong>1. Underwriting</strong></h3>\r\n\r\n<h4><strong>1.1 Reindex all portfolios&nbsp;</strong></h4>\r\n\r\n<p><u>Issues:</u></p>\r\n\r\n<p>&nbsp; a. Amount/Rate values were stored as string.</p>\r\n\r\n<p>&nbsp; b. Case sensitive search.</p>\r\n\r\n<p><u>Solutions:</u></p>\r\n\r\n<p>&nbsp; a. Manual conversion of amount/rate value to double.</p>\r\n\r\n<p>&nbsp; b. Addition of lowercase normalizer making it case insensitive search.</p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<h4><strong>1.2. Index remaining 8 portfolios</strong></h4>\r\n\r\n<p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;1.BankersBlanket</p>\r\n\r\n<p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;2.Fidelity</p>\r\n\r\n<p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;3.MHH</p>\r\n\r\n<p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;4.MIM</p>\r\n\r\n<p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;5.PHI</p>\r\n\r\n<p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;6.PublicLiability</p>\r\n\r\n<p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;7.ContractorAllRisk</p>\r\n\r\n<p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;8.Boiler</p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<h3><strong>2. Reinsurance</strong></h3>\r\n\r\n<h4><strong>2.1. Flat 10 tables:</strong></h4>\r\n\r\n<p>&nbsp; &nbsp; &nbsp; 1.Basic Bifurcates</p>\r\n\r\n<p>&nbsp; &nbsp; &nbsp; 2.Bifurcation Commissions</p>\r\n\r\n<p>&nbsp; &nbsp; &nbsp; 3.Brokers</p>\r\n\r\n<p>&nbsp; &nbsp; &nbsp; 4.Reinsurance Companies</p>\r\n\r\n<p>&nbsp; &nbsp; &nbsp; 5.Reinsurances</p>\r\n\r\n<p>&nbsp; &nbsp; &nbsp; 6.Reinsurer Distributions</p>\r\n\r\n<p>&nbsp; &nbsp; &nbsp; 7.RSMDT Bifurcates</p>\r\n\r\n<p>&nbsp; &nbsp; &nbsp; 8.ThirdParty Bifurcates</p>\r\n\r\n<p>&nbsp; &nbsp; &nbsp; 9.Treaty Configs</p>\r\n\r\n<p>&nbsp; &nbsp; &nbsp; 10.Treaty Portfolios</p>\r\n\r\n<h4><strong>2.2. Index reinsurance data</strong></h4>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<h3><strong>3. Report</strong></h3>\r\n\r\n<h4><strong>3.1. Autocompletion</strong></h4>\r\n\r\n<p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;- Using JQuery</p>\r\n\r\n<h4><strong>3.2.Bifurcate Reports:</strong></h4>\r\n\r\n<p>&nbsp; &nbsp; &nbsp; &nbsp; - All Portolio Details</p>\r\n\r\n<p>&nbsp; &nbsp; &nbsp; &nbsp; -&nbsp;Miscellaneous Details</p>\r\n\r\n<p>&nbsp; &nbsp; &nbsp; &nbsp;-&nbsp;Agriculture Details</p>\r\n\r\n<p>&nbsp;</p>",
      "date": "2019-11-01"
    }
  },
  {
    "model": "documents.createnewdocuments",
    "pk": 50,
    "fields": {
      "created_by": 13,
      "title": "2019-11-01",
      "content": "<h1><strong>Task 1&nbsp;</strong></h1>\r\n\r\n<h2>Work on HGI Schema and generated the report for monthly portfolio calculation</h2>\r\n\r\n<p><strong>Description</strong></p>\r\n\r\n<p>1) Corrected schemas for few classes</p>\r\n\r\n<p><strong>Time taken: 3&nbsp;days</strong></p>\r\n\r\n<h1>Task 2</h1>\r\n\r\n<h2>Server up time for KUO</h2>\r\n\r\n<p><strong>Description</strong></p>\r\n\r\n<p>1)&nbsp; calculated up time for server and added status for services</p>\r\n\r\n<p><strong>Time taken: 1 day</strong></p>",
      "date": "2019-11-01"
    }
  },
  {
    "model": "documents.createnewdocuments",
    "pk": 51,
    "fields": {
      "created_by": 3,
      "title": "2019-(11-04 to 11-15)",
      "content": "<p><strong>Works Done</strong></p>\r\n\r\n<p><strong>Column Masking</strong></p>\r\n\r\n<p><span style=\"background-color:transparent; color:#000000\">Jdbc Source Connector api fetches the records from specified db url, we wanted to modify its code in order to encrypt some of the columns which can be critical if its values are known to the developer or any other threat. First problem was to know the flow of how the data is passed to the topic defined, so that we can modify/encrypt the columns before sending to the topic. Some of the issues faced are:</span></p>\r\n\r\n<p><strong>Dependency Error</strong></p>\r\n\r\n<p><span style=\"background-color:transparent; color:#000000\">Jdbc Source Connector is dependent on two other projects, kafka and common (kafka). So, the pom.xml file was trying to find the kafka and common project locally which according to the documentation also had to be built&nbsp; locally for any feature addition or modification. But, although I built those projects locally it was not able to fetch all the dependency required. So, I removed all the reference to the other projects and also removed all the dependency and added only the dependency which was required for the project to run.</span></p>\r\n\r\n<p><strong>Column to Mask</strong></p>\r\n\r\n<p><span style=\"background-color:transparent; color:#000000\">A new key had to be added so that we can mask only those columns. I added &#39;column.mask&#39; key in the properties file and fetched all the columns name in a list.</span></p>\r\n\r\n<p><strong>Save Encrypted Data</strong></p>\r\n\r\n<p><span style=\"background-color:transparent; color:#000000\">I added new property file for the db url where we will be saving the encrypted data along with the original column data. Right now I am using Postgres for saving the encrypted data. While saving the records, the records had to be fetched from a structured schema and convert it into prepared statement.</span></p>\r\n\r\n<p><strong>Encrypt Records</strong></p>\r\n\r\n<p><span style=\"background-color:transparent; color:#000000\">At first I wanted to change the resultset that was fetched from the db, but doing so I would also change the records in the db, so the idea was dropped. Next I changed the structured schema but it required to change lots of file which also required to add new encrypted data to its schema and was more time consuming and not the best way for what we were trying to achieve. Finally I added the encrypted data in a list and defined its own schema&nbsp; and converted it to prepared statement. Now, the column is being encrypted and is saved in the postgres db as we had planned.</span></p>\r\n\r\n<p>&nbsp;\r\n<p><strong>Rest API</strong></p>\r\n</p>\r\n\r\n<p><span style=\"background-color:transparent; color:#000000\">Due to some issues to get the direct access from the clients db we need to make a rest api from which we will be fetching the records instead from the db. So, all the code modification done in Jdbc should be changed in the API connector.</span></p>\r\n\r\n<p>&nbsp;</p>",
      "date": "2019-11-18"
    }
  },
  {
    "model": "documents.createnewdocuments",
    "pk": 52,
    "fields": {
      "created_by": 23,
      "title": "2019 -(11-04 to 11-15)",
      "content": "<p><strong>Works&nbsp;Done:</strong></p>\r\n\r\n<p><strong>DB Related:</strong></p>\r\n\r\n<p><strong>-&nbsp;</strong>Create Jar to dump csv and dump all Hbase tables to csv and backup in 10.10.5.25:/nfs/hbase_backup</p>\r\n\r\n<p><strong>-&nbsp;</strong>Create application to bulk import file to Cassandra</p>\r\n\r\n<p>-&nbsp;Import bigmart data to Cassandra in development server.</p>\r\n\r\n<p><strong>-&nbsp;</strong>Study Bash/shell scripting. Write shell script to install cassandra.</p>\r\n\r\n<p>- Install Cassandra in production</p>\r\n\r\n<p>-&nbsp;Create cassandra.service</p>\r\n\r\n<p>- ssh connection set up on&nbsp;servers (Prod, Dev, Elastic.S) to be accessible only by a gateway (10.10.5.40)</p>\r\n\r\n<p>-&nbsp;Started Studying Docker</p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p>&nbsp;</p>",
      "date": "2019-11-18"
    }
  },
  {
    "model": "documents.createnewdocuments",
    "pk": 53,
    "fields": {
      "created_by": 13,
      "title": "2019-11-19",
      "content": "<h1>Task 1</h1>\r\n\r\n<h2>Data Pipeline for bigmart,gps</h2>\r\n\r\n<p><strong>Description</strong>:</p>\r\n\r\n<p>1) designed data pipeline for bigmart</p>\r\n\r\n<p>2) work on test api</p>\r\n\r\n<p>3) worked on new design for fetching gps data</p>\r\n\r\n<p><strong>Problem faced</strong>: None</p>\r\n\r\n<p><strong>Time taken </strong>: 10 days</p>\r\n\r\n<p>&nbsp;</p>",
      "date": "2019-11-19"
    }
  },
  {
    "model": "documents.createnewdocuments",
    "pk": 54,
    "fields": {
      "created_by": 27,
      "title": "2019 -(11-25 to 11-29)",
      "content": "<p><strong>Works Done:</strong></p>\r\n\r\n<p>1. Setup a django project in ubuntu</p>\r\n\r\n<p>2. Created a custom user model based on the AbstractBaseUser and BaseUserManager classes of django</p>\r\n\r\n<p>3. Created a simple API that used simple basic token authentication</p>\r\n\r\n<p>4. Studied about serializers and deserializers amd implemented a custom serializer that used a custom validator of its own</p>\r\n\r\n<p>5. Tried how sendgrid could be used to send e-mails from the application</p>\r\n\r\n<p>6. Studied about custom management commands in django and created a simple command(with optional argument) that extracted values from the database</p>\r\n\r\n<p>7. Learned the ways in which the admin interface of Django application can be customized</p>\r\n\r\n<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; a. Added new actions for certain table in admin interface</p>\r\n\r\n<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; b. Overriding the existing action functions such as delete_queryset</p>\r\n\r\n<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; c. Extending the admin templates</p>\r\n\r\n<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; d. Adding derived fields in admin interface that do not exist in the database table itself</p>\r\n\r\n<p>All these learning were implemented in the django project created earlier</p>\r\n\r\n<p>8. Installed postgres and migrated the database from sqllite to postgres</p>\r\n\r\n<p>9. Started researching about celery and redis</p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p><strong>&nbsp;</strong></p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p>&nbsp;</p>",
      "date": "2019-11-29"
    }
  },
  {
    "model": "documents.createnewdocuments",
    "pk": 55,
    "fields": {
      "created_by": 16,
      "title": "18_29_Nov_2019",
      "content": "<p><span style=\"color:#000000\"><strong>Weekly Report</strong></span></p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p><strong>Tasks:</strong></p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<ol>\r\n\t<li>\r\n\t<p><strong>GIS paper study:</strong></p>\r\n\t</li>\r\n</ol>\r\n\r\n<ul>\r\n\t<li>\r\n\t<p>An automated approach from GPS traces to complete trip information.</p>\r\n\t</li>\r\n\t<li>\r\n\t<p>A Trip Reconstruction Tool for GPS-based Personal Travel Surveys.</p>\r\n\t</li>\r\n\t<li>\r\n\t<p>Matching GPS Observations to Locations on a Digital Map.</p>\r\n\t</li>\r\n</ul>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<ol start=\"2\">\r\n\t<li>\r\n\t<p><strong>Implement UDF and Stored procedure using PostgreSQL</strong></p>\r\n\t</li>\r\n</ol>\r\n\r\n<table border=\"1\" cellpadding=\"1\" cellspacing=\"1\" style=\"width:500px\">\r\n\t<tbody>\r\n\t\t<tr>\r\n\t\t\t<td>create or replace function first_udf<br />\r\n\t\t\t(<br />\r\n\t\t\t&nbsp; &nbsp;userid integer<br />\r\n\t\t\t)<br />\r\n\t\t\treturns table<br />\r\n\t\t\t(<br />\r\n\t\t\t&nbsp; &nbsp;user_id integer,<br />\r\n\t\t\t&nbsp; &nbsp;ax text,<br />\r\n\t\t\t&nbsp; &nbsp;lat double precision,<br />\r\n\t\t\t&nbsp; &nbsp;lng double precision<br />\r\n\t\t\t)<br />\r\n\t\t\tlanguage plpgsql as $body$ begin return query<br />\r\n\t\t\tselect<br />\r\n\t\t\tcast(geodata.user_id as integer),<br />\r\n\t\t\tcast(geodata.ax as text),<br />\r\n\t\t\tcast(geodata.lat as double precision),<br />\r\n\t\t\tcast(geodata.lng as double precision)<br />\r\n\t\t\tfrom geodata<br />\r\n\t\t\twhere geodata.user_id=userid limit 10;<br />\r\n\t\t\tend;<br />\r\n\t\t\t$body$</td>\r\n\t\t</tr>\r\n\t</tbody>\r\n</table>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<ol start=\"3\">\r\n\t<li>\r\n\t<p><strong>Get familar with POSTGIS functions.</strong></p>\r\n\r\n\t<ol>\r\n\t\t<li>\r\n\t\t<p>ST_ClosestPoint</p>\r\n\t\t</li>\r\n\t\t<li>\r\n\t\t<p>ST_MakePoint</p>\r\n\t\t</li>\r\n\t\t<li>\r\n\t\t<p>ST_Intersects</p>\r\n\t\t</li>\r\n\t\t<li>\r\n\t\t<p>ST_Centroid</p>\r\n\t\t</li>\r\n\t</ol>\r\n\t</li>\r\n</ol>\r\n\r\n<p>&nbsp;</p>",
      "date": "2019-12-02"
    }
  },
  {
    "model": "documents.createnewdocuments",
    "pk": 56,
    "fields": {
      "created_by": 23,
      "title": "2019 -(11-18 to 11-29)",
      "content": "<h2><strong>Works&nbsp;Done:</strong></h2>\r\n\r\n<p>- Study Spring Boot Framework</p>\r\n\r\n<p>- Create API to fetch bigmart data. (Help: Saurav dai and Bibek dai).</p>\r\n\r\n<p>- Study NGINX. Configure NGINX.</p>\r\n\r\n<p>- Create tables and roles in cassandra to store masked bigmart data.</p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p>&nbsp;</p>",
      "date": "2019-12-02"
    }
  },
  {
    "model": "documents.createnewdocuments",
    "pk": 57,
    "fields": {
      "created_by": 13,
      "title": "2019-12-16",
      "content": "<h1>Task 1</h1>\r\n\r\n<h1>SHM</h1>\r\n\r\n<p><strong>Description</strong>:</p>\r\n\r\n<p>- had to add interfaces to ignore</p>\r\n\r\n<p>- had to check if defined port was open</p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p><strong>time taken</strong>:2 days</p>\r\n\r\n<p><strong>problem faced</strong>: needed general discussion</p>\r\n\r\n<h1>Task 2</h1>\r\n\r\n<h1>Bigmart data pipeline</h1>\r\n\r\n<p><strong>Description</strong></p>\r\n\r\n<p>- had to create api and deploy to access bigmart oracle db</p>\r\n\r\n<p>- maintain nginx server and provide security</p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p><strong>Problem faced</strong>: None</p>\r\n\r\n<p><strong>Time Taken</strong>: 10 days</p>\r\n\r\n<h1>Task 3</h1>\r\n\r\n<h2>Hashing and encryption&nbsp;</h2>\r\n\r\n<p><strong>Description</strong></p>\r\n\r\n<p>- data stored in cassandra was to be encrypted or hashed</p>\r\n\r\n<p><strong>problem faced</strong> : it was hard to define workflow since storage format was not confirmed</p>\r\n\r\n<p><strong>Solution</strong> : all values were hashed and encrypted</p>\r\n\r\n<p><strong>time taken</strong> : 10 days</p>\r\n\r\n<h1>Task 4</h1>\r\n\r\n<h2>Encryption in proxy</h2>\r\n\r\n<p><strong>Description</strong>&nbsp;</p>\r\n\r\n<p>Response from server was to be intercepted and encrypted before sending to client</p>\r\n\r\n<p><strong>problem faced</strong> : unable to intercept response till now</p>\r\n\r\n<p><strong>time taken :</strong>&nbsp;3 days and still going</p>\r\n\r\n<p>&nbsp;</p>",
      "date": "2019-12-02"
    }
  },
  {
    "model": "documents.createnewdocuments",
    "pk": 58,
    "fields": {
      "created_by": 27,
      "title": "2019 -(12-09 to 12-13)",
      "content": "<p><strong>Works Done:</strong></p>\r\n\r\n<p>1. Studied about celery and rabbitmq server along with daemon tasks and periodic tasks</p>\r\n\r\n<p>2. Studied how mails are sent using the send-grid api</p>\r\n\r\n<p>3. Implemented sending emails from web application using celery and rabbitmq server</p>\r\n\r\n<p>4. Studied the official documentation of stripe and implemented it in an application and made some test payments</p>\r\n\r\n<p>5. The Unplug project was set up on my machine</p>\r\n\r\n<p>6. Mapped all the API endpoints which were called from each page of the Unplug website</p>\r\n\r\n<p>7. Tried understanding how the requests to the api endpoints affected the database</p>\r\n\r\n<p>8. Studied how sessions are maintained while watching videos on unplug</p>\r\n\r\n<p>9. Observed how the web application models made connection with actual objects on stripe gateway</p>\r\n\r\n<p>10. Studied the policy of Unplug regarding team-subscription, gift-subscriptions, coupons and promo-codes</p>\r\n\r\n<p>11. Studied how the admin analytics page is designed and what data it requires</p>\r\n\r\n<p>12. Studied how to write raw sql queries in django usinig django.db.connection module and also learned about sub-query refactoring</p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p><strong>Issues Faced:</strong></p>\r\n\r\n<p>1. The Unplug project seemed very overwhelming at the beginning as it implements a lot of technologies which i was not familiar with. I was quite unsure about how to try and understand the project</p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p><strong>Solution:</strong></p>\r\n\r\n<p>I took advice from my immediate supervisor about how to approach the Unplug project. I asked questions to him to try and understand the dynamics of the project. His guidance has been very helpful for me</p>",
      "date": "2019-12-13"
    }
  },
  {
    "model": "documents.createnewdocuments",
    "pk": 59,
    "fields": {
      "created_by": 3,
      "title": "2019-(11-18 to 12-16)",
      "content": "<p><strong>Data Dump from Oracle to Cassandra</strong></p>\r\n\r\n<ol>\r\n\t<li style=\"list-style-type:decimal\">\r\n\t<p><strong>Using Kafka:</strong></p>\r\n\t</li>\r\n</ol>\r\n\r\n<p style=\"margin-left:36pt\"><span style=\"background-color:transparent; color:#000000\">While fetching data from oracle we had to use api which returns json value with request size larger than 1 MB. So, in order to receive such huge data we needed to adjust some of the settings such as </span><span style=\"background-color:#a4c2f4; color:#000000\">max.request.size</span><span style=\"background-color:transparent; color:#000000\">, </span><span style=\"background-color:#a2c4c9; color:#000000\">fetch.message.max.bytes</span><span style=\"background-color:transparent; color:#000000\"> and </span><span style=\"background-color:#a2c4c9; color:#000000\">message.max.bytes</span><span style=\"background-color:transparent; color:#000000\">. But even after adjusting those settings the default value i.e. 1 MB did not change, and due to project deadline issue we did not get time to fix that issue and had to stop the work in kafka.&nbsp;</span></p>\r\n\r\n<ol start=\"2\">\r\n\t<li style=\"list-style-type:decimal\">\r\n\t<p><strong>Using JAVA API:</strong></p>\r\n\t</li>\r\n</ol>\r\n\r\n<p style=\"margin-left:36pt\"><span style=\"background-color:transparent; color:#000000\">Since kafka was stopped we had to find another way to fetch data from oracle and sink it to cassandra and the deadline was met, we wrote an api to fetch data from bigmart server with </span><span style=\"background-color:#a2c4c9; color:#000000\">encryption</span><span style=\"background-color:transparent; color:#000000\"> enabled.</span></p>\r\n\r\n<p style=\"margin-left:36pt\">&nbsp;</p>\r\n\r\n<p><strong>Encryption and Decryption</strong></p>\r\n\r\n<ol>\r\n\t<li style=\"list-style-type:decimal\">\r\n\t<p><strong>Decryption</strong></p>\r\n\t</li>\r\n</ol>\r\n\r\n<p style=\"margin-left:36pt\"><span style=\"background-color:transparent; color:#000000\">While fetching data from bigdata server/API we get the data in encryption form so at first we need to decrypt the data and start the encryption/hashing in our system.</span></p>\r\n\r\n<ol start=\"2\">\r\n\t<li style=\"list-style-type:decimal\">\r\n\t<p><strong>Hashing&nbsp;</strong></p>\r\n\t</li>\r\n</ol>\r\n\r\n<p style=\"margin-left:36pt\"><span style=\"background-color:transparent; color:#000000\">At first we had decided to hash some particular columns and put the hash and original data somewhere else such as postgres database. But programmers had to use two connections in order to fetch any data, so for that reason we had to stop the idea of hashing the data.</span></p>\r\n\r\n<ol start=\"3\">\r\n\t<li style=\"list-style-type:decimal\">\r\n\t<p><strong>Encryption</strong></p>\r\n\t</li>\r\n</ol>\r\n\r\n<p style=\"margin-left:36pt\"><span style=\"background-color:transparent; color:#000000\">In encryption we at first decided to encrypt whole data and store it in cassandra and only few users were given the decryption key, but doing so we need to modify the sql in runtime which was not an appropriate way. So, after some discussion with the team and project lead we decided not to encrypt the data in database but encrypt the data while sending it to the user/programmer and the user will decrypt the result if he/she has the decryption key. </span></p>\r\n\r\n<p style=\"margin-left:36pt\">&nbsp;</p>\r\n\r\n<p><strong>Applayer and RestCache Modification</strong></p>\r\n\r\n<p><span style=\"background-color:transparent; color:#000000\">We had to modify both applayer and restcache for the encryption of the data. We have defined a new end point for the encryption data which will be used for only bigmart, and decryption will be done in user side which is already tested and is working fine. But recently we decided to not encrypt the data from our side, rather we will encrypt the data in nginx layer which will be executed by sysadmin.</span></p>\r\n\r\n<p>&nbsp;</p>",
      "date": "2019-12-16"
    }
  },
  {
    "model": "documents.createnewdocuments",
    "pk": 60,
    "fields": {
      "created_by": 23,
      "title": "2019 -(12-02 to 12-13)",
      "content": "<p><strong>Works Done:</strong></p>\r\n\r\n<p>- Study ElasticSearch</p>\r\n\r\n<p>- Install ES Cluster and Kibana&nbsp;in local.</p>\r\n\r\n<p>- Upgrade dev server ES Cluster from 6.7 to 7.5</p>\r\n\r\n<p>Issue Faced: Different configration parameter names in 6.7 and 7.5. Had to compare with old configurations and&nbsp;upgrade&nbsp;ES cluster.</p>\r\n\r\n<p>- Create Index for bigmart table and fetch data from Prod Cassandra and insert into ES dev.</p>\r\n\r\n<p>- Check query time. First created index with data type as &quot;text&quot; which took comparatively longer time than creating with &quot;keyword&quot;. (Tokenization). Query returned in ms.</p>\r\n\r\n<p>- Modify the offset code in bigmart fetch api. Offset was incorrectly returned. Made minor modifcation to return correct offset.</p>",
      "date": "2019-12-16"
    }
  },
  {
    "model": "documents.createnewdocuments",
    "pk": 61,
    "fields": {
      "created_by": 28,
      "title": "2076-09-01",
      "content": "<p><small><strong>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Weekely report</strong></small></p>\r\n\r\n<p><small><strong>1. Kafka Architecture</strong></small></p>\r\n\r\n<p><small>It consist&nbsp;of:-</small></p>\r\n\r\n<p><small><strong>a) Producers</strong></small></p>\r\n\r\n<p><small>- producers writes to topic</small></p>\r\n\r\n<p><small><strong>b) Consumers</strong></small></p>\r\n\r\n<p><small>- Consumers read from topics</small></p>\r\n\r\n<p><small><strong>c) Topics</strong></small></p>\r\n\r\n<p><small>- Stream of records</small></p>\r\n\r\n<p><strong><small>2. Why Kafka is&nbsp;preferred?</small></strong></p>\r\n\r\n<p><small>- reliable</small></p>\r\n\r\n<p>- higher throughput</p>\r\n\r\n<p>- higher replication characteristics</p>\r\n\r\n<p><strong>3. ElasticSearch</strong></p>\r\n\r\n<p>- Open source distributed search engine built on top of Apache Lucene used for all types of data icluding textual, numerical, geospatial, structured and unstructured.</p>\r\n\r\n<p>- Java based</p>\r\n\r\n<p><strong>4. How does Elasticsearch works?</strong></p>\r\n\r\n<p>- Raw data flows in ElasticSearch from various sources</p>\r\n\r\n<p>- Raw data is parsed, normalized and enriched before it is indexed in ElasticSearch by Data ingestion process</p>\r\n\r\n<p>- Once indexed users can run complex queries against their data anduse aggregations to retrieve summaries of their data</p>\r\n\r\n<p><strong>5. KIbana</strong></p>\r\n\r\n<p>- From kibana users can create powerful visualizations of their data, share dashboards</p>\r\n\r\n<p>- Provides realtime&nbsp; histograms, line graph, pie charts,maps</p>\r\n\r\n<p><strong>6. ElasticSearch index</strong></p>\r\n\r\n<p>- Collection of documents</p>\r\n\r\n<p><strong>7. Inverted Index&nbsp;</strong></p>\r\n\r\n<p>- Allows fast full-text search</p>\r\n\r\n<p>- Lists every unique word that appears in any document and identify all of the document each words occurs in</p>\r\n\r\n<p><strong>8. Apache Lucene</strong></p>\r\n\r\n<p>- Full text search library in java</p>\r\n\r\n<p>- Fast because instead of searching text directly, it searches the index</p>\r\n\r\n<p><strong>9. Sharding</strong></p>\r\n\r\n<p>- Index are split into elements konown as shards that are distributed accross multiple nodes</p>\r\n\r\n<p>- Methd of splitting and storing a single logical dataset in multiple databases</p>\r\n\r\n<p>- Necessary if dataset is too large to be stored in a single databases.</p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p>&nbsp;</p>",
      "date": "2019-12-17"
    }
  },
  {
    "model": "documents.createnewdocuments",
    "pk": 62,
    "fields": {
      "created_by": 23,
      "title": "2019 -(12-16 to 12-27)",
      "content": "<p><strong>Works Done:</strong></p>\r\n\r\n<p>- Study Shard Allocation and Segmentaion Merging ES.</p>\r\n\r\n<p>-&nbsp;Study Logstash, Filebeat</p>\r\n\r\n<p>- Setup Logstash and stash first event</p>\r\n\r\n<p>-&nbsp;Parsing Logs with Logstash</p>\r\n\r\n<p>-&nbsp;Get Logstash working with filebeat. Take Logs from different servers and store in different index in ES with custom filter.</p>\r\n\r\n<p>Issues Faced: ( Tried to filter and create template using filebeat. Unable to create index template with filebeat, filebeat only sends message as source and filter has to be applied by logstash. Cannot create index template without specifying an index)</p>\r\n\r\n<p>- Article:&nbsp;<a href=\"https://office.ekbana.info/products/files/doceditor.aspx?fileid=3599\">https://office.ekbana.info/products/files/doceditor.aspx?fileid=3599&amp;doc=NDlhczJySUp4Z3pDblQ1Z09DT1hTbUk2VHQzQVg0di9RcWxIOHczTS90dz0_IjM1OTki0</a></p>\r\n\r\n<p>- Upgrade Production Server&#39;s Elasticsearch cluster to 7.5</p>\r\n\r\n<p>- Take snapshot of ES cluster and store in Shared FS.</p>\r\n\r\n<p>- Minor code change in Bigmart Data Fetch Api. (validate endpoint return lpcardno).</p>",
      "date": "2019-12-30"
    }
  },
  {
    "model": "documents.createnewdocuments",
    "pk": 63,
    "fields": {
      "created_by": 16,
      "title": "Dec16_27_2019",
      "content": "<p><strong>Weekly Report</strong></p>\r\n\r\n<p><strong>GPS Data&nbsp;Analysis:</strong></p>\r\n\r\n<p>- QGIS visualization</p>\r\n\r\n<p>- Find clusters in&nbsp;other gps data.</p>\r\n\r\n<p>- Data reduction by finding centroid of cluster.</p>\r\n\r\n<p>- Create UDF for data processing.</p>\r\n\r\n<p>- Test time acquired by cluster calculation&nbsp;code.</p>",
      "date": "2019-12-30"
    }
  },
  {
    "model": "documents.createnewdocuments",
    "pk": 66,
    "fields": {
      "created_by": 28,
      "title": "Dec 31,2019",
      "content": "<p><strong>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Weekly Report</strong></p>\r\n\r\n<p><strong>1. Query DSL(Elastic Search)</strong></p>\r\n\r\n<p>i) Leaf Query Clauses</p>\r\n\r\n<p>-&nbsp;&nbsp;&nbsp;look for a particular value in a particular field, such as match, term&nbsp;or range&nbsp;queries.</p>\r\n\r\n<p>ii) Compound Query Clauses</p>\r\n\r\n<p>-&nbsp; wrap other leaf or compound queries and are used to combine multiple queries&nbsp;such as bool query.</p>\r\n\r\n<p><strong>2) Full text queries</strong></p>\r\n\r\n<p>a) match query:-&nbsp;Returns documents that match a provided text, number, date or boolean value.</p>\r\n\r\n<p>b) match_phrase query:- Analyzes the text and creates a&nbsp;<code>phrase</code>&nbsp;query out of the analyzed text.</p>\r\n\r\n<p>c) multi_match query:- Allow multi field queries.</p>\r\n\r\n<p>d) query_string query:-&nbsp;Supports the compact Lucene query string syntax&nbsp;allowing to specify AND|OR|NOT conditions and multi-field search within a single query string.(used by experts)</p>\r\n\r\n<p>e) Simple_query_string query:-&nbsp;Returns documents based on a provided query string, using a parser with a limited but fault-tolerant syntax.</p>\r\n\r\n<p><strong>3) Compound Queries</strong></p>\r\n\r\n<p>a) bool query:-&nbsp;matches documents matching boolean combinations of other queries.</p>\r\n\r\n<p>Occurrence type:</p>\r\n\r\n<p>- must</p>\r\n\r\n<p>- filter</p>\r\n\r\n<p>- should</p>\r\n\r\n<p>- must_not</p>\r\n\r\n<p>b)&nbsp;Returns documents matching a&nbsp;<code>positive</code>&nbsp;query while reducing the relevance score&nbsp;of documents that also match a&nbsp;<code>negative</code>&nbsp;query.</p>\r\n\r\n<p>Occurence type:</p>\r\n\r\n<p>- positive</p>\r\n\r\n<p>- negative</p>\r\n\r\n<p>- negative_boost</p>\r\n\r\n<p>c) dis_max query:-&nbsp;Returns documents matching one or more wrapped queries, called query clauses or clauses.</p>\r\n\r\n<p>Occurence type:</p>\r\n\r\n<p>- queries</p>\r\n\r\n<p>- tie_breaker</p>\r\n\r\n<p>d) function_score query:-&nbsp;allows &nbsp;to modify the score of documents that are retrieved by a query.</p>\r\n\r\n<p>Occurence type:</p>\r\n\r\n<p>- multiply</p>\r\n\r\n<p>- sum</p>\r\n\r\n<p>- avg</p>\r\n\r\n<p>- min</p>\r\n\r\n<p>- max</p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p><strong>4) Aggregations</strong></p>\r\n\r\n<p>&nbsp;- Provides&nbsp;with the ability to group and perform calculations and statistics (such as sums and averages) on &nbsp;data by using a simple search query.</p>\r\n\r\n<p>-&nbsp;Using aggregations, we can extract&nbsp;data we&nbsp;want by running the GET method in Kibana UI&rsquo;s Dev Tools.</p>\r\n\r\n<p>-&nbsp;Divided into four groups: bucket aggregations, metric aggregations, matrix aggregations and pipeline aggregations.</p>\r\n\r\n<p><strong>i) Bucket Aggregations:-&nbsp;</strong></p>\r\n\r\n<p>-&nbsp; Method of grouping documents.</p>\r\n\r\n<p>-&nbsp; Used for grouping or creating data buckets.</p>\r\n\r\n<p>-&nbsp; Buckets can be made on the basis of an existing field, customized filters, ranges, etc.</p>\r\n\r\n<p><strong>II) Metric Aggregations:-</strong></p>\r\n\r\n<p>- Helps in calculating the matrices from the field of aggregated documents values.</p>\r\n\r\n<p><strong>iii) Pipeline Aggregations:-</strong>&nbsp;</p>\r\n\r\n<p>- Takes input from the output results of other Aggregations.</p>\r\n\r\n<p><strong>iv) Matrix Aggregations:-</strong>&nbsp;</p>\r\n\r\n<p>- still in development phase.</p>\r\n\r\n<p>-&nbsp;work on more than one field and provide statistical results based on the documents utilized by the used fields.</p>",
      "date": "2019-12-31"
    }
  },
  {
    "model": "documents.createnewdocuments",
    "pk": 67,
    "fields": {
      "created_by": 13,
      "title": "last week_2020-01-13",
      "content": "<p><strong>1) SHM</strong></p>\r\n\r\n<p>a) added recovery mode for resources and services</p>\r\n\r\n<p>b) changed notification msg</p>\r\n\r\n<p>c) revamped project</p>\r\n\r\n<p><strong>2) GPS</strong></p>\r\n\r\n<p>a) api for POI</p>\r\n\r\n<p><strong>3) BIGMART</strong></p>\r\n\r\n<p>a) helped bibek and tchiring</p>\r\n\r\n<p>&nbsp;</p>",
      "date": "2020-01-13"
    }
  },
  {
    "model": "documents.createnewdocuments",
    "pk": 68,
    "fields": {
      "created_by": 23,
      "title": "2019/2020 -(12-30-19 to 10-01-20)",
      "content": "<h2><strong>Works&nbsp;Done:</strong></h2>\r\n\r\n<p>- Upgrade Production Elastic Search server to 7.5</p>\r\n\r\n<p>- Take whole cluster snapshot of development server in NFS: /10.10.5.11:/volume16/Bigmart-Elastic-Search</p>\r\n\r\n<p>- Study a little bit about threading,&nbsp;profiling,&nbsp;components of&nbsp;JVM, garbage collection and memory issues.</p>\r\n\r\n<p>- Cassandra, change bigmart data&nbsp;model and set replication to 3 to achieve failover tolerancy.</p>\r\n\r\n<p>- Create new endpoints in bigmart data fetch API:&nbsp;New endpoint fetches sales history of customers on per day basis. (query to fetch data with respect to lpcardno, help&nbsp;by: Saurav and Bibek dai).</p>\r\n\r\n<p>- Worked on Connector Sink API, modified bibek dai&#39;s code.</p>\r\n\r\n<p>Issue: Incase a store&#39;s data was not synced for a day in oracle DB our system would miss data for that particular store as we were fetching all data&nbsp;on per day basis. Now fetching sales data with respect to store and their last synced date. API will now update sites and their last synced dates in hsqldb&nbsp;and request data from bigmart fetch api&nbsp;respective of the store and their last fetched date.</p>",
      "date": "2020-01-13"
    }
  },
  {
    "model": "documents.createnewdocuments",
    "pk": 69,
    "fields": {
      "created_by": 28,
      "title": "10th jan, 2020",
      "content": "<p><strong>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Weekly Report</strong></p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p><strong>1. Pagination</strong></p>\r\n\r\n<p>- To use we ned to set the from and size parameters when we search.</p>\r\n\r\n<p>- Size &nbsp;-&gt; tells elasticsearch how many results to return.</p>\r\n\r\n<p>- From -&gt; tells elasticsearch how many to offset by for when we don&#39;t want the first page of results.</p>\r\n\r\n<p><strong>&nbsp; 2.&nbsp; Limitations of terms aggregation</strong></p>\r\n\r\n<p><strong>a) Size</strong></p>\r\n\r\n<p>- set to define how many term buckets should be returned out of the overall terms list</p>\r\n\r\n<p><strong>b) Documents count are approximate</strong></p>\r\n\r\n<p>- Documents count are not always accurate because each shard provides&nbsp;&nbsp;its own view of what the ordered list of terms should be and these are combined to give a final view</p>\r\n\r\n<p><strong>c) Calculating document count error</strong></p>\r\n\r\n<p>- Two error values can be shown on the terms aggregation</p>\r\n\r\n<p>- First gives a value for the aggregation as a whole which represents the maximum potential document count for a term which did not make it into the final list of terms which is calculated as the sum of the document count from the last term returned from each shard</p>\r\n\r\n<p><strong>d) Per bucket document count error</strong></p>\r\n\r\n<p>- Shows an error value for each term returned by the aggregation which represents the worst case&nbsp;error in the document count</p>\r\n\r\n<p>- Calculated by summing the document counts for the last term returned by all shards which did not return the term</p>\r\n\r\n<p><strong>e) Order</strong></p>\r\n\r\n<p>- Order of the buckets can be customized by setting the order parameter</p>\r\n\r\n<p>- By default, the buckets are ordered by their&nbsp;<code>doc_count</code>&nbsp;descending</p>\r\n\r\n<p><strong>f) Minimum Document Count</strong></p>\r\n\r\n<p>- min_doc_count option makes it possible to only return terms that match more than a configured number of hits&nbsp;</p>\r\n\r\n<p><strong>g) Filtering Values</strong></p>\r\n\r\n<p>- Makes&nbsp;possible to filter the values for which buckets will be created</p>\r\n\r\n<p>- Can be done using the include&nbsp;and exclude&nbsp;parameters which are based on regular expression strings or arrays of exact values</p>\r\n\r\n<p><strong>h) Multi-field Terms Aggregation</strong></p>\r\n\r\n<p><code>- Terms</code>&nbsp;aggregation does not support collecting terms from multiple fields in the same document</p>\r\n\r\n<p>- It&#39;s because the&nbsp;<code>terms</code>&nbsp;agg doesn&rsquo;t collect the string term values themselves, but rather uses global ordinals to produce a list of all of the unique values in the field</p>\r\n\r\n<p><strong>i) Collect Mode</strong></p>\r\n\r\n<p>- Deferring calculations of child aggregations</p>\r\n\r\n<p><strong>j) Execution hint</strong></p>\r\n\r\n<p>- Terms aggregations can be executed:</p>\r\n\r\n<ul>\r\n\t<li>by using field values directly in order to aggregate data per-bucket (<code>map</code>)</li>\r\n\t<li>by using global ordinals of the field and allocating one bucket per global ordinas</li>\r\n</ul>\r\n\r\n<p><strong>k) Missing Value</strong></p>\r\n\r\n<p>-&nbsp;Documents without a value in the&nbsp;field will fall into the same bucket as documents that have the value&nbsp;<code>N/A</code></p>\r\n\r\n<p><strong>l) Mixing field types</strong></p>\r\n\r\n<p>- When aggregating on multiple indices the types might be&nbsp;mix of decimal and non-decimal number, in such case the terms aggregation will promote the non-decimal numbers to decimal numbers resulting &nbsp;loss of precision in the bucket values.</p>",
      "date": "2020-01-13"
    }
  },
  {
    "model": "documents.createnewdocuments",
    "pk": 70,
    "fields": {
      "created_by": 21,
      "title": "19th Report",
      "content": "<h2>i) Spell Check:</h2>\r\n\r\n<hr />\r\n<p>-&nbsp; New architecture is deployed can be accessed using suddhanepali.com</p>\r\n\r\n<h2>ii) Accessing Database Using Singleton design Pattern</h2>\r\n\r\n<hr />\r\n<p>-&nbsp; A instance is created of the class and used without making anyother instance.&nbsp;</p>\r\n\r\n<p>- The same instance can be used over other file.</p>\r\n\r\n<h2>iii) Rasa Chatbot:</h2>\r\n\r\n<hr />\r\n<p>- Added intermediate function to retrieve price of vegetable from the api&nbsp;</p>\r\n\r\n<p>- Handling the issue of stories by making proper story</p>\r\n\r\n<p>- Augmentation testing for the present data</p>\r\n\r\n<p>&nbsp;</p>",
      "date": "2020-01-14"
    }
  },
  {
    "model": "documents.createnewdocuments",
    "pk": 71,
    "fields": {
      "created_by": 3,
      "title": "2020-(01-13 to 01-24)",
      "content": "<p><strong>Works Done</strong></p>\r\n\r\n<p><strong>Kafka Rest Source Connector</strong></p>\r\n\r\n<ul>\r\n\t<li>\r\n\t<p>Created a development and live environment for fetching sales_history based on registered lpcardno in mongodb collection.&nbsp;</p>\r\n\t</li>\r\n\t<li>\r\n\t<p>Sending notification on mattermost if any error occured after 3 three requests.&nbsp;</p>\r\n\t</li>\r\n\t<li>\r\n\t<p>Tested the availability of data from bigmart api to our system and stored the data fetched log.</p>\r\n\t</li>\r\n\t<li>\r\n\t<p>Added all the fields related to amount, created new schema for the records.</p>\r\n\t</li>\r\n\t<li>\r\n\t<p>Added the connector in production servers.</p>\r\n\t</li>\r\n</ul>\r\n\r\n<p><strong>Problem</strong></p>\r\n\r\n<p>Need to find a way to use QA for testing kafka connect api and other kafka related services.</p>\r\n\r\n<p><strong>Logstash</strong></p>\r\n\r\n<ul>\r\n\t<li>\r\n\t<p>Started working on logstash.</p>\r\n\t</li>\r\n\t<li>\r\n\t<p>Added count log in elasticsearch using logstash and filebeat.</p>\r\n\t</li>\r\n\t<li>\r\n\t<p>Sent a format and example of logging to bijendra so that all the bigmart related work is maintained through log and can visualize the usage in elasticsearch</p>\r\n\t</li>\r\n</ul>\r\n\r\n<p><strong>Video Streaming</strong></p>\r\n\r\n<p>After a small session with wowtime we started to work on the stream provided by wowtime. For video streaming we are using kafka and opencv to fetch the frames from video. And for audio&nbsp; now we are trying to use MediaPlayer (python library), but we have not integrated it with kafka. We are still looking for a better solution to send both video and audio frames in kafka topic&nbsp; and consuming and syncing it together.</p>",
      "date": "2020-01-27"
    }
  },
  {
    "model": "documents.createnewdocuments",
    "pk": 72,
    "fields": {
      "created_by": 23,
      "title": "2020 - (13-01 to 24-01)",
      "content": "<h2><strong>Works&nbsp;Done:</strong></h2>\r\n\r\n<p>- Create loggers&nbsp;(exceptions and fetch count) for bigmart data fetch api.</p>\r\n\r\n<p>-&nbsp;Install Kafka in production and create service for kafka.</p>\r\n\r\n<p>-&nbsp;Study Kafka</p>\r\n\r\n<p>-&nbsp;Install Kafka in local system for testing and learning.</p>\r\n\r\n<p>- Cassandra: Create a new keyspace, same tables but add new&nbsp;column&#39;s&nbsp;and change col data type. Issue: Change int to float.</p>\r\n\r\n<p>-&nbsp;Fix validate endpoint issue. Fetching null lpcardno values from table.</p>\r\n\r\n<p>-&nbsp;Create Kafka Producer, Topic and Consumer from CLI</p>\r\n\r\n<p>- Java program&nbsp;to create&nbsp;Kafka Producer, Topic and Consumer set offset, also&nbsp;insert into Elasticsearch (local test)</p>\r\n\r\n<p>- Use twitter api to fetch and display tweets real time using Kafka Producer, Consumer</p>\r\n\r\n<p>-&nbsp;Setup Logstash and Filebeat in dev server&nbsp;for demo.</p>\r\n\r\n<p>- Little basic study about kafka video-streaming and test streaming .mp4 video using opencv&nbsp;lib using only 1&nbsp;broker.</p>",
      "date": "2020-01-27"
    }
  },
  {
    "model": "documents.createnewdocuments",
    "pk": 73,
    "fields": {
      "created_by": 16,
      "title": "13_24_Jan_2020",
      "content": "<p>&nbsp;</p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<h1><span style=\"color:#000000\"><strong>Weekly Report</strong></span></h1>\r\n\r\n<h2>&nbsp;</h2>\r\n\r\n<h2><strong>Trip Analysis</strong></h2>\r\n\r\n<ul>\r\n\t<li>\r\n\t<p>Rerun full fledge trip analysis project</p>\r\n\t</li>\r\n\t<li>\r\n\t<p>Create support for multithreading</p>\r\n\t</li>\r\n\t<li>\r\n\t<p>Schedule project in cron</p>\r\n\t</li>\r\n\t<li>\r\n\t<p>Deployment</p>\r\n\r\n\t<p>&nbsp;</p>\r\n\t</li>\r\n</ul>\r\n\r\n<h3><strong>Issues:</strong></h3>\r\n\r\n<ol>\r\n\t<li>\r\n\t<p><strong>Spark sql interpretation problem</strong></p>\r\n\r\n\t<p>Solution: Update spark to <span style=\"color:#000000\">spark </span><span style=\"color:#000000\">version from 2.3.0 to 2.4.0.</span></p>\r\n\r\n\t<p>&nbsp;</p>\r\n\t</li>\r\n\t<li>\r\n\t<p><strong>getPOI</strong></p>\r\n\r\n\t<p>Solution: Rewrite nearest building logic.</p>\r\n\r\n\t<p>&nbsp;</p>\r\n\t</li>\r\n\t<li>\r\n\t<p><strong>Timestamp issue: Addition of +5:45 in database table</strong></p>\r\n\r\n\t<p>Solution: Change timestamp to unix value.</p>\r\n\r\n\t<p>&nbsp;</p>\r\n\t</li>\r\n\t<li>\r\n\t<p><strong>Termination of spark application while multithreading analysis jobs.</strong></p>\r\n\r\n\t<p>Solution: Terminate spark application after multithreading process ends.</p>\r\n\r\n\t<p>&nbsp;</p>\r\n\t</li>\r\n\t<li>\r\n\t<p><strong>Inavailability of POI of previous dates.</strong></p>\r\n\r\n\t<p>Solution: POI api with date and user value.</p>\r\n\r\n\t<p>&nbsp;</p>\r\n\t</li>\r\n\t<li>\r\n\t<p><strong>Info logger was not shown in log files.</strong></p>\r\n\r\n\t<p>Solution: Change logger class to Rolling Appender</p>\r\n\t</li>\r\n</ol>",
      "date": "2020-01-27"
    }
  },
  {
    "model": "documents.createnewdocuments",
    "pk": 74,
    "fields": {
      "created_by": 3,
      "title": "2020-(01-27 to 02-07)",
      "content": "<h2><strong>Bigmart</strong></h2>\r\n\r\n<p><strong>Listagg to Clob</strong></p>\r\n\r\n<p><span style=\"background-color:transparent; color:#000000\">We had a query that uses the listagg function to get all the rows of bill info as a semi-colon delimited string. After 1-2 weeks we got an error with some of the bill numbers which had lots of product purchased info.</span></p>\r\n\r\n<p><strong>Problem</strong></p>\r\n\r\n<p><span style=\"background-color:transparent; color:#000000\">SQL Error: </span><strong>ORA-01489</strong><span style=\"background-color:transparent; color:#000000\">: Result of string concatenation is too long.</span></p>\r\n\r\n<p><span style=\"background-color:transparent; color:#000000\">The problem is that the query being run to aggregate the data is returning so many rows that the string concatenation that listagg is doing violates the 4000 char limit.</span></p>\r\n\r\n<p><strong>Solution</strong></p>\r\n\r\n<p><span style=\"background-color:transparent; color:#000000\">Prior to Oracle RDBMS 12.1.0.2 the upper limit for VARCHAR2 was 4K. With Oracle RDBMS 12.1.0.2 this limit has been raised to 32K. This increase may solve a lot of issues but it does require a change to the database parameter MAX_STRING_SIZE. By setting MAX_STRING_SIZE = EXTENDED this enables the new 32767 byte limit. However, if we use </span><strong>Clob </strong><span style=\"background-color:transparent; color:#000000\">it has a limit of (4 gigabytes - 1) * (database block size), then more than the varchar2 with this </span><strong>maximum</strong><span style=\"background-color:transparent; color:#000000\"> of 4000 bytes.</span></p>\r\n\r\n<p><span style=\"background-color:transparent; color:#000000\">Though Clob solved the issue of ORA-01489, it returned the result with a single character separated with comma, for e.g. it would return &quot;Apple&quot; as &#39;A&#39;,&#39;P&#39;,&#39;P&#39;,&#39;L&#39;,&#39;E&#39;. So, we solved the Clob issue from the code itself, and now the issue has been resolved.</span></p>\r\n\r\n<p><strong>Delimiter [; to ;; and : to !]</strong></p>\r\n\r\n<p><span style=\"background-color:transparent; color:#000000\">Some of the data in oracle is saved with the delimiter we used to separate the row data with, but since the delimiter we used were also in the data, the result that we fetched got conflicted and was showing unexpected results. After changing the delimiter to ;; and ! the issue has been resolved. One of the main reasons for changing &#39;;&#39; to &#39;;;&#39; was the data in the records had &#39;&amp;amp;&#39; so we changed the delimiter to ;;.</span></p>\r\n\r\n<p><strong>Large Registered Users</strong></p>\r\n\r\n<p><span style=\"background-color:transparent; color:#000000\">From bigmart we are fetching sales history of each logged in users, so the amount of users are more than 1000 which was one of the issues we faced while fetching the record.</span></p>\r\n\r\n<p><strong>Problem</strong></p>\r\n\r\n<p><span style=\"background-color:transparent; color:#000000\">SQL Error: </span><strong>ORA-01795</strong><span style=\"background-color:transparent; color:#000000\">: maximum number of expressions in a list is 1000</span></p>\r\n\r\n<p><strong>Solution</strong></p>\r\n\r\n<p><span style=\"background-color:transparent; color:#000000\">We splitted the Logged in users with 500 chunks and sent the request multiple times to fetch the records.</span></p>\r\n\r\n<h2><strong>Logstash</strong></h2>\r\n\r\n<p><span style=\"background-color:transparent; color:#000000\">We are implementing the logstash to fetch the log of bigmart apis and sync that to elasticsearch for more logical visualization and also we are planning to send an alert to the concerned individuals if the api is slow or any kind of issue is detected.</span></p>\r\n\r\n<p><span style=\"background-color:transparent; color:#000000\">Currently, the log of bigmart dev-env is being synced to elasticsearch, in 1-2 weeks we will have a working solution for the alerts and different visualization for different projects.</span></p>\r\n\r\n<h2><strong>Kafka Stream</strong></h2>\r\n\r\n<p><span style=\"background-color:transparent; color:#000000\">One of the research we are doing is on streams, mostly we are concerned with detecting ads and also inserting ads in a video or audio. For video we are using opencv and for audio we are using librosa and ffmpeg.&nbsp;</span></p>\r\n\r\n<p><span style=\"background-color:transparent; color:#000000\">With librosa we were only able to stream stored audio so we moved to ffmpeg to stream the live feeds.&nbsp;</span></p>\r\n\r\n<p><span style=\"background-color:transparent; color:#000000\">We are also trying to send the ts file/bytes to the stream for both the audio and video using ffmpeg. For the audio part </span><strong>Dipesh </strong><span style=\"background-color:transparent; color:#000000\">is helping us, he is the one who suggested us to use the librosa library and he is also helping us to detect/play the sound using ffmpeg and ts file content.</span></p>\r\n\r\n<p><span style=\"background-color:transparent; color:#000000\">Currently, we are able to insert an ad into a stream but we are having some issues in playing audio and video in sync, we are working on that. After 1-2 days we will also start working in detecting the ad in video as well as in audio</span>.</p>\r\n\r\n<p>&nbsp;</p>",
      "date": "2020-02-09"
    }
  },
  {
    "model": "documents.createnewdocuments",
    "pk": 75,
    "fields": {
      "created_by": 28,
      "title": "February 10, 2019",
      "content": "<p>1. Reindexing and delete by query</p>\r\n\r\n<p>2. Studied&nbsp;about Logstash and filebeat configuration</p>\r\n\r\n<p>3. Configured logstash with filebeat&nbsp;</p>\r\n\r\n<p>- read log from filebeat and read send to logstash and from logstash sent to elasticsearch.</p>\r\n\r\n<p>-&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;<a href=\"http://office.ekbana.info/products/files/#986\">http://office.ekbana.info/products/files/#986</a></p>\r\n\r\n<p>4. Visaulization in kibana&nbsp;</p>\r\n\r\n<p>- visualized log data using different visualization types( pie, line, time series, map, vertical bar)</p>\r\n\r\n<p><img alt=\"\" src=\"/home/sajita/Pictures/Screenshot from 2020-02-12 10-18-08.png\" /><img alt=\"\" src=\"/home/sajita/Pictures/\" /></p>\r\n\r\n<p>- helps in understanding our data</p>\r\n\r\n<p>&nbsp;- based on aggregations performed by Elasticsearch</p>\r\n\r\n<p>- for eg: response_code&nbsp;is mapped as a string and not an integer, you will know that you cannot use metric aggregations with it in visualizations.</p>",
      "date": "2020-02-10"
    }
  },
  {
    "model": "documents.createnewdocuments",
    "pk": 76,
    "fields": {
      "created_by": 23,
      "title": "2020 - (27-01 to 07-02)",
      "content": "<h2><strong>Works&nbsp;Done:</strong></h2>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p>- Kafka Video-streaming: Using thread to read frames from video.</p>\r\n\r\n<p>Issue: Stream ends abruptly, cause stream sends lots of frames and waits.</p>\r\n\r\n<p>-&nbsp;Fix issue of stream ending abruptly.&nbsp;Consumers working in sync. Tested 5-6 consumers.</p>\r\n\r\n<p>-&nbsp;Bigmart multiple issues occured, sit with bbek bro.</p>\r\n\r\n<p>-&nbsp;Logstash and filebeat installation for bigmart backend bijen bro&#39;s api&#39;s logs.</p>\r\n\r\n<p>-&nbsp;Inject another video (ad) in topic kafka</p>\r\n\r\n<p>- Study and look&nbsp;into ffmepg documentation and m3u8 format.</p>\r\n\r\n<p>- Different streams have different resolutions and frame/rates, tried&nbsp;to make them&nbsp;dynamic.</p>\r\n\r\n<p>Solution found, to use m3u8 playlist and&nbsp;ts files which had audio and video bytes together.</p>\r\n\r\n<p>- Discuss the design model with bbek bro for demo and further working.</p>\r\n\r\n<p>&nbsp;</p>",
      "date": "2020-02-10"
    }
  },
  {
    "model": "documents.createnewdocuments",
    "pk": 77,
    "fields": {
      "created_by": 16,
      "title": "27Jan_8Feb_2020",
      "content": "<p>&nbsp;</p>\r\n\r\n<p><strong>Tasks</strong></p>\r\n\r\n<ul>\r\n\t<li>\r\n\t<p>Crontab</p>\r\n\t</li>\r\n\t<li>\r\n\t<p>Verify data</p>\r\n\t</li>\r\n\t<li>\r\n\t<p>Adjust data from timestamp to unix and id to uuid</p>\r\n\t</li>\r\n\t<li>\r\n\t<p>Trip analysis :</p>\r\n\r\n\t<ul>\r\n\t\t<li>\r\n\t\t<p>Get data from API.</p>\r\n\t\t</li>\r\n\t\t<li>\r\n\t\t<p>Get userllist from Postgres.</p>\r\n\t\t</li>\r\n\t</ul>\r\n\t</li>\r\n</ul>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p>Issue:</p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<ol>\r\n\t<li>\r\n\t<p>crontab in server</p>\r\n\r\n\t<ol>\r\n\t\t<li>\r\n\t\t<p>server system was in UTC.</p>\r\n\r\n\t\t<p>$ timedatectl</p>\r\n\t\t</li>\r\n\t\t<li>\r\n\t\t<p>environment variable is not taken in cron, it must be explicitly defined.</p>\r\n\r\n\t\t<p>$ 32 10 * * * . $HOME/.bashrc; /usr/bin/java -jar /usr/local/api_gps_analysis/target/api_gps_analysis-0.1.0.jar</p>\r\n\t\t</li>\r\n\t\t<li>\r\n\t\t<p>org.postgresql.util.PSQLException: FATAL: no pg_hba.conf entry for host &quot;68.183.91.197&quot;, user &quot;tracking&quot;, database &quot;org_skrfuejheb&quot;, SSL off</p>\r\n\r\n\t\t<ol>\r\n\t\t\t<li>\r\n\t\t\t<p>gps.conf:</p>\r\n\r\n\t\t\t<p>process_jdbc_url=&quot;jdbc:postgresql://localhost:5432/org_skrfuejheb&quot;</p>\r\n\r\n\t\t\t<p>&nbsp;</p>\r\n\t\t\t</li>\r\n\t\t</ol>\r\n\t\t</li>\r\n\t</ol>\r\n\t</li>\r\n\t<li>\r\n\t<p>AbstractQueuedSynchronizer.doAcquireSharedInterruptibly</p>\r\n\r\n\t<p>java.lang.InterruptedException</p>\r\n\tat java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedInterruptibly(AbstractQueuedSynchronizer.java:998)</li>\r\n\t<li>\r\n\t<p>Solution:</p>\r\n\t</li>\r\n</ol>\r\n\r\n<ul>\r\n\t<li>\r\n\t<p>Increase memory in choto tracking server.</p>\r\n\t</li>\r\n\t<li>\r\n\t<p>Java multithreading</p>\r\n\r\n\t<p>&nbsp;</p>\r\n\t</li>\r\n</ul>\r\n\r\n<ol start=\"3\">\r\n\t<li>\r\n\t<p>Change ts to timestamp field and id to uniqueid.</p>\r\n\t</li>\r\n</ol>\r\n\r\n<ul>\r\n\t<li>\r\n\t<p>Edit overall project flow.</p>\r\n\t</li>\r\n\t<li>\r\n\t<p>Create store procedure.</p>\r\n\t</li>\r\n</ul>",
      "date": "2020-02-11"
    }
  },
  {
    "model": "documents.createnewdocuments",
    "pk": 78,
    "fields": {
      "created_by": 3,
      "title": "2020-(02-10 to 02-21)",
      "content": "<p><strong>Bigmart</strong></p>\r\n\r\n<p><strong>Data sync</strong></p>\r\n\r\n<p>New view table collection was given to us for the extra information on bills such as mode of payment and the cashier name also tax information was given in the sales table.&nbsp; While making the index for bill punched daily, we found out that the data in the collection table is more than the sales table which is populated after a record is synced in the collection table. Since, the records were more than 1k in a day which was missing it could be an issue that can affect the customers engagement in the app.</p>\r\n\r\n<p>We have informed Ajeet sir about the issue and he told us he will talk with Ashok sir about it, so right now we have been working with the amount of data that is synced in the sales table.</p>\r\n\r\n<p><strong>Indexing</strong></p>\r\n\r\n<p><strong><img src=\"https://lh5.googleusercontent.com/s_5rO0G6VQt0ktUr6r2mq6m0ukPt7RrCoLL29oqkwLFvVPdqrTe_51ZRuj3VbeksZNTX5gczJ65DJplufr2d2M234t6yuwfHTFZAUPAAFnjZbaqfJkKF2mHAPzVWRDH8pxiODuhd\" style=\"height:377px; width:294px\" /></strong></p>\r\n\r\n<p>Sales amount calculation and bill punched count indexing was done by Sajita, but due to large data size in bill punched it was showing bucket too large and response time out error. So, I decided to index the bill punched count from the code in a separate index. The issue was due to multiple aggregations which were taking time and also the result size. I have provided the new index to Prajwal bati&nbsp; and Pratik, now they will be doing data validation and will inform us if any further work is to be done.</p>\r\n\r\n<p><strong>Logstash</strong></p>\r\n\r\n<p>We had a small session with Jeeten sir and other developers about the implementation of logstash in their ongoing projects. So, right now in almost 6 projects developers have implemented the log format that we had provided. Tchiring also gave a session to Sachin on how to get log data from filebeat to logstash and then to elasticsearch. So, from now on Sachin will be making a service for the filebeat in all the projects.</p>\r\n\r\n<p><strong>Kafka Stream</strong></p>\r\n\r\n<p>A small demo was shown to the Wowtime team on Ad injection and gave us some feedback to show a stream in two windows and only inject Ad on one window and also add a skip button for the Ad. Faraz and Tchiring are working on the streams and after the demo is ready we will discuss with the Wowtime team about further works.</p>\r\n\r\n<p>Also, Dipesh and Sharmila are working on the Audio processing (audio to text).</p>\r\n\r\n<p><br />\r\n&nbsp;</p>",
      "date": "2020-02-24"
    }
  },
  {
    "model": "documents.createnewdocuments",
    "pk": 79,
    "fields": {
      "created_by": 23,
      "title": "2020 - (10-02 to 21-02)",
      "content": "<h2><strong>Works&nbsp;Done:</strong></h2>\r\n\r\n<p>-&nbsp;Fix data error in elasticsearch (unable to use&nbsp;query on timestamp data). Date format was not matching. Reinsert bigmart-live log data.</p>\r\n\r\n<p>-&nbsp;Provide faraz with inject ad kafka project so he could design frontend for it.</p>\r\n\r\n<p>-&nbsp;Demo&nbsp;wowtime.</p>\r\n\r\n<p>-&nbsp;Changes while injecting ad. User should be able to skip ad and stream should not pause while ad is playing. (Credit - Faraz)</p>\r\n\r\n<p>- Extract audio bytes from m3u8 playlist stream and play using AudioSegment lib.</p>\r\n\r\n<p>-&nbsp;Get audio and video bytes together without ts file.</p>\r\n\r\n<p>-&nbsp;Minor changes in bigmart-data-fetch api. Some columns were missing when fetching data. Added column names&nbsp;in&nbsp;query.</p>\r\n\r\n<p>-&nbsp;Breifing Session to Sachin and Sandesh about elasticsearch and logstash.</p>\r\n\r\n<p>-&nbsp;Issue Faced: Unable to play combined audio and video byte stream. (Player was unable to recognize byte format)</p>\r\n\r\n<p>&nbsp;</p>",
      "date": "2020-02-24"
    }
  },
  {
    "model": "documents.createnewdocuments",
    "pk": 80,
    "fields": {
      "created_by": 22,
      "title": "24Feb-6March 2020",
      "content": "<ol>\r\n\t<li>\r\n\t<p><strong>Manual Search Engine</strong></p>\r\n\r\n\t<ol>\r\n\t\t<li>\r\n\t\t<p>had a weird experience with the result as in when we delete the database yet we got the same answer. Kept testing it, then we realized that the correct answer was the one shown and the one we got was after manipulating the feedback. While me, <strong>Sharmila</strong>, and <strong>Dipesh</strong> were kinda amazed and confused.</p>\r\n\t\t</li>\r\n\t\t<li>\r\n\t\t<p>Since there was a lot of confusion, we displayed the scores in the frontend itself. So that it will be easier for the client to understand how it works.</p>\r\n\t\t</li>\r\n\t</ol>\r\n\t</li>\r\n\t<li>\r\n\t<p><strong>Data Entry</strong></p>\r\n\r\n\t<ol>\r\n\t\t<li>\r\n\t\t<p>Created count API where they can view the daily count of their task: <a href=\"http://10.10.5.80:8000/task_json#\">http://10.10.5.80:8000/task_json#</a></p>\r\n\t\t</li>\r\n\t\t<li>\r\n\t\t<p>March 2- March 6 they were extracting and cleaning the Nepali samvidhan&#39;s data extracted from the web by <strong>Dipesh</strong>.</p>\r\n\t\t</li>\r\n\t\t<li>\r\n\t\t<p>hira&#39;s count not being updated issue. it was due to the service not being restarted.</p>\r\n\t\t</li>\r\n\t</ol>\r\n\t</li>\r\n\t<li>\r\n\t<p><strong>Text Summarizer</strong></p>\r\n\r\n\t<ol>\r\n\t\t<li>\r\n\t\t<p>Created a benchmark along with <strong>Sharmila</strong> as we both were playing with various algorithms: <a href=\"https://docs.google.com/spreadsheets/d/1FzxYckWmpYhra1uY1E29MtN-YHbyHsWDFCKF4LoaFk0/edit#gid=0\">https://docs.google.com/spreadsheets/d/1FzxYckWmpYhra1uY1E29MtN-YHbyHsWDFCKF4LoaFk0/edit#gid=0</a></p>\r\n\t\t</li>\r\n\t\t<li>\r\n\t\t<p><strong>REf for BERT: <a href=\"https://github.com/dmmiller612/bert-extractive-summarizer\">https://github.com/dmmiller612/bert-extractive-summarizer</a>&nbsp;</strong>which has more algorithms included such as GPT,XLNET,etc.</p>\r\n\t\t</li>\r\n\t\t<li>\r\n\t\t<p><strong>Neural Coref </strong>for coref resolution in extractive text summary.</p>\r\n\r\n\t\t<ol>\r\n\t\t\t<li>\r\n\t\t\t<p>REF: https://github.com/huggingface/neuralcoref</p>\r\n\t\t\t</li>\r\n\t\t\t<li>\r\n\t\t\t<p>Example:</p>\r\n\r\n\t\t\t<pre>\r\nDeepika has a dog. She loves him. The movie star has always been fond of animals.</pre>\r\n\t\t\t<u>RESULT</u>:&nbsp;\r\n\r\n\t\t\t<pre>\r\nDeepika has a dog. Deepika loves a dog. Deepika has always been fond of animals</pre>\r\n\t\t\t</li>\r\n\t\t</ol>\r\n\t\t</li>\r\n\t\t<li>\r\n\t\t<p>abstractive summarizer research</p>\r\n\r\n\t\t<ol>\r\n\t\t\t<li>\r\n\t\t\t<p>ran https://github.com/yaushian/Unparalleled-Text-Summarization-using-GAN based on&nbsp;<a href=\"https://arxiv.org/pdf/1810.02851.pdf\">https://arxiv.org/pdf/1810.02851.pdf</a>&nbsp;paper</p>\r\n\t\t\t</li>\r\n\t\t\t<li>\r\n\t\t\t<p>To train the generator&nbsp;which encodes the input text into a shorter word sequence took me one day.</p>\r\n\t\t\t</li>\r\n\t\t\t<li>\r\n\t\t\t<p>To train the summary it took more than a day so stopped it since i was running it my laptop.</p>\r\n\t\t\t</li>\r\n\t\t\t<li>\r\n\t\t\t<p>Note: need to train and test it.</p>\r\n\t\t\t</li>\r\n\t\t</ol>\r\n\t\t</li>\r\n\t</ol>\r\n\t</li>\r\n\t<li>\r\n\t<p><strong>RASA</strong></p>\r\n\r\n\t<ol>\r\n\t\t<li>\r\n\t\t<p>Audio chatbot: created a small modules for TTS and STT using deepspeech and Tacotron python library. Issue is that the deepspeech model didn&#39;t give a good result in English for which more training will be required. While Tacotron was working was fine for english.&nbsp;</p>\r\n\t\t</li>\r\n\t\t<li>\r\n\t\t<p>Rasa Form: tried working with the forms. Created a simple one which saves the input of user from the conversation and does the checkout process at the end of the conversation. <strong>Sharmila</strong> did the other way of working with the Forms.</p>\r\n\t\t</li>\r\n\t</ol>\r\n\t</li>\r\n\t<li>\r\n\t<p><strong>Resume Filtering</strong></p>\r\n\t</li>\r\n\t<li>\r\n\t<p><strong>Study</strong></p>\r\n\r\n\t<ol>\r\n\t\t<li>\r\n\t\t<p>Linear Algebra by Gilbert Strang: studied chapter 1&nbsp;</p>\r\n\t\t</li>\r\n\t</ol>\r\n\t</li>\r\n</ol>\r\n\r\n<p><strong>Others</strong></p>\r\n\r\n<ol>\r\n\t<li>\r\n\t<p>Meetings</p>\r\n\r\n\t<ol>\r\n\t\t<li>\r\n\t\t<p>Data Entry</p>\r\n\t\t</li>\r\n\t\t<li>\r\n\t\t<p>Talk with kobayashi san with regard to<strong> Feedback in manual search engine </strong>and <strong>Japanese Text summarizer</strong></p>\r\n\t\t</li>\r\n\t\t<li>\r\n\t\t<p>meeting with <strong>Gaurav Sir,&nbsp;Suman Sir&nbsp;</strong>after the interview</p>\r\n\t\t</li>\r\n\t\t<li>\r\n\t\t<p>meeting pointnemo</p>\r\n\t\t</li>\r\n\t</ol>\r\n\t</li>\r\n\t<li>\r\n\t<p>going through interview tasks and resume</p>\r\n\t</li>\r\n\t<li>\r\n\t<p>interview ronast n sakar</p>\r\n\t</li>\r\n\t<li>\r\n\t<p>vaachan issues&nbsp;</p>\r\n\t</li>\r\n</ol>",
      "date": "2020-03-10"
    }
  },
  {
    "model": "documents.createnewdocuments",
    "pk": 81,
    "fields": {
      "created_by": 23,
      "title": "2020 - (24-02 to 06-03)",
      "content": "<h2><strong>Works&nbsp;Done:</strong></h2>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p><strong>WowTime:</strong></p>\r\n\r\n<p>- figure out that ffmpeg is able to play and update m3u8 playlist and remove old ts file. (hls_list_size * 2 + 1) segments are stored.</p>\r\n\r\n<p>parametes: hls_list_size , hls_time, -hls_delete_segments</p>\r\n\r\n<p>-&nbsp;Audio control volume, try and get metadata from stream (pending).</p>\r\n\r\n<p>-&nbsp;Study little endian and big endian. Look into&nbsp;audio_segment&#39;s code.</p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p><strong>Cassandra:</strong></p>\r\n\r\n<p><strong>-&nbsp;</strong>Change Cassandra (production) cluster default port and also&nbsp;updated in server ports&nbsp;doc.</p>\r\n\r\n<p>- Lookup on Cassandra Tokenization to improve api&#39;s fetch performance and solve memory issues.&nbsp;On each node, rows are sorted by the token generated by the partitioner. Trying to use tokens as offset similar to Oracle.&nbsp;(Pending)</p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p><strong>Bigmart:</strong></p>\r\n\r\n<p>- Bigmart change query, added new columns (loyalty discount and promoname).</p>\r\n\r\n<p>-&nbsp;Create an api for QA&#39;s for testing bigmart data. (Takes&nbsp;start_date, end_data, table_mapping and fetches count from cassandra and orcale)</p>\r\n\r\n<p>Issue Faced: Response timeout. Created Socket object&nbsp;and increase timeout and pass it as an config option to cluster config.</p>\r\n\r\n<p>- Update Doc on&nbsp;Java code on bigmart-fetch application</p>\r\n\r\n<p>-&nbsp;Statement param (setfetchsize) can improve fetch performance. Default 100 being used at the moment,&nbsp;changed it to 20000 and tested.</p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p><strong>Kafka:</strong></p>\r\n\r\n<p>- Install Postgres in local.</p>\r\n\r\n<p>- Study&nbsp;Change Data Capture with Debezium and Kafka Source Connector (Postgres Debezium)</p>\r\n\r\n<p>-&nbsp;Write Ahead Logs (Postgres)</p>\r\n\r\n<p>-&nbsp;Install &nbsp;1) wal2json plugin and&nbsp;2) protobuff plugin for postgres to track write a head log.</p>\r\n\r\n<p>Issues Faced: While installing by (make)&nbsp;wal2json header files could not be located.</p>\r\n\r\n<p>Solution: Postgres include files installed from a separate package. postgres-server-dev-12</p>\r\n\r\n<p>- Create slots for postgres WAL and test if logs were being commited.</p>\r\n\r\n<p>Issue: Which user executed which query could not be tracked. But queries were being logged in slots.</p>\r\n\r\n<p>-&nbsp;Run Kafka-Connect-Distributed and connect to postgres and send logs to topic.</p>\r\n\r\n<p>Issue Faced: Password was not recognized. Was using default username and password.</p>\r\n\r\n<p>Solution:&nbsp;Created new user in postgres and tried connecting through that user.</p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p>&nbsp;</p>",
      "date": "2020-03-10"
    }
  },
  {
    "model": "documents.createnewdocuments",
    "pk": 82,
    "fields": {
      "created_by": 13,
      "title": "2020-03-17",
      "content": "<h1>Task 1</h1>\r\n\r\n<h2><strong>GPS</strong></h2>\r\n\r\n<p><strong>Description:</strong></p>\r\n\r\n<p>1) Trip analysis</p>\r\n\r\n<p>2) Order analysis</p>\r\n\r\n<p><strong>Problem faced</strong> : error in timestamp , incorrect vendor and customer data\\</p>\r\n\r\n<p><strong>Time taken</strong> : 2 weeks</p>\r\n\r\n<h1>Task 2</h1>\r\n\r\n<h2><strong>Traffic prediction</strong></h2>\r\n\r\n<p><strong>Description</strong>:</p>\r\n\r\n<p>- peak traffic jam time</p>\r\n\r\n<p><strong>Problem faced</strong> : need to correct road data and junction data</p>\r\n\r\n<p><strong>time taken</strong> : 2 weeks</p>\r\n\r\n<h1>Task 3&nbsp;</h1>\r\n\r\n<p><strong>Description</strong> :</p>\r\n\r\n<p>Audio analysis : just starting&nbsp;</p>",
      "date": "2020-03-17"
    }
  },
  {
    "model": "documents.createnewdocuments",
    "pk": 83,
    "fields": {
      "created_by": 22,
      "title": "10-20March 2020",
      "content": "<p><strong>Japanese Text Summarizer</strong></p>\r\n\r\n<p><strong>Gave a Demo Video to Kobayashi San&nbsp;</strong></p>\r\n\r\n<p><strong>Resume Filtering</strong></p>\r\n\r\n<p><strong>Research</strong></p>\r\n\r\n<p><strong>Data conversion from pdf,doc to text</strong></p>\r\n\r\n<p><strong>Preprocessing the converted data</strong></p>\r\n\r\n<p><strong>Target Ads:</strong></p>\r\n\r\n<p><strong>save URL landing page: category/subpage/homepage</strong></p>\r\n\r\n<p><strong>Added url count</strong></p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p><strong>Others:</strong></p>\r\n\r\n<ol>\r\n\t<li>\r\n\t<p><strong>Audio Study</strong></p>\r\n\r\n\t<ol>\r\n\t\t<li>\r\n\t\t<p><strong>Sharmila and Dipesh are helping me with the Audio Learning part.</strong></p>\r\n\t\t</li>\r\n\t</ol>\r\n\t</li>\r\n\t<li>\r\n\t<p><strong>Transliteration</strong></p>\r\n\r\n\t<ol>\r\n\t\t<li>\r\n\t\t<p><strong>transliterated manish n furbas data i.e the location data from english to nepa-angregi. Paper:FIRE2014_IIITH.pdf</strong></p>\r\n\t\t</li>\r\n\t</ol>\r\n\t</li>\r\n\t<li>\r\n\t<p><strong>Meetings</strong></p>\r\n\r\n\t<ol>\r\n\t\t<li>\r\n\t\t<p><strong>1 full day went on interview and nlp ppt demo to Gaurav sir</strong></p>\r\n\t\t</li>\r\n\t\t<li>\r\n\t\t<p><strong>NLP presentation to Sales Team</strong></p>\r\n\t\t</li>\r\n\t\t<li>\r\n\t\t<p><strong>Audio Proj listing: <a href=\"https://docs.google.com/spreadsheets/d/1PJpOCPFYTv8lCmCbtXHWDh-3Lo7PtRQN--ULz7_7k10/edit#gid=0\">https://docs.google.com/spreadsheets/d/1PJpOCPFYTv8lCmCbtXHWDh-3Lo7PtRQN--ULz7_7k10/edit#gid=0</a></strong></p>\r\n\t\t</li>\r\n\t\t<li>\r\n\t\t<p><strong>Target Ads Meeting</strong></p>\r\n\t\t</li>\r\n\t</ol>\r\n\t</li>\r\n\t<li>\r\n\t<p><strong>2 weeks before the lockdown meetings with all the project teams [target ads,nlp,data entry,yeti,foodmandu].</strong></p>\r\n\t</li>\r\n</ol>",
      "date": "2020-03-23"
    }
  },
  {
    "model": "documents.createnewdocuments",
    "pk": 84,
    "fields": {
      "created_by": 13,
      "title": "2020-04-08",
      "content": "<h1>Task 1</h1>\r\n\r\n<h2>Traffic Analysis&nbsp;</h2>\r\n\r\n<p><strong>Description</strong></p>\r\n\r\n<p>Task was to study traffic status in main chowks in kathmandu.</p>\r\n\r\n<p><strong>Methods used: </strong></p>\r\n\r\n<p>1) average speed [ lower avg speed is indicator of traffic congestion ]</p>\r\n\r\n<p>2) Density based clustering&nbsp;</p>\r\n\r\n<p><strong>Problem faced</strong></p>\r\n\r\n<p>- lack of data</p>\r\n\r\n<p>Time taken : 7 days</p>\r\n\r\n<h1>task2</h1>\r\n\r\n<h2>Description&nbsp;</h2>\r\n\r\n<p>task was to analyse pollution data from drishti</p>\r\n\r\n<p><strong>problem faced</strong>: None</p>\r\n\r\n<p><strong>time taken</strong> : 3 days</p>\r\n\r\n<h1>task3</h1>\r\n\r\n<h2>NEA</h2>\r\n\r\n<p><strong>Description</strong></p>\r\n\r\n<p>task was to perform EDA on Nepal Electricity Authority data</p>\r\n\r\n<p><strong>problem faced</strong> : None</p>\r\n\r\n<p><strong>time taken </strong>: 2&nbsp;days</p>\r\n\r\n<p>&nbsp;</p>",
      "date": "2020-04-08"
    }
  },
  {
    "model": "documents.createnewdocuments",
    "pk": 85,
    "fields": {
      "created_by": 16,
      "title": "March_23_April_3_2020",
      "content": "<p><u><strong>WEEKLY REPORT</strong></u></p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p><strong>1. Traffic Analysis</strong></p>\r\n\r\n<p><strong>Tasks Performed:</strong></p>\r\n\r\n<ul>\r\n\t<li>\r\n\t<p>Write existing traffic prediction project written in Python to Scala spark.</p>\r\n\t</li>\r\n\t<li>\r\n\t<p>Data preparation combining all data into single file and date wise files.</p>\r\n\t</li>\r\n\t<li>\r\n\t<p>Feature Extraction and Data generation using store procedure, udf and others.</p>\r\n\t</li>\r\n\t<li>\r\n\t<p>Custom dbscan implementation for better understanding of algorithm.</p>\r\n\t</li>\r\n\t<li>\r\n\t<p>Implement main chowk approach.</p>\r\n\t</li>\r\n\t<li>\r\n\t<p>Google case study for traffic prediction.</p>\r\n\t</li>\r\n</ul>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p><strong>Main Chowk Approach:</strong></p>\r\n\r\n<ul>\r\n\t<li>\r\n\t<p>Average speed of vehicles around main chowks.</p>\r\n\t</li>\r\n\t<li>\r\n\t<p>Formation of clusters around main chowks.</p>\r\n\t</li>\r\n</ul>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p><strong>Self Study:</strong></p>\r\n\r\n<p><strong>Traffic Prediction by Google:</strong></p>\r\n\r\n<ol>\r\n\t<li>\r\n\t<p>Google Maps Check the traffic <strong>by tracking moving of android phones on roads</strong>.</p>\r\n\t</li>\r\n\t<li>\r\n\t<p>Early Version of Google maps use to depends on Traffic Sensors which was installed by Government and private companies. By using Radar technology and sensor where enabled to detect Traffic situation of roads.</p>\r\n\t</li>\r\n\t<li>\r\n\t<p>Driver use <strong>Waze app to Traffic incidents</strong> which includes disabled vehicles , Accidents , slowdown and even Speed traps.</p>\r\n\t</li>\r\n\t<li>\r\n\t<p>Signs: Green, Yellow, Red and Grey</p>\r\n\t</li>\r\n</ol>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p><strong>References:</strong></p>\r\n\r\n<ul>\r\n\t<li>\r\n\t<p><strong><a href=\"https://electronics.howstuffworks.com/how-does-google-maps-predict-traffic.htm\">https://electronics.howstuffworks.com/how-does-google-maps-predict-traffic.htm</a></strong></p>\r\n\t</li>\r\n\t<li>\r\n\t<p><strong><a href=\"https://medium.com/@imtechpros_87395/where-does-google-maps-get-its-traffic-data-from-2562f984d82f\">https://medium.com/@imtechpros_87395/where-does-google-maps-get-its-traffic-data-from-2562f984d82f</a></strong></p>\r\n\t</li>\r\n</ul>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p><strong>2. Drishti Dataset Exploratory Analysis</strong></p>\r\n\r\n<ul>\r\n\t<li>\r\n\t<p>Meeting</p>\r\n\t</li>\r\n\t<li>\r\n\t<p>Regression Analysis</p>\r\n\t</li>\r\n\t<li>\r\n\t<p>Trend and Seasonality Analysis</p>\r\n\t</li>\r\n\t<li>\r\n\t<p>Explore Data Distribution</p>\r\n\t</li>\r\n\t<li>\r\n\t<p>Find Correlation using Heatmap</p>\r\n\t</li>\r\n\t<li>\r\n\t<p>Simple SQL Analysis</p>\r\n\t</li>\r\n</ul>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p><strong>3. Nepal Electricity Authority Data Apprehension</strong></p>\r\n\r\n<ul>\r\n\t<li>\r\n\t<p>Meeting with Samina dd</p>\r\n\t</li>\r\n\t<li>\r\n\t<p>Data Exploration</p>\r\n\t</li>\r\n</ul>",
      "date": "2020-04-08"
    }
  },
  {
    "model": "documents.createnewdocuments",
    "pk": 86,
    "fields": {
      "created_by": 23,
      "title": "2020 - (09-03 to 20-03)",
      "content": "<h2><strong>Works&nbsp;Done:</strong></h2>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p><strong>Postgres:</strong></p>\r\n\r\n<p>-&nbsp;Install decoderbuff plugin for postgres to track change data capture. (Learned about make)</p>\r\n\r\n<p>-&nbsp;Look into Protobuffers</p>\r\n\r\n<p>- Finding: Postgres itself provides log configurations to track which user exectued what command.</p>\r\n\r\n<p>-&nbsp;Enabled log level setting in postgresql.conf</p>\r\n\r\n<p>-&nbsp;Study Postgres transaction log, replication slots, logical decoding</p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p><strong>Kafka:&nbsp;</strong></p>\r\n\r\n<p><strong>- </strong>Look into&nbsp;Kafka avro console consumer</p>\r\n\r\n<p>-&nbsp;Get Kafka-debezium-postgres-connector working and was able to capture data change using wal2json plugin for postgresql.</p>\r\n\r\n<p>Issue: Previous&nbsp;values of data before being changed&nbsp;were not being logged.</p>\r\n\r\n<p>Solution: alter table replica identity full. [ replica identity: A parameter that can be used to control the information written to WAL to identify tuple data that is being deleted or updated]</p>\r\n\r\n<p>-&nbsp;Study Kafka ksql and kstreams. Learn how to create streams and tables.</p>\r\n\r\n<p>- Create stream&nbsp;that takes input from kafka topic.</p>\r\n\r\n<p>Issue: Avro format contained&nbsp;nested data. Struct type (compound data type)</p>\r\n\r\n<p>Solution: Flatten the nested avro data.</p>\r\n\r\n<p>-&nbsp;Create new final stream as select query by applying join on two streams.</p>\r\n\r\n<p>Issue Faced: stream-stream join required&nbsp;withinin clause. (window function)</p>\r\n\r\n<p>Solution: Used within clause and got the expected result</p>\r\n\r\n<p>-&nbsp;Install CDC in dev servers.</p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p><strong>Logstash:</strong></p>\r\n\r\n<p>-&nbsp;Found a grok debugger.&nbsp;</p>\r\n\r\n<p>-&nbsp;Parse Postgresql Audit&nbsp;Logs using logstash. [ Took sometime to adjust and get the configurations and log format right. ]</p>\r\n\r\n<p>&nbsp;</p>",
      "date": "2020-04-10"
    }
  },
  {
    "model": "documents.createnewdocuments",
    "pk": 87,
    "fields": {
      "created_by": 23,
      "title": "2020 - (23-03 to 03-04)",
      "content": "<h2><strong>Works&nbsp;Done:</strong></h2>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p><strong>Cassandra CDC:</strong></p>\r\n\r\n<p>-&nbsp;Look up CDC on cassandra. Found 2 connectors from debezium and landoop</p>\r\n\r\n<p>-&nbsp;Tried using debezium and landoop cdc connectors.</p>\r\n\r\n<p>Issue: Both in dev phase / beta phase. Could not find any proper documentation. Could not get it working. Decided to wait for the official release and docs from confluent.</p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p><strong>Cassandra:</strong></p>\r\n\r\n<p>- Lookup on Cassandra Tokenization to improve api&#39;s fetch performance and solve memory issues.</p>\r\n\r\n<p>- Cassandra Offset and Pagination. Found two ways to implement. [ 1.Token functions 2. Java stmt.pagingstate ]</p>\r\n\r\n<p>Issue Faced: To use token functions, the table&#39;s schema had to be changed. This would be a tedious task. Hence tried fixing this issue through code itself using pagingstate</p>\r\n\r\n<p>Solution: set pagingstate&nbsp;[ Sets the paging state, a token&nbsp;representing the current page state of query used to continue paging by retrieving the following result page. Setting the paging state will disable automatic paging. ]</p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p><strong>Logstash:</strong></p>\r\n\r\n<p>- Implemented geoip feature in logstash. [ Provides intricate details from ip address such as Country, country code, lat, lon, etc. ]</p>\r\n\r\n<p>- Provide a bried session to sachin and coordinate with him&nbsp;to implement logstash geoip in projects</p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p><strong>Mongo CDC and Kafka:</strong></p>\r\n\r\n<p>-&nbsp;Study about mongodb</p>\r\n\r\n<p>- Install mongodb and mongodb-kafka-source-connector</p>\r\n\r\n<p>-&nbsp;Create use and collections</p>\r\n\r\n<p>-&nbsp;Implement audit logging and CDC</p>\r\n\r\n<p>Issue: Source connector was unable to read operation log (oplog). Version issue Connector only support mongo version&nbsp;3.5 or above.</p>\r\n\r\n<p>Debezium-mongo-connector does not capture the previous state of data but capture the updated data.</p>\r\n\r\n<p>Solution: Reinstall mongo and reimplement everything</p>\r\n\r\n<p>Issue:&nbsp;Mongodb audit log only available for paid versions.</p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p><strong>Other:</strong></p>\r\n\r\n<p>- Did some reading&nbsp;on Nvidia IVA</p>\r\n\r\n<p>-&nbsp;Create small doc on NVIDIA-IVA</p>\r\n\r\n<p>- Study about Junit testing</p>\r\n\r\n<p>- Wrote&nbsp;documentaion and&nbsp;&nbsp;unit test code for API</p>",
      "date": "2020-04-10"
    }
  },
  {
    "model": "documents.createnewdocuments",
    "pk": 88,
    "fields": {
      "created_by": 24,
      "title": "March 9- April 3",
      "content": "<h1>Report&nbsp;</h1>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<h2>RASA AIRLINE TICKET BOOKING APPLICATION</h2>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<ul>\r\n\t<li>\r\n\t<p>Implemented the rasa module for booking systems.</p>\r\n\t</li>\r\n\t<li>\r\n\t<p>Data creation for intent classification, such as a user asking for either flight schedule details or flight status or flight booking, along with integration of feedback from the user in the case clear distinction is missing.</p>\r\n\t</li>\r\n\t<li>\r\n\t<p>Created a flow of conversation, which includes asking required information for flight booking. Referred Yeti&rsquo;s website for it.</p>\r\n\t</li>\r\n\t<li>\r\n\t<p>Scheduled detail and status is not integrated with data.</p>\r\n\t</li>\r\n\t<li>\r\n\t<p>Extracted the user&rsquo;s answer as entity and parsed to extract required information</p>\r\n\t</li>\r\n\t<li>\r\n\t<p>UI taken a open source template and integrated with rasa api</p>\r\n\t</li>\r\n\t<li>\r\n\t<p>Created a dummy database that for now helps to form architecture for data flow.</p>\r\n\t</li>\r\n\t<li>\r\n\t<p>Demo ready and finalized by Pema And Vaghwan dai.</p>\r\n\t</li>\r\n</ul>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<h2>LIBROSA - A Library for sound analysis</h2>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<ul>\r\n\t<li>\r\n\t<p>SInce, vachaan app was not working, requested a data entry team to record names of districts in a single audio file with space of 3 second between each of them.</p>\r\n\t</li>\r\n\t<li>\r\n\t<p>Empirical study of average gap between words: around 2 sec</p>\r\n\t</li>\r\n\t<li>\r\n\t<p>Then set the frame length 2, so that the silence detected will be at least in the gap of 2 sec. Otherwise, minor silence between the same word is also splitted.&nbsp;</p>\r\n\t</li>\r\n\t<li>\r\n\t<p>Implemented the code, and the result is good with few glitches that could be handled manually. Like if there were 280 words, splitted audio was of 278 counts.</p>\r\n\t</li>\r\n</ul>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<h2>UnitTest, PyTest, DocTest</h2>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<ul>\r\n\t<li>\r\n\t<p>How you could test functionality of your module using doctest.</p>\r\n\t</li>\r\n\t<li>\r\n\t<p>Unittest- book by Ashwin , basics of unittest, asserting and test structure</p>\r\n\t</li>\r\n\t<li>\r\n\t<p>Pytest, followed documentation, learned fixtures, parameters.</p>\r\n\t</li>\r\n\t<li>\r\n\t<p>Chode Pytest for our flask application, which has both classes and individual modules because of the easy accessibility of fixtures in pytest.</p>\r\n\t</li>\r\n\t<li>\r\n\t<p>Documented the learning and hands on and pushed it to git:</p>\r\n\t</li>\r\n\t<li>\r\n\t<p>Implementing unit testing on already written codes, though is not feasible. So, refactoring codes alongside. Just completed four modules of manual search engine project.</p>\r\n\t</li>\r\n</ul>\r\n\r\n<p>REPO: <a href=\"https://github.com/sarmilaupadhyaya/Unit-Test-Using-Python\">https://github.com/sarmilaupadhyaya/Unit-Test-Using-Python</a></p>\r\n\r\n<p><br />\r\n&nbsp;</p>\r\n\r\n<h2>AUDIO RESEARCH</h2>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<ul>\r\n\t<li>\r\n\t<p>Learning application of Digital Signal Processing for audio Analysis.</p>\r\n\t</li>\r\n\t<li>\r\n\t<p>Been through various links for short span, nothing very intact, just to clear up concepts of :</p>\r\n\t</li>\r\n</ul>\r\n\r\n<ul>\r\n\t<li>\r\n\t<p>Complex Plane</p>\r\n\t</li>\r\n\t<li>\r\n\t<p>Euler Identity</p>\r\n\t</li>\r\n\t<li>\r\n\t<p>Power Series, Taylor Series</p>\r\n\t</li>\r\n\t<li>\r\n\t<p>Sinusoids</p>\r\n\t</li>\r\n\t<li>\r\n\t<p>Complex Sinusoids</p>\r\n\t</li>\r\n\t<li>\r\n\t<p>Discrete fourier transform (Fast fourier transform)</p>\r\n\t</li>\r\n</ul>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<ul>\r\n\t<li>\r\n\t<p>Following a course in coursera.&nbsp;</p>\r\n\t</li>\r\n</ul>\r\n\r\n<p>LINK: <a href=\"https://www.coursera.org/learn/audio-signal-processing/home/welcome\">https://www.coursera.org/learn/audio-signal-processing/home/welcome</a></p>\r\n\r\n<ul>\r\n\t<li>\r\n\t<p>Completed upto week 2 with programming assignments. This week completed week 3 content too.</p>\r\n\t</li>\r\n\t<li>\r\n\t<p>That course is free and provides with stanfords unlisted link to basic topics mentioned above. So, I felt less need to document things. The course for now covers things needed.</p>\r\n\t</li>\r\n</ul>\r\n\r\n<p><br />\r\n&nbsp;</p>\r\n\r\n<p>OTHERS</p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<ul>\r\n\t<li>\r\n\t<p>Listed issues on new vaachan application</p>\r\n\t</li>\r\n\t<li>\r\n\t<p>Handed it over to new QA</p>\r\n\t</li>\r\n</ul>\r\n\r\n<p>&nbsp;</p>",
      "date": "2020-04-10"
    }
  },
  {
    "model": "documents.createnewdocuments",
    "pk": 89,
    "fields": {
      "created_by": 3,
      "title": "2020-(03-23 to 04-03)",
      "content": "<p><strong>Works Done</strong></p>\r\n\r\n<p><strong>Log Analysis &amp; Visualization</strong></p>\r\n\r\n<ul>\r\n\t<li>\r\n\t<p>Added new Visualization for application logging. (Invalid/UnAuthorized Attempts &amp; Page visits user wise)</p>\r\n\t</li>\r\n\t<li>\r\n\t<p>Real time graph update but only for the total hits, line and bar graph update for real time is left.</p>\r\n\t</li>\r\n\t<li>\r\n\t<p>Feature for adding multiple indexes for a single user and separating web and mobile views.</p>\r\n\t</li>\r\n\t<li>\r\n\t<p>Integration and some code refactor for the visualization.&nbsp;&nbsp;</p>\r\n\t</li>\r\n\t<li>\r\n\t<p>Sending Notification is remaining.</p>\r\n\t</li>\r\n</ul>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p><strong>System Metrics Visualization</strong></p>\r\n\r\n<ul>\r\n\t<li>\r\n\t<p>Studied Lucene query for visualization in grafana.</p>\r\n\t</li>\r\n\t<li>\r\n\t<p>Tested &amp; implemented for application logs and also tested notification</p>\r\n\t</li>\r\n</ul>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p><strong>Intrusion Detection Research &amp; Implementation</strong></p>\r\n\r\n<ul>\r\n\t<li>\r\n\t<p>Studying Cyber Operation book</p>\r\n\t</li>\r\n\t<li>\r\n\t<p>Installed Windows, Ubuntu &amp; Kali linux in Virtual machine</p>\r\n\t</li>\r\n\t<li>\r\n\t<p>Trying to implement Metasploit from Basic Offense chapter 2.</p>\r\n\t</li>\r\n</ul>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p><strong>Application Layer (cassandra)</strong></p>\r\n\r\n<ul>\r\n\t<li>\r\n\t<p>Trying to solve Memory leak issue and program halt issue after fetching more than 3 million records.</p>\r\n\t</li>\r\n</ul>",
      "date": "2020-04-08"
    }
  },
  {
    "model": "documents.createnewdocuments",
    "pk": 91,
    "fields": {
      "created_by": 3,
      "title": "2020-(04-06 to 05-01)",
      "content": "<p><strong>Works Done (04-06 to 04-17)</strong></p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p><strong><span style=\"background-color:transparent; color:#000000\">Log Analysis &amp; Visualization</span></strong></p>\r\n\r\n<ul>\r\n\t<li style=\"list-style-type:disc\">\r\n\t<p><span style=\"background-color:transparent; color:#000000\">Map Analysis (Country, City &amp; Location visualization)</span></p>\r\n\t</li>\r\n\t<li style=\"list-style-type:disc\">\r\n\t<p><span style=\"background-color:transparent; color:#000000\">Tried to implement ES in spark, took more than 10 sec to load a dashboard, so the idea was kept in hold.</span></p>\r\n\t</li>\r\n\t<li style=\"list-style-type:disc\">\r\n\t<p><span style=\"background-color:transparent; color:#000000\">Integration with UI made by Faraz.</span></p>\r\n\t</li>\r\n\t<li style=\"list-style-type:disc\">\r\n\t<p><span style=\"background-color:transparent; color:#000000\">Implementation of Notification using es-sql, but the library was available for only python2.</span></p>\r\n\t</li>\r\n\t<li style=\"list-style-type:disc\">\r\n\t<p><span style=\"background-color:transparent; color:#000000\">Discussed about the implementation of Mobile events with the Bigmart App team.</span></p>\r\n\t</li>\r\n</ul>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p><strong><span style=\"background-color:transparent; color:#000000\">Intrusion Detection Research &amp; Implementation</span></strong></p>\r\n\r\n<ul>\r\n\t<li style=\"list-style-type:disc\">\r\n\t<p><span style=\"background-color:transparent; color:#000000\">Implemented webpage exploits, found minor vulnerability related to cache</span></p>\r\n\t</li>\r\n\t<li style=\"list-style-type:disc\">\r\n\t<p><span style=\"background-color:transparent; color:#000000\">Scanned servers for openport and found OpenSSH 7.2p2 used in most of our servers, which can be used to exploit the server and gain the user of it.</span></p>\r\n\t</li>\r\n\t<li style=\"list-style-type:disc\">\r\n\t<p><span style=\"background-color:transparent; color:#000000\">Also, implemented ssh attack in windows and linux. Older versions of them were vulnerable enough to create a session and attack file systems.</span></p>\r\n\t</li>\r\n</ul>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p><strong><span style=\"background-color:transparent; color:#000000\">Application Layer (cassandra)</span></strong></p>\r\n\r\n<ul>\r\n\t<li style=\"list-style-type:disc\">\r\n\t<p><span style=\"background-color:transparent; color:#000000\">Cassandra with pagination was deployed and then tested by Ramesh, it was working fine. So, the issue regarding memory consumption and restarting the Application Layer was solved.</span></p>\r\n\t</li>\r\n\t<li style=\"list-style-type:disc\">\r\n\t<p><span style=\"background-color:transparent; color:#000000\">Need to deploy it in docker, which will be done by Tchiring.</span></p>\r\n\t</li>\r\n</ul>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p><strong>Works Done (04-20 to 05-01)</strong></p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p><strong><span style=\"background-color:transparent; color:#000000\">Log Analysis &amp; Visualization</span></strong></p>\r\n\r\n<ul>\r\n\t<li style=\"list-style-type:disc\">\r\n\t<p><span style=\"background-color:transparent; color:#000000\">New Log format was created after discussing it with the Log Analysis Team.Also, a format discussion was done for mobile click events, still need to research more about the event capture.</span></p>\r\n\t</li>\r\n</ul>\r\n\r\n<p>&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp; &quot;level&quot;: &quot;&quot;<br />\r\n&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&quot;statusCode&quot;: &quot;&quot;<br />\r\n&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&quot;msg&quot;: &quot;&quot;<br />\r\n&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&quot;method&quot;: &quot;&quot;<br />\r\n&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&quot;host_address&quot;: &quot;&quot;<br />\r\n&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&quot;userId&quot;:&quot;&quot;<br />\r\n&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&quot;useragent&quot;: &quot;&quot;<br />\r\n&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&quot;deviceId&quot;: &quot;&quot;<br />\r\n&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&quot;deviceOs&quot;: &quot;&quot;<br />\r\n&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&quot;deviceModel&quot;: &quot;&quot;<br />\r\n&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&quot;deviceVersionName&quot;: &quot;&quot;<br />\r\n&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&quot;deviceLatLong&quot;: &quot;&quot;<br />\r\n&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&quot;useragent&quot;: &quot;&quot;<br />\r\n&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&quot;url&quot;: &quot;&quot;<br />\r\n&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&quot;date&quot;: &quot;yyyy-MM-dd HH:mm:ss&quot;<br />\r\n&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&quot;time&quot;: &quot;in millisecond&quot;<br />\r\n&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&quot;remoteFamily&quot;: &quot;&quot;<br />\r\n&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&quot;ip&quot;: &quot;&quot;<br />\r\n&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&quot;connection_name&quot;: &quot;&quot;<br />\r\n&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&quot;connection_type&quot;: &quot;&quot;</p>\r\n\r\n<ul>\r\n\t<li style=\"list-style-type:disc\">\r\n\t<p><span style=\"background-color:transparent; color:#000000\">Notification &amp; Alerts was set up according to grafana view, and also implementing ksql streams integration in eklogstash dashboard for notification.</span></p>\r\n\t</li>\r\n\t<li style=\"list-style-type:disc\">\r\n\t<p><span style=\"background-color:transparent; color:#000000\">Discussion of API counts and how to block users after a threshold to use API is crossed was done, will be implemented in near future.</span></p>\r\n\t</li>\r\n\t<li style=\"list-style-type:disc\">\r\n\t<p><span style=\"background-color:transparent; color:#000000\">Syslog and Authlog debugging with Sajita.</span></p>\r\n\t</li>\r\n</ul>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p><strong><span style=\"background-color:transparent; color:#000000\">Intrusion Detection Research &amp; Implementation</span></strong></p>\r\n\r\n<ul>\r\n\t<li style=\"list-style-type:disc\">\r\n\t<p><span style=\"background-color:transparent; color:#000000\">QR Codegen and Decode.</span></p>\r\n\t</li>\r\n\t<li style=\"list-style-type:disc\">\r\n\t<p><span style=\"background-color:transparent; color:#000000\">PNG image decode.</span></p>\r\n\t</li>\r\n</ul>",
      "date": "2020-05-01"
    }
  },
  {
    "model": "documents.createnewdocuments",
    "pk": 93,
    "fields": {
      "created_by": 23,
      "title": "2020 - (06-04 to 30-04)",
      "content": "<h2><strong>Works&nbsp;Done:</strong></h2>\r\n\r\n<p><strong>Docker:</strong></p>\r\n\r\n<p>- Learn about Docker.</p>\r\n\r\n<p>-&nbsp;Deploy flask application in docker container</p>\r\n\r\n<p>-&nbsp;Deploy maven project in docker container</p>\r\n\r\n<p>Issue: Port not forwarded properly. Soln (host_port : container_port)</p>\r\n\r\n<p>- Learn<strong>&nbsp;Docker Swarm</strong>,&nbsp;Docker compose and Docker compose.yml</p>\r\n\r\n<p><strong>Test Docker Swarm:</strong></p>\r\n\r\n<p>- Deploy flask and maven application in swarm mode and create replicas</p>\r\n\r\n<p>- Kill one container and see if request is being served by another container. (It Worked)</p>\r\n\r\n<p><strong>Docker Filebeat:</strong></p>\r\n\r\n<p>-&nbsp;Deploy filebeat in docker swarm mode with replica/scale=3</p>\r\n\r\n<p>Issue:&nbsp;Volume was not being mounted in container</p>\r\n\r\n<p>Solution:&nbsp;After scaling to 3 replicas 3 containers mainted own registry data and 3 times logs were sent to logstash. (Tried to use a shared data directory but didnt work)</p>\r\n\r\n<p><strong>Docker RestCache and Applayer:</strong></p>\r\n\r\n<p>-&nbsp;Install Docker in 5.71</p>\r\n\r\n<p>- Deploy applayer and restcache in Docker</p>\r\n\r\n<p>Issue: hsqldb jar not found local host maven dependency</p>\r\n\r\n<p>Solution: mount .m2 to container</p>\r\n\r\n<p>Issue: hsqldb connection failed</p>\r\n\r\n<p>Solution:&nbsp;hsqldb had to be pointed to the correct address.</p>\r\n\r\n<p>Issue: Spark master could not be found and&nbsp;Image did not have scala.</p>\r\n\r\n<p>Solution: Pulled another image which included scala, maven, spark.</p>\r\n\r\n<p>Issue: Containers were unable to communicate with one another even though the connections were properly&nbsp;set.</p>\r\n\r\n<p>Solution: Run the containers on local host&#39;s network space. (Container are created with their own network space generally but we can run them on the hosts network space by providing network config details in the docker-compose.yml file)</p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p><strong>Logstash:</strong></p>\r\n\r\n<p>-&nbsp;Work with sachin on on&nbsp;logstash</p>\r\n\r\n<p>Issue:&nbsp;Bigmart has different date format from espo and oa project logs</p>\r\n\r\n<p>Temp Solution:&nbsp;Created two different index templates with different date formats for now (further handeled by Bibek dai)</p>\r\n\r\n<p>- Logs date&nbsp;mismatch tried to&nbsp;fix.</p>\r\n\r\n<p>Issue: Unable to read CSV properly</p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p><strong>Twitter Data Sink:</strong></p>\r\n\r\n<p>-&nbsp;Look into Twitter Api and Hosebird Client API to stream realtime twitter tweets.</p>\r\n\r\n<p>- Currently using&nbsp;<a href=\"https://developer.twitter.com/en/docs/tweets/filter-realtime/overview/statuses-filter\">https://developer.twitter.com/en/docs/tweets/filter-realtime/overview/statuses-filter</a></p>\r\n\r\n<p>- Twitter data limits&nbsp;<a href=\"https://stackoverflow.com/questions/58881810/max-amount-of-tweets-returned-by-the-statuses-filter-endpoint-streaming-api\">https://stackoverflow.com/questions/58881810/max-amount-of-tweets-returned-by-the-statuses-filter-endpoint-streaming-api</a></p>\r\n\r\n<p>-&nbsp;Try to fetch tweets based on Location.</p>\r\n\r\n<p>-&nbsp;Learn about bounding boxes</p>\r\n\r\n<p>-&nbsp;Use bouding box to fetch tweets based on location.</p>\r\n\r\n<p>Issue: Tweets coming from india aswell quite inacurate.</p>\r\n\r\n<p>Solution: Bounding box for major cities in nepal (Tweets from india were fetched less often)</p>\r\n\r\n<p>-&nbsp;Create an application to sink Twitter tweets filtered based on location into Mongodb (Kafka used as a message broker)</p>\r\n\r\n<p>&nbsp;</p>",
      "date": "2020-05-01"
    }
  },
  {
    "model": "documents.createnewdocuments",
    "pk": 94,
    "fields": {
      "created_by": 13,
      "title": "Nea report",
      "content": "<p>Task 1</p>\r\n\r\n<p>NEA REPORT</p>\r\n\r\n<p>Description</p>\r\n\r\n<p>Performed eda on NEA data</p>\r\n\r\n<p>Problem faced : data provided is really messed up, so is taking time to understand it.&nbsp;</p>\r\n\r\n<p>Time taken : 10 days and still going</p>\r\n\r\n<p>&nbsp;</p>",
      "date": "2020-05-01"
    }
  },
  {
    "model": "documents.createnewdocuments",
    "pk": 95,
    "fields": {
      "created_by": 16,
      "title": "April_6-30_2020",
      "content": "<p><strong>Weekly Report</strong></p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p><strong>Nepal Electricity Authority Data Analysis</strong></p>\r\n\r\n<p style=\"margin-left:48px\">&nbsp;</p>\r\n\r\n<ol>\r\n\t<li>\r\n\t<p><strong>Domestic Analysis:</strong></p>\r\n\r\n\t<ul>\r\n\t\t<li>\r\n\t\t<p>Total units consumed per year</p>\r\n\t\t</li>\r\n\t\t<li>\r\n\t\t<p>Total connections per year</p>\r\n\t\t</li>\r\n\t\t<li>\r\n\t\t<p>Regional distribution of units over month</p>\r\n\t\t</li>\r\n\t\t<li>\r\n\t\t<p>Distribution of units in overall stations falling in Gandaki region</p>\r\n\t\t</li>\r\n\t\t<li>\r\n\t\t<p>Monthly comparison of gain or loss of units in each DCS of Gandaki region</p>\r\n\t\t</li>\r\n\t\t<li>\r\n\t\t<p>Units and Connections per year of abu Khaireni and Baglung DCS</p>\r\n\t\t</li>\r\n\t\t<li>\r\n\t\t<p>Categorical unit consumption divided upon type, range and category for the 2076/2077 fiscal year of Abu Khaireni and Baglung DCS.</p>\r\n\t\t</li>\r\n\t</ul>\r\n\t</li>\r\n</ol>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<ol start=\"2\">\r\n\t<li>\r\n\t<p><strong>NEA </strong><strong>Analysis:</strong></p>\r\n\r\n\t<ul>\r\n\t\t<li>\r\n\t\t<p><span style=\"color:#000000\">Connection </span></p>\r\n\r\n\t\t<ul>\r\n\t\t\t<li>\r\n\t\t\t<p><span style=\"color:#000000\">Growth by % </span></p>\r\n\t\t\t</li>\r\n\t\t\t<li>\r\n\t\t\t<p><span style=\"color:#000000\">Growth by Number </span></p>\r\n\t\t\t</li>\r\n\t\t\t<li>\r\n\t\t\t<p><span style=\"color:#000000\">Compare to National Growth/Area Growth </span></p>\r\n\t\t\t</li>\r\n\t\t</ul>\r\n\t\t</li>\r\n\t\t<li>\r\n\t\t<p><span style=\"color:#000000\">Unit Consumption </span></p>\r\n\r\n\t\t<ul>\r\n\t\t\t<li>\r\n\t\t\t<p><span style=\"color:#000000\">Growth by % </span></p>\r\n\t\t\t</li>\r\n\t\t\t<li>\r\n\t\t\t<p><span style=\"color:#000000\">Growth by Number </span></p>\r\n\t\t\t</li>\r\n\t\t\t<li>\r\n\t\t\t<p><span style=\"color:#000000\">Compare to National Growth/Area Growth </span></p>\r\n\t\t\t</li>\r\n\t\t</ul>\r\n\t\t</li>\r\n\t</ul>\r\n\t</li>\r\n</ol>\r\n\r\n<p style=\"margin-left:48px\">&nbsp;</p>\r\n\r\n<ol>\r\n\t<li>\r\n\t<p><span style=\"color:#000000\">ARPU Metrics (Area, DCS) </span></p>\r\n\r\n\t<ul>\r\n\t\t<li>\r\n\t\t<p><span style=\"color:#000000\">Per Connection </span></p>\r\n\t\t</li>\r\n\t\t<li>\r\n\t\t<p><span style=\"color:#000000\">Per Unit </span></p>\r\n\t\t</li>\r\n\t</ul>\r\n\t</li>\r\n</ol>\r\n\r\n<p style=\"margin-left:48px\">&nbsp;</p>\r\n\r\n<ol>\r\n\t<li>\r\n\t<p><span style=\"color:#000000\">Category Search </span></p>\r\n\r\n\t<ul>\r\n\t\t<li>\r\n\t\t<p>DOMESTIC(16-30AMPS)</p>\r\n\t\t</li>\r\n\t\t<li>\r\n\t\t<p><span style=\"background-color:#ffffff\">DOMESTIC(31-60A)</span></p>\r\n\t\t</li>\r\n\t\t<li>\r\n\t\t<p><span style=\"background-color:#ffffff\">DOMESTIC(5A) </span></p>\r\n\t\t</li>\r\n\t\t<li>\r\n\t\t<p><span style=\"background-color:#ffffff\">DOMESTIC(6-15A) </span></p>\r\n\t\t</li>\r\n\t\t<li>\r\n\t\t<p><span style=\"color:#000000\">Compare to National Growth/Area Growth</span></p>\r\n\t\t</li>\r\n\t</ul>\r\n\t</li>\r\n</ol>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<ol start=\"3\">\r\n\t<li>\r\n\t<p><strong>Analytics for Gandaki :</strong></p>\r\n\t</li>\r\n</ol>\r\n\r\n<p style=\"margin-left:48px\">The analysis is performed using only region 4 (Gandaki) of only domestic consumers except for national growth analysis which consists of all regions. The data comprises of fiscal year 2073/2074, 2074/2075, 2075/2076 . The analysis performed with this data are listed below:</p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<ol>\r\n\t<li>\r\n\t<p>Fastest Growing DCS (Top 3)</p>\r\n\r\n\t<ul>\r\n\t\t<li>\r\n\t\t<p>Connection</p>\r\n\t\t</li>\r\n\t\t<li>\r\n\t\t<p>Unit</p>\r\n\t\t</li>\r\n\t\t<li>\r\n\t\t<p>Revenue</p>\r\n\t\t</li>\r\n\t</ul>\r\n\t</li>\r\n</ol>\r\n\r\n<p style=\"margin-left:72px\">&nbsp;</p>\r\n\r\n<ol>\r\n\t<li>\r\n\t<p>Slowest Growing DCS (Bottom 3)</p>\r\n\r\n\t<ul>\r\n\t\t<li>\r\n\t\t<p>Connection</p>\r\n\t\t</li>\r\n\t\t<li>\r\n\t\t<p>Unit</p>\r\n\t\t</li>\r\n\t\t<li>\r\n\t\t<p>Revenue</p>\r\n\t\t</li>\r\n\t</ul>\r\n\t</li>\r\n</ol>\r\n\r\n<p style=\"margin-left:72px\">&nbsp;</p>\r\n\r\n<ol>\r\n\t<li>\r\n\t<p>National Growth</p>\r\n\r\n\t<ul>\r\n\t\t<li>\r\n\t\t<p>Connection</p>\r\n\t\t</li>\r\n\t\t<li>\r\n\t\t<p>Unit</p>\r\n\t\t</li>\r\n\t\t<li>\r\n\t\t<p>Revenue</p>\r\n\t\t</li>\r\n\t</ul>\r\n\t</li>\r\n</ol>\r\n\r\n<p style=\"margin-left:72px\">&nbsp;</p>\r\n\r\n<ol>\r\n\t<li>\r\n\t<p>Most Unit Consumption per Connection</p>\r\n\t</li>\r\n\t<li>\r\n\t<p>Highest Average Revenue per Connection</p>\r\n\t</li>\r\n\t<li>\r\n\t<p>Fastest Growing Highest Average Revenue per Connection</p>\r\n\t</li>\r\n</ol>",
      "date": "2020-05-01"
    }
  },
  {
    "model": "documents.createnewdocuments",
    "pk": 96,
    "fields": {
      "created_by": 3,
      "title": "2020-(05-04 to 05-15)",
      "content": "<p><strong>Works Done</strong></p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p><strong>Log Analysis &amp; Visualization</strong></p>\r\n\r\n<ul>\r\n\t<li style=\"list-style-type:disc\">\r\n\t<p><span style=\"background-color:transparent; color:#000000\">Crud operation was done for Kafka Stream. Made an api for the crud operation and integrated it with UI made by Faraz.</span></p>\r\n\t</li>\r\n\t<li style=\"list-style-type:disc\">\r\n\t<p><span style=\"background-color:transparent; color:#000000\">Api to run Ksql query and view the result in realtime was made, still need to show the result in UI.</span></p>\r\n\t</li>\r\n</ul>\r\n\r\n<p><strong><span style=\"background-color:transparent; color:#000000\">Grafana</span></strong></p>\r\n\r\n<p><span style=\"background-color:transparent; color:#000000\">&nbsp;&nbsp;&nbsp; </span><span style=\"background-color:transparent; color:#000000\">Deployed Grafana in development server.</span></p>\r\n\r\n<p><strong><span style=\"background-color:transparent; color:#000000\">Issue</span></strong></p>\r\n\r\n<p><span style=\"background-color:transparent; color:#000000\">&nbsp;&nbsp;&nbsp; </span><span style=\"background-color:transparent; color:#000000\">Login/Dashboard page kept loading and showed a blank screen. Some of the JS was not able to load properly. </span></p>\r\n\r\n<p><strong><span style=\"background-color:transparent; color:#000000\">Solution</span></strong></p>\r\n\r\n<ul>\r\n\t<li style=\"list-style-type:disc\">\r\n\t<p><span style=\"background-color:transparent; color:#000000\">Tried to use apache/nginx to deploy Grafana but was still not able to solve the issue.</span></p>\r\n\t</li>\r\n\t<li style=\"list-style-type:disc\">\r\n\t<p><span style=\"background-color:transparent; color:#000000\">Local port forward was working to load the Grafana login/dashboard page.</span></p>\r\n\t</li>\r\n</ul>\r\n\r\n<p><span style=\"background-color:transparent; color:#000000\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span><span style=\"color:#000000\"><span style=\"background-color:#dddddd\">ssh -fN -L 5000:localhost:3000 user@grafana-server-ip</span></span></p>\r\n\r\n<p><strong>Confluent JDBC Connector</strong><strong>&nbsp;&nbsp;&nbsp; </strong></p>\r\n\r\n<p><span style=\"background-color:transparent; color:#000000\">Modified Confluent JDBC Connector create and insert query.</span></p>\r\n\r\n<p><strong><span style=\"background-color:transparent; color:#000000\">Issue</span></strong></p>\r\n\r\n<p><strong>&nbsp;&nbsp;&nbsp; </strong><span style=\"background-color:transparent; color:#000000\">When a kafka stream is created the stream name and the columns are stored in Uppercase letters, but in postgres db it is saved in small case letters. If in postgres we tried to store all the columns including the table name in Uppercase then while inserting and selecting we have to provide quotes in all the columns and the table name, which would be difficult for the developers.</span></p>\r\n\r\n<p><strong><span style=\"background-color:transparent; color:#000000\">Solution</span></strong></p>\r\n\r\n<p><span style=\"background-color:transparent; color:#000000\">&nbsp;&nbsp;&nbsp; </span><span style=\"background-color:transparent; color:#000000\">Modify the Confluent JDBC sink connector. I modified the insert and create query builder in GenericDatabaseDialect class, built a new jar file and sent it to Tchiring.</span></p>",
      "date": "2020-05-25"
    }
  },
  {
    "model": "documents.createnewdocuments",
    "pk": 97,
    "fields": {
      "created_by": 3,
      "title": "2020-(05-18 to 05-29)",
      "content": "<p><strong>Works Done</strong></p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p><strong>Log Analysis &amp; Visualization</strong></p>\r\n\r\n<ul>\r\n\t<li style=\"list-style-type:disc\">\r\n\t<p><span style=\"background-color:transparent; color:#000000\">Crud operation was done for Kafka Table. Made an api for the crud operation and integrated it with the UI made by Faraz.</span></p>\r\n\t</li>\r\n\t<li style=\"list-style-type:disc\">\r\n\t<p><span style=\"background-color:transparent; color:#000000\">User wise visualization completed.</span></p>\r\n\t</li>\r\n</ul>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p><strong>Grafana</strong></p>\r\n\r\n<p><span style=\"background-color:transparent; color:#000000\">&nbsp;&nbsp;&nbsp; </span><span style=\"background-color:transparent; color:#000000\">Deployed Grafana in development server. Issue with dashboard loading is solved. </span></p>\r\n\r\n<p><span style=\"background-color:transparent; color:#000000\">Solution: Deployed Grafana in HTTPS.</span></p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p><strong>Bigmart</strong></p>\r\n\r\n<p><span style=\"background-color:transparent; color:#000000\">Prepared a dataset with user_id and lat/long of users&#39; latest connection from logs. Provided the result in csv, but due to security issue, I had to save the encrypted records in mongodb along with some extra fields i.e. lpcardNo and primary &amp; secondary store id, which was later discarded and now they want an API for user_id and lat/long of users&rsquo; latest connection with masked user_id.</span></p>\r\n\r\n<p><span style=\"background-color:transparent; color:#000000\">Sajita is working on how to send a masked user_id from Elasticsearch.</span></p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p><strong>Mobile Events</strong></p>\r\n\r\n<p><span style=\"background-color:transparent; color:#000000\">Mobile click events were sent to Elasticsearch using logstash and filebeat but due to heavy traffic we decided to use Kafka API in order to send the logs to Elasticsearch. Rest proxy of Kafka is being used right now and we are working on kafka security for sending the logs securely.</span></p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p><strong>Kafka Security</strong></p>\r\n\r\n<p><span style=\"background-color:transparent; color:#000000\">Kafka Security has three components:</span></p>\r\n\r\n<ul>\r\n\t<li style=\"list-style-type:disc\">\r\n\t<p><strong>Encryption of data in-flight using SSL / TLS:</strong><span style=\"background-color:transparent; color:#000000\"> This allows your data to be encrypted between your producers and Kafka and your consumers and Kafka. This is a very common pattern everyone has used when going on the web. That&rsquo;s the &ldquo;S&rdquo; of HTTPS (that beautiful green lock you see everywhere on the web).</span></p>\r\n\t</li>\r\n\t<li style=\"list-style-type:disc\">\r\n\t<p><strong>Authentication using SSL or SASL:</strong><span style=\"background-color:transparent; color:#000000\"> This allows your producers and your consumers to authenticate to your Kafka cluster, which verifies their identity. It&rsquo;s also a secure way to enable your clients to endorse an identity. Why would you want that? Well, for authorization!</span></p>\r\n\t</li>\r\n\t<li style=\"list-style-type:disc\">\r\n\t<p><strong>Authorization using ACLs:</strong><span style=\"background-color:transparent; color:#000000\"> Once your clients are authenticated, your Kafka brokers can run them against access control lists (ACL) to determine whether or not a particular client would be authorised to write or read to some topic.</span></p>\r\n\t</li>\r\n</ul>\r\n\r\n<p><span style=\"background-color:transparent; color:#000000\">We will be using SASL/SSL Security for Kafka.</span></p>\r\n\r\n<p><span style=\"background-color:transparent; color:#000000\">SSL is already enabled&nbsp; now we are working on SASL (Kerberos).</span></p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p><strong>Others</strong></p>\r\n\r\n<ul>\r\n\t<li style=\"list-style-type:disc\">\r\n\t<p><span style=\"background-color:transparent; color:#000000\">Webcam live stream (working with Faraz)</span></p>\r\n\t</li>\r\n\t<li style=\"list-style-type:disc\">\r\n\t<p><span style=\"background-color:transparent; color:#000000\">Different Encryption for different projects</span></p>\r\n\t</li>\r\n\t<li style=\"list-style-type:disc\">\r\n\t<p><span style=\"background-color:transparent; color:#000000\">Kafka/ES/Cassandra monitoring application</span></p>\r\n\t</li>\r\n\t<li style=\"list-style-type:disc\">\r\n\t<p><span style=\"background-color:transparent; color:#000000\">Grafana Dashboard for system/service logs</span></p>\r\n\t</li>\r\n</ul>",
      "date": "2020-06-01"
    }
  },
  {
    "model": "documents.createnewdocuments",
    "pk": 98,
    "fields": {
      "created_by": 16,
      "title": "May2_June_3_2020",
      "content": "<p><strong>WEEKLY&nbsp;REPORT</strong></p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p><strong>NEA</strong></p>\r\n\r\n<ul>\r\n\t<li>\r\n\t<p>Recalculate &ldquo;Top Performer DCS&rdquo; and &ldquo;Fast Growing DCS&rdquo; in Analytics for Gandaki.</p>\r\n\r\n\t<p>&nbsp;</p>\r\n\t</li>\r\n</ul>\r\n\r\n<p><strong>MOF</strong></p>\r\n\r\n<ul>\r\n\t<li>\r\n\t<p>Data Comprehension</p>\r\n\t</li>\r\n\t<li>\r\n\t<p>Data Wrangling</p>\r\n\t</li>\r\n\t<li>\r\n\t<p>Analysis:</p>\r\n\r\n\t<ul>\r\n\t\t<li>\r\n\t\t<p>Top export product</p>\r\n\t\t</li>\r\n\t\t<li>\r\n\t\t<p>Top import product</p>\r\n\t\t</li>\r\n\t\t<li>\r\n\t\t<p>Growth rate</p>\r\n\t\t</li>\r\n\t\t<li>\r\n\t\t<p>Percentage contribution</p>\r\n\t\t</li>\r\n\t\t<li>\r\n\t\t<p>Trade balance</p>\r\n\t\t</li>\r\n\t\t<li>\r\n\t\t<p>Export destinations</p>\r\n\t\t</li>\r\n\t\t<li>\r\n\t\t<p>Import origins</p>\r\n\t\t</li>\r\n\t</ul>\r\n\t</li>\r\n\t<li>\r\n\t<p>Index analysis results in Elasticsearch</p>\r\n\r\n\t<p>&nbsp;</p>\r\n\t</li>\r\n</ul>\r\n\r\n<p><strong>OEC</strong></p>\r\n\r\n<ul>\r\n\t<li>\r\n\t<p>Explore OEC site, API, methodology and D3plus</p>\r\n\t</li>\r\n\t<li>\r\n\t<p>Data Comprehension</p>\r\n\t</li>\r\n\t<li>\r\n\t<p>Interpretation of Product Network Graph presented in economic complexity of Nepal.</p>\r\n\r\n\t<p>&nbsp;</p>\r\n\t</li>\r\n</ul>\r\n\r\n<p><strong>Research and Study</strong></p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<ul>\r\n\t<li>\r\n\t<p>Neural Network</p>\r\n\r\n\t<ul>\r\n\t\t<li>\r\n\t\t<p>Review neural network and deep learning book.</p>\r\n\r\n\t\t<p>Reference:</p>\r\n\r\n\t\t<p><a href=\"http://neuralnetworksanddeeplearning.com/\">http://neuralnetworksanddeeplearning.com/</a></p>\r\n\r\n\t\t<p>&nbsp;</p>\r\n\t\t</li>\r\n\t</ul>\r\n\t</li>\r\n\t<li>\r\n\t<p>GraphX</p>\r\n\r\n\t<ul>\r\n\t\t<li>\r\n\t\t<p>Introduction to GraphX</p>\r\n\r\n\t\t<p>Reference:</p>\r\n\r\n\t\t<p><a href=\"https://www.packtpub.com/big-data-and-business-intelligence/apache-spark-graph-processing\">https://www.packtpub.com/big-data-and-business-intelligence/apache-spark-graph-processing</a></p>\r\n\t\t</li>\r\n\t</ul>\r\n\t</li>\r\n</ul>\r\n\r\n<p>&nbsp;</p>",
      "date": "2020-06-03"
    }
  },
  {
    "model": "documents.createnewdocuments",
    "pk": 99,
    "fields": {
      "created_by": 13,
      "title": "2020-06-03",
      "content": "<h1>Task 1 : Data Analysis Platform</h1>\r\n\r\n<p><strong>Descrption</strong>&nbsp;</p>\r\n\r\n<p>a) worked on planning different components for analysis</p>\r\n\r\n<p><strong>problem faced</strong> : Internet</p>\r\n\r\n<p><strong>time taken</strong> : still going</p>\r\n\r\n<h1>Task 2 : OEC data</h1>\r\n\r\n<p><strong>Description</strong></p>\r\n\r\n<p>cleaned and populated cassandra with OEC data</p>\r\n\r\n<p><strong>problem faced</strong> : unmanaged goverment&nbsp;</p>\r\n\r\n<p><strong>time taken</strong> : 5 days and still going</p>\r\n\r\n<h1>Task 3 : Security in Elasticsearch</h1>\r\n\r\n<p><strong>Description</strong></p>\r\n\r\n<p>Established SSL encryption in Elasticsearch on prodiction server</p>\r\n\r\n<p><strong>problem faced </strong>: Internet</p>\r\n\r\n<p><strong>time taken</strong> : 4 days</p>",
      "date": "2020-06-03"
    }
  },
  {
    "model": "documents.createnewdocuments",
    "pk": 100,
    "fields": {
      "created_by": 23,
      "title": "2020 - (04-05 to 02-06)",
      "content": "<p><strong>Twitter Data Sink (Mongo, Postgres, Kafka):</strong></p>\r\n\r\n<p>-&nbsp;Learn mongo query and aggregations.</p>\r\n\r\n<p>-&nbsp;Mongo Export and Import,&nbsp;Mongo Dump</p>\r\n\r\n<p>-&nbsp;Sink only unique users to mongodb</p>\r\n\r\n<p>- Decided to use PostgresDB, started to sink data in PostgresDB.</p>\r\n\r\n<p>-&nbsp;Create kafka topics with proper configurations.</p>\r\n\r\n<p>- Issues: While inserting kafka casts column name to uppercase and postgres table columns are in lowercase. Fixed by bibek dai.</p>\r\n\r\n<p>-&nbsp;Create Postgres tables and role</p>\r\n\r\n<p>-&nbsp;Translate Location Data Cleaning python code to Scala</p>\r\n\r\n<p>- Date was previously in string type, converted it to date time with UTC offset. Also changed existing data date type.</p>\r\n\r\n<p>-&nbsp;Coordinates needed to be changed to polygon datatype, was being wrapped in double quotes and inserted as string.</p>\r\n\r\n<p>-&nbsp;Learn about Postgres and Postgis, the different datatypes.</p>\r\n\r\n<p>-&nbsp;Learn about user defined functions and triggers in postgres.</p>\r\n\r\n<p>-&nbsp;Create a trigger function to cast datatype to polygon before inserting into table.</p>\r\n\r\n<p>-&nbsp;If tweets contain the key full text store full text instead of partial text.</p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p><strong>Elasticsearch:</strong></p>\r\n\r\n<p>-&nbsp;Migrate bigmart indexes to production elasticsearch cluster</p>\r\n\r\n<p>-&nbsp;Change default ElasticSearch Shard Distribution and Replication Factor</p>\r\n\r\n<p>Note: To change existing index&#39;s shard distibution, we need to create a new index with specific settings and re-index data.</p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p><strong>Cassandra Security:&nbsp;</strong></p>\r\n\r\n<p>- Table Creation</p>\r\n\r\n<p>-&nbsp;Revise TLS and SSL</p>\r\n\r\n<p>-&nbsp;Implement SSL and TLS security in casssandra Internode communication.</p>\r\n\r\n<p>- Learn about Wireshark</p>\r\n\r\n<p>-&nbsp;Use wireshark to sniff cassandra cql&nbsp;data packets.</p>\r\n\r\n<p>-&nbsp;Implement ssl and tls security in cassandra client server encryption. Unable to sniff data after implementing client server encryption.</p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p><strong>Other:&nbsp;</strong></p>\r\n\r\n<p>-&nbsp;Learn about Brute Force Attacks</p>\r\n\r\n<p>-&nbsp;Install Kali and use hydra&nbsp;to perform telnet and ssh attacks.</p>",
      "date": "2020-06-03"
    }
  },
  {
    "model": "documents.createnewdocuments",
    "pk": 101,
    "fields": {
      "created_by": 24,
      "title": "REPORT JUNE 2020",
      "content": "<h2><strong><span style=\"color:#e67e22\">JOURNEY- MENTAL HEALTH CHATBOT</span></strong></h2>\r\n\r\n<p><span style=\"color:#e74c3c\"><strong>1. Work Done</strong></span></p>\r\n\r\n<p><strong><span style=\"color:#e67e22\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span></strong>&nbsp; a. Meetings and dialogue flow creation and revision</p>\r\n\r\n<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; b. Completed a flow for generic chat</p>\r\n\r\n<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; c. Client collaboration for clearing up script and content</p>\r\n\r\n<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; d. Discussion and finalizing the server architecture and specifications for aws environment with journey team.</p>\r\n\r\n<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; e. Went through few youtube videos about sagemaker, boto and S3.</p>\r\n\r\n<p><span style=\"color:#c0392b\"><strong>2. To be Done</strong></span></p>\r\n\r\n<p><strong><span style=\"color:#e67e22\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span></strong>&nbsp;&nbsp; a. Implement the script in development level</p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<h2><span style=\"color:#3498db\"><strong>TWITTER ANALYSIS</strong></span></h2>\r\n\r\n<p><strong><span style=\"color:#c0392b\">1. Work Done</span></strong></p>\r\n\r\n<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; a. Tweaking APIs for top keywords, tweets, top negative, positive tweets for multiple clients.</p>\r\n\r\n<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; b. Changing top keywords to word clouds and scoring top tweets based on both word frequency and retweet count</p>\r\n\r\n<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; c. making toxic labels available on the API&#39;s response. (probability) How, obscene, threatening, insult.. is each tweet.</p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<h2><span style=\"color:#27ae60\">Summarization- Implementation</span></h2>\r\n\r\n<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 1. Pre-trained model for CNN/DAILY MAIL for extractive, extractive + abstractive, abstractive summarization.</p>\r\n\r\n<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 2. Result not that good.</p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<h2><strong><span style=\"color:#7f8c8d\">Missing Semester</span></strong></h2>\r\n\r\n<p>Youtube playlist: https://www.youtube.com/watch?v=Z56Jmr9Z34Q&amp;list=PLyzOVJj3bHQuloKGG59rS43e29ro7I57J</p>\r\n\r\n<p>Been writing about it. https://github.com/sarmilaupadhyaya/linux-system/blob/master/missing_semester.md</p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p>&nbsp;</p>",
      "date": "2020-06-30"
    }
  },
  {
    "model": "documents.createnewdocuments",
    "pk": 102,
    "fields": {
      "created_by": 13,
      "title": "2020-06-30",
      "content": "<h1>Task 1</h1>\r\n\r\n<h2>Documentation</h2>\r\n\r\n<p><strong>Description</strong> :</p>\r\n\r\n<p>created docs for all components for data analysis platform</p>\r\n\r\n<p><strong>Time taken</strong> : 1 month&nbsp;</p>\r\n\r\n<p><strong>Problem faced:</strong> None</p>\r\n\r\n<h1>Task2</h1>\r\n\r\n<h2>API management system</h2>\r\n\r\n<p><strong>Description</strong>:</p>\r\n\r\n<p>revamped already created system</p>\r\n\r\n<p><strong>Time taken</strong> : 2 week</p>\r\n\r\n<p><strong>Problem faced</strong>: None</p>\r\n\r\n<h1>Task 3</h1>\r\n\r\n<h2>Project Managemnt System</h2>\r\n\r\n<p><strong>Description</strong> :&nbsp;</p>\r\n\r\n<p>it is a central authentication/authorization system for managing components-project-user flow</p>\r\n\r\n<p><strong>Time taken </strong>: 2 days and still going</p>\r\n\r\n<p><strong>Problem faced</strong> : not yet</p>",
      "date": "2020-06-30"
    }
  },
  {
    "model": "documents.createnewdocuments",
    "pk": 103,
    "fields": {
      "created_by": 28,
      "title": "June 30, 2020",
      "content": "<p><strong>Weekly Report</strong></p>\r\n\r\n<p><span style=\"color:#000000\"><strong>Works done:</strong></span></p>\r\n\r\n<p><span style=\"color:#000000\">1)<strong> Studied About:</strong></span></p>\r\n\r\n<p><span style=\"color:#000000\">&nbsp; &nbsp; i) Field Masking in Elasticsearch:</span></p>\r\n\r\n<p><span style=\"color:#000000\">&nbsp; &nbsp; &nbsp; &nbsp; -&nbsp;&nbsp;replacing the field&rsquo;s value with a cryptographic hash</span></p>\r\n\r\n<p><span style=\"color:#000000\">I Studied and tried the following processes to perform this task:</span></p>\r\n\r\n<p><span style=\"color:#000000\"><strong>i) Scripting (Painless Script)</strong></span></p>\r\n\r\n<p><span style=\"color:#000000\">Issue: It was not able to fill our requirements.</span></p>\r\n\r\n<p><span style=\"color:#000000\"><strong>ii) Shield</strong></span></p>\r\n\r\n<p><span style=\"color:#000000\"><strong>-&nbsp;&nbsp;</strong>Securing data with Shield is possible at the index level by defining privileges for indices and aliases via Shield&#39;s&nbsp;role-based access control.&nbsp;</span></p>\r\n\r\n<p><span style=\"color:#000000\">issue: From&nbsp;Elasticsearch 5.0 and later, the shield features are migrated to x-pack security features and as we have already enabled SSL in production these features were included in those security features.</span></p>\r\n\r\n<p><span style=\"color:#000000\"><strong>iii) Cipher Filter Plugin</strong></span></p>\r\n\r\n<p><span style=\"color:#000000\">-&nbsp; This filter was able to parse a source and apply a cipher or decipher before storing it in the target.</span></p>\r\n\r\n<p><span style=\"color:#000000\">Issue: It contains one configuration known as iv_random_length. This will force the plugin to generate a unique random IV for each encryption call. Random iv&#39;s are better for security but here in our case we need to have the statically hardcoded IVs.</span></p>\r\n\r\n<p><span style=\"color:#000000\">So, Saurab&nbsp;dai suggested making our own plugin. He gave me an overview of this plugin.</span></p>\r\n\r\n<p><span style=\"color:#000000\"><strong>2) The plugin was made for encrypting and decrypting a specific field of index&nbsp;accordingly.</strong></span></p>\r\n\r\n<p><span style=\"color:#000000\">&nbsp;-&nbsp; iv was generated both statically and randomly. (bibek dai helped me here).</span></p>\r\n\r\n<p><span style=\"color:#000000\">There was still an issue as the source that we provided if it doesn&#39;t match the value in the index, the program was able to run smoothly in spite of throwing the issue. Later, it was solved.</span></p>\r\n\r\n<p><span style=\"color:#000000\"><strong>3)&nbsp; Data Cleaning</strong></span></p>\r\n\r\n<p><span style=\"color:#000000\">&nbsp; &nbsp; &nbsp;- Took data in CSV &nbsp;as input and added columns.</span></p>\r\n\r\n<p><span style=\"color:#000000\">&nbsp; &nbsp; &nbsp;- Took raw JSON data as input and later split them&nbsp;into columns.&nbsp;</span></p>\r\n\r\n<p><span style=\"color:#000000\">&nbsp; &nbsp; &nbsp;-&nbsp; Removed regular expressions from that data and also date format was changed.</span></p>\r\n\r\n<p><span style=\"color:#000000\"><strong>3) Studied Kafka Metrics (Monitoring Kafka )</strong></span></p>\r\n\r\n<p><span style=\"color:#000000\">&nbsp; -&nbsp; &nbsp; For monitoring, I configured JMX in the development server to be accessed remotely.&nbsp;</span></p>\r\n\r\n<p><span style=\"color:#000000\">&nbsp; -&nbsp; &nbsp; JConsole was used for remote monitoring.</span></p>\r\n\r\n<p><span style=\"color:#000000\">&nbsp; &nbsp;-&nbsp; &nbsp; List out the required metrics for kafka.</span></p>\r\n\r\n<p><span style=\"color:#000000\">&nbsp; &nbsp;-&nbsp; &nbsp; Fetch Metrics from JMX.&nbsp;</span></p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p>&nbsp;</p>",
      "date": "2020-06-30"
    }
  },
  {
    "model": "documents.createnewdocuments",
    "pk": 104,
    "fields": {
      "created_by": 16,
      "title": "June5_30_2020",
      "content": "<p><strong>Weekly Report</strong></p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p><strong>MOF/OEC</strong></p>\r\n\r\n<ul>\r\n\t<li>\r\n\t<p>Create elasticsearch queries for visualization.</p>\r\n\t</li>\r\n\t<li>\r\n\t<p>Collaborate with Faraz for visualization with chart js.</p>\r\n\t</li>\r\n</ul>\r\n\r\n<ul>\r\n\t<li>\r\n\t<p>Interpretation of Product Network Graph presented in the economic complexity of Nepal.</p>\r\n\t</li>\r\n\t<li>\r\n\t<p>Extract section, chapter and product relationship, and HScode from the web.</p>\r\n\t</li>\r\n\t<li>\r\n\t<p>Recreate analysis from additional &ldquo;section-chapter-product&rdquo; and &ldquo;country-continents&rdquo; (Thanks to Prajita) data.</p>\r\n\t</li>\r\n\t<li>\r\n\t<p>Reindex indices with additional information.</p>\r\n\t</li>\r\n\t<li>\r\n\t<p>Study trade indicators that portray the economic of a country.</p>\r\n\t</li>\r\n\t<li>\r\n\t<p>Meetings</p>\r\n\t</li>\r\n</ul>\r\n\r\n<p><strong>Reference</strong></p>\r\n\r\n<ul>\r\n\t<li>\r\n\t<p><a href=\"https://legacy.oec.world/en/profile/country/npl/\">https://legacy.oec.world/en/profile/country/npl/</a></p>\r\n\t</li>\r\n\t<li>\r\n\t<p><a href=\"https://asycuda.org/en/online-hs/\">https://asycuda.org/en/online-hs/</a></p>\r\n\t</li>\r\n\t<li>\r\n\t<p><a href=\"https://github.com/datasets/harmonized-system/tree/master/data\">https://github.com/datasets/harmonized-system/tree/master/data</a></p>\r\n\t</li>\r\n</ul>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p><strong>Kafka Django CMS</strong></p>\r\n\r\n<ul>\r\n\t<li>\r\n\t<p>Revise Django</p>\r\n\t</li>\r\n\t<li>\r\n\t<p>Create a JDBC source and sink connector platform</p>\r\n\t</li>\r\n</ul>\r\n\r\n<ul>\r\n\t<li>\r\n\t<p>Implement Javascript, Jquery, and AJAX to send an asynchronous requests to Kafka.</p>\r\n\r\n\t<p>(Thanks to Bibek)</p>\r\n\t</li>\r\n</ul>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p><strong>Elasticsearch</strong></p>\r\n\r\n<ul>\r\n\t<li>\r\n\t<p>Mentor Prajita</p>\r\n\t</li>\r\n\t<li>\r\n\t<p>Create basic elasticsearch and analysis tasks</p>\r\n\t</li>\r\n\t<li>\r\n\t<p>Verify and validate tasks</p>\r\n\t</li>\r\n</ul>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p><strong>D3.js</strong></p>\r\n\r\n<ul>\r\n\t<li>\r\n\t<p>Supervise Ashir</p>\r\n\t</li>\r\n\t<li>\r\n\t<p>Create tasks for visualization.</p>\r\n\t</li>\r\n\t<li>\r\n\t<p>Write elasticsearch queries for visualizations.</p>\r\n\t</li>\r\n</ul>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<ul>\r\n</ul>\r\n\r\n<p><strong>Research and Study</strong></p>\r\n\r\n<ul>\r\n\t<li>\r\n\t<p>Economic Complexity:</p>\r\n\r\n\t<ul>\r\n\t\t<li>\r\n\t\t<p><a href=\"http://www.cepii.fr/CEPII/en/bdd_modele/download.asp?id=37\">http://www.cepii.fr/CEPII/en/bdd_modele/download.asp?id=37</a></p>\r\n\t\t</li>\r\n\t\t<li>\r\n\t\t<p><a href=\"https://unctadstat.unctad.org/EN/RcaRadar.html\">https://unctadstat.unctad.org/EN/RcaRadar.html</a></p>\r\n\t\t</li>\r\n\t\t<li>\r\n\t\t<p><a href=\"https://wits.worldbank.org/wits/wits/witshelp/Content/Utilities/e1.trade_indicators.htm\">https://wits.worldbank.org/wits/wits/witshelp/Content/Utilities/e1.trade_indicators.htm</a></p>\r\n\t\t</li>\r\n\t\t<li>\r\n\t\t<p><a href=\"https://wits.worldbank.org/countrysnapshot/en/WLD\">https://wits.worldbank.org/countrysnapshot/en/WLD</a></p>\r\n\r\n\t\t<p>&nbsp;</p>\r\n\t\t</li>\r\n\t</ul>\r\n\t</li>\r\n\t<li>\r\n\t<p>EC paper review:</p>\r\n\r\n\t<ul>\r\n\t\t<li>\r\n\t\t<p><strong><a href=\"https://medium.com/@albertoarenaza/economic-complexity-analysis-and-predictions-for-automation-and-income-inequality-af31eaab42a6\">https://medium.com/@albertoarenaza/economic-complexity-analysis-and-predictions-for-automation-and-income-inequality-af31eaab42a6</a></strong></p>\r\n\t\t</li>\r\n\t</ul>\r\n\t</li>\r\n</ul>",
      "date": "2020-07-03"
    }
  },
  {
    "model": "documents.createnewdocuments",
    "pk": 105,
    "fields": {
      "created_by": 33,
      "title": "ProjectDesc",
      "content": "<p><strong>Problem definition:&nbsp;Scrape different Nepali news website with the given project requirement</strong></p>\r\n\r\n<p><strong>Time Spent:4 days.</strong></p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p><strong>Works Done:</strong></p>\r\n\r\n<p>1)Wrote Python code using request and Beautifulsoup and make it dynamic so that it can handle the same code in a different format.</p>\r\n\r\n<p>2)Scrape the title, date, and content of each news website</p>\r\n\r\n<p>3)Also scrape the links that are provided on the page.</p>\r\n\r\n<p>4)Handle pagination on the webpage.</p>\r\n\r\n<p>5) Made a data frame and CSV file in coordination with Ramesh Rawal.</p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p>1.&nbsp;<strong>The Problem Faced: </strong></p>\r\n\r\n<p>-Some of the news websites block the content and didn&#39;t give access to the content which was not possible to scrape but would have been possible if used Scrapy framework.</p>\r\n\r\n<p>-Some of the news websites don&#39;t store the archived contains but only store the recent news.</p>\r\n\r\n<p>--The News website of Nepal didn&#39;t have the search bar which didn&#39;t meet our project requirement.</p>\r\n\r\n<p>-Some of the news websites haven&#39;t the search results matched with the passed data in the search result.</p>\r\n\r\n<p>-The News Website of Nepal doesn&#39;t follow the strict HTML structure and the path to it is difficult to obtain. For example in some news website, the whole title, date published and the content are put in a single tag with difficulty in accessing a single required content as proposed in the requirement</p>\r\n\r\n<p>-There is a very small authentic news portal in Nepal. Most of the news portal copy-paste the authentic news portal also the name tends to seem to be authentic but the news and the content may not.</p>\r\n\r\n<p>-The news websites are too slow and the content with pagination was difficult to handle and validate.</p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p><strong>2.Solution</strong>:</p>\r\n\r\n<p>-Decided to obtain a news portal only from Kathmandu valley and decided to find the authentic news portal site only.</p>\r\n\r\n<p>-Decided to write script/python code and validating at the same time and updating the data in the spreadsheet.</p>\r\n\r\n<p>-Decided to use the news website that matches with the guidelines having a search bar, gives authentic news, popular news website along with those storing the archived content.</p>\r\n\r\n<p>-Also, different news websites have different ways of handling the pagination some of them threw an error, and exception handling technique was used.</p>\r\n\r\n<p>-For pagination handling code was written which was suited to the specific news website.</p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p><strong>3.Time Period.</strong></p>\r\n\r\n<p>-1 Week</p>\r\n\r\n<p>-My deadline: 7th July 2020</p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p><strong>4.Task Assigned-</strong></p>\r\n\r\n<p>-Python Problems given by Ramesh and Bibek dai.</p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p>&nbsp;</p>",
      "date": "2020-07-03"
    }
  },
  {
    "model": "documents.createnewdocuments",
    "pk": 106,
    "fields": {
      "created_by": 23,
      "title": "2020 - (03-06 to 02-07)",
      "content": "<h2><strong>Works&nbsp;Done:</strong></h2>\r\n\r\n<p><strong>Cassandra:&nbsp;</strong></p>\r\n\r\n<p><strong>a) Security:</strong></p>\r\n\r\n<p>-&nbsp;Cassandra Client Server Encryption from code. (Add option withSSL() while creating cluster object and pass the truststore, keystore and password as system properties)</p>\r\n\r\n<p>- Configure Cassandra Production Server Internode SSL and Client Server SSL.</p>\r\n\r\n<p><strong>b) Metrics and Monitoring Cluster</strong></p>\r\n\r\n<p>- Look into Cassandra metrics.&nbsp;Install jmxterm (Dumps the full list of JMX MBeans from a database node)</p>\r\n\r\n<p>-&nbsp;Check python driver for Cassandra and the extent of metrics provided by it. (In the end it was metrics for the driver and performace related to Cassandra)</p>\r\n\r\n<p>-&nbsp;Compare nodetool metrics and jmx metrics. (Better to use jmx than call nodetool through sub process)</p>\r\n\r\n<p>-&nbsp;Enable jmx to be accessed remotely. Use jconsole to observe and monitor monitor metrics.</p>\r\n\r\n<p>- Set jmx authentication.</p>\r\n\r\n<p>-&nbsp;Try jmxpython module (Provides a Python module to easily run queries and collect metrics from a Java Virtual Machine via JMX.)</p>\r\n\r\n<p>Issue: Unable to parse a certain payload from Cassandra&#39;s Mbean. (Help from bibek dai to edit the library)</p>\r\n\r\n<p>-&nbsp;List out the required metrics for cassandra</p>\r\n\r\n<p>- Fetch tables and keyspace from jmx</p>\r\n\r\n<p><strong>c) Start Cassandra Service</strong></p>\r\n\r\n<p>-&nbsp;Cassandra Start Script, Stop Script from any node where Cassandra is installed.</p>\r\n\r\n<p>First method: ssh into another server and run the service.</p>\r\n\r\n<p>Issue: To run service start command, sudo privleges were required. This was solved by prompting an interactive ssh but reading a file with the password but it is an unsafe and unsecure method, where the password maybe exposed.</p>\r\n\r\n<p>Second method: ssh into another server and run the binaries and track pids</p>\r\n\r\n<p>Issue: It takes some time for the cluster to start running hence process takes some time to acquire pids. Periodically check for pid and store it in a file.</p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p><strong>Twitter:</strong></p>\r\n\r\n<p>- Research what Twitter Premium Search API provides, the working and the limitations.</p>\r\n\r\n<p>-&nbsp;Create a new project to sink tweets by fetching historical data</p>\r\n\r\n<p>- Set Pagination&nbsp;and connection limit</p>\r\n\r\n<p>- Lookinto Account Activity API twitter. (Dipesh&nbsp;working on it)</p>\r\n\r\n<p>- Added keywords and userid&#39;s&nbsp;in real time tweets fetch api</p>\r\n\r\n<p>- Add country filter in tweets table.</p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p><strong>Other:</strong></p>\r\n\r\n<p>-&nbsp;Wireshark Kafka Check if packets show data after enabling TLS/SSL. (Not showing data after enabling security)</p>\r\n\r\n<p>-&nbsp;Create new db in hsqldb&nbsp;apiref and table according to data model provided by saurav dai. (Pending Cascade on delete)</p>\r\n\r\n<p>- QA Count API change ssl configs</p>",
      "date": "2020-07-03"
    }
  },
  {
    "model": "documents.createnewdocuments",
    "pk": 107,
    "fields": {
      "created_by": 28,
      "title": "August 7, 2020",
      "content": "<p><span style=\"color:#000000\"><strong>Works Done</strong></span></p>\r\n\r\n<p><span style=\"color:#000000\">1) JMX Authentication.</span></p>\r\n\r\n<p><span style=\"color:#000000\">2) Studied about REST API.</span></p>\r\n\r\n<p><span style=\"color:#000000\">3) API Development using Python and flask(&nbsp;PostgreSQL was used as a relational database).</span></p>\r\n\r\n<p><span style=\"color:#000000\">4) <strong>Data Analysis</strong></span></p>\r\n\r\n<p><span style=\"color:#000000\">i) Scraping&nbsp;table&nbsp;from pdf using Python</span></p>\r\n\r\n<p><span style=\"color:#000000\">-&nbsp; Used PyPDF2.</span></p>\r\n\r\n<p><span style=\"color:#000000\">Problem: it was not able to Scrape the whole table, only some portions were scraped.</span></p>\r\n\r\n<p><span style=\"color:#000000\">- Used tabula-py</span></p>\r\n\r\n<p><span style=\"color:#000000\">Problem: Same as PyPDF2</span></p>\r\n\r\n<p><span style=\"color:#000000\">- Used Camelot (Was able to scrape as per the requirement).</span></p>\r\n\r\n<p><span style=\"color:#000000\">ii) Scraped multiple tables and imported it in CSV format.</span></p>\r\n\r\n<p><span style=\"color:#000000\">iii) Loaded data in Spark data frames&nbsp;and performed some cleaning tasks.</span></p>\r\n\r\n<p><span style=\"color:#000000\">&nbsp;Faraz was working on Service&nbsp;Metrics of Kafka&nbsp;</span></p>\r\n\r\n<p><span style=\"color:#000000\">5) Research about how to obtain IP of a Kafka node that we are monitoring and implemented some method but none of them works.&nbsp;</span></p>\r\n\r\n<p><span style=\"color:#000000\">6) Created streams in Ksql Kafka.</span></p>\r\n\r\n<p><span style=\"color:#000000\">7) Worked in Mattermost Notification features with bibek dai. (one in the table&nbsp;and another one in box)</span></p>\r\n\r\n<p><span style=\"color:#000000\">8)<strong> Studied about Kafka Custom Connector</strong></span></p>\r\n\r\n<p><span style=\"color:#000000\">- First, I have tested the connector for a file and it worked.</span></p>\r\n\r\n<p><span style=\"color:#000000\">- Studied its workflow and tried to build a custom connector for Postgres as per the instructions provided by bibek dai.</span></p>\r\n\r\n<p><span style=\"color:#000000\">Issues: Class not found exception PostgreSQL.driver</span></p>\r\n\r\n<p><span style=\"color:#000000\">- Built separate test projects in Gradle to solve the issue (it Worked in the test project but not in the connector).&nbsp;</span></p>\r\n\r\n<p><span style=\"color:#000000\">- Bibek dai suggested converting Gradle Project into a maven and converted it. (ongoing)</span></p>\r\n\r\n<p>&nbsp;</p>",
      "date": "2020-08-07"
    }
  },
  {
    "model": "documents.createnewdocuments",
    "pk": 109,
    "fields": {
      "created_by": 13,
      "title": "2020-aug-7",
      "content": "<h1>Task 1</h1>\r\n\r\n<h1>Data Analysis Platform</h1>\r\n\r\n<p>a) service metrics</p>\r\n\r\n<p>b) project management system</p>\r\n\r\n<p>c) api management system</p>\r\n\r\n<p>d) visualization</p>\r\n\r\n<p>e) api middleware</p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p><strong>Time taken </strong>: 1 month</p>\r\n\r\n<p><strong>Problem faced</strong> : project planning ,internet,bijuli</p>",
      "date": "2020-08-07"
    }
  },
  {
    "model": "documents.createnewdocuments",
    "pk": 110,
    "fields": {
      "created_by": 35,
      "title": "content",
      "content": "<p><strong>27th July 2020 - 7th Aug 2020</strong></p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p><strong>1.&nbsp; The Problem Faced:</strong></p>\r\n\r\n<p><strong>SSH key&nbsp; - While accessing two different computers(virtual machines) with SSH key, only one computer could be accessed. Also, while uploading my java code in gitlab, the process was denied. Had to delete everything and do it again.</strong></p>\r\n\r\n<p><strong>Realized that the ip address was the same when I configured it using ifconfig command as it was cloned.&nbsp;</strong></p>\r\n\r\n<p><strong>Thus, I changed the ip of one of the virtual box(the cloned one) by going to the settings &rarr; network &rarr; bridged adapter &rarr;&nbsp; advanced &rarr; clicking on the refresh icon in the ip.Then the installation of ssh server by using the command - sudo apt-get install openssh-server was necessary.</strong></p>\r\n\r\n<p><strong>Learned about Data structures and algorithms in Java ,Java IO/NIO and elasticsearch theoretically but since I tend to forget about it, I think it is important for me to simultaneously practice it (learn from a practical approach) so that it will be easier for me to understand and remember it.&nbsp;</strong></p>\r\n\r\n<p><strong>&nbsp;</strong></p>\r\n\r\n<p><strong>2. Description of the approach you&rsquo;ve tried:</strong></p>\r\n\r\n<p><strong>&nbsp;Ifconfig(make sure the ip is different to one another) &rarr; sudo apt-get install openssh-server &rarr; sudo rm -r .ssh(delete any existing ssh key) &rarr; ssh-keygen -t rsa(generate ssh key) &rarr; ping the ip addresses to each other &rarr; ssh-copy-id user@ip-address (copy the ip address)</strong></p>\r\n\r\n<p><strong>Once completed, simply using the command &rarr;&nbsp; ssh user@ip-address will provide access to the server.&nbsp;</strong></p>\r\n\r\n<p><strong>For the gitlab access&nbsp;the use of SSH key was essential.&nbsp;</strong></p>\r\n\r\n<p><strong>We can access the public key using the command : cat .ssh/id_rsa.pub.</strong></p>\r\n\r\n<p><strong>Thus I pasted the public key in the gitlab section and while providing the git address to the java folder I had to make sure to provide the SSH address not the http address.&nbsp;&nbsp;</strong></p>\r\n\r\n<p><strong>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </strong></p>\r\n\r\n<p><strong>3. Time Table:</strong></p>\r\n\r\n<p><strong>&nbsp;Since it did not give the result it was supposed to, I immediately searched for its solution on the internet and asked my mentors&rsquo; advice from the team.&nbsp;</strong></p>\r\n\r\n<p><strong>I researched what I was supposed to do and how it is possible so it did not take much time to understand what was going on as the time taken to gather the prerequisite knowledge was more.</strong></p>\r\n\r\n<p><strong>Wednesday - 29th July 2020 took me around an hour&nbsp;to resolve the issue regarding SSH key.&nbsp;&nbsp;</strong></p>\r\n\r\n<p><strong>Tuesday- 4th August 2020 took me around half an hour to resolve the gitlab issue.</strong></p>\r\n\r\n<p><strong>&nbsp;</strong></p>\r\n\r\n<p><strong>4. Outcomes:</strong></p>\r\n\r\n<p><strong>I learned about SSH key, how it works, its pros and cons and how to implement it practically. Understanding the working mechanisms and the correct implementation of appropriate linux commands is the solution to figure out and solve problems. I was able to elicit the theorotical concept of data structure and algorithms in java, java io/nio and elasticsearch.</strong></p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p><strong>5. Theoretical Complexities:</strong></p>\r\n\r\n<p><strong>Understanding encryption, decryption, asymmetric cipher, public/private key was important in understanding the working mechanism of ssh keys and how it is a secure shell that prevents any mid-intrusion.</strong></p>\r\n\r\n<p><strong>&nbsp;</strong></p>\r\n\r\n<p><strong>&nbsp;6. Technical Complexities: &nbsp;</strong></p>\r\n\r\n<p><strong>Learning to implement proper linux commands required meticulousness as even a minor mistake would not let the command to operate what it was meant to and can cause loss of important documents. Also I shall give time to learn Java IO/NIO practically.&nbsp;</strong></p>\r\n\r\n<p><strong>&nbsp;</strong></p>\r\n\r\n<p><strong>7. Remarks:</strong></p>\r\n\r\n<p><strong>Learning approach has so far been fruitful. The mentors of my team have been supportive and guiding me through my queries which has indeed boosted my learning energy.</strong></p>\r\n\r\n<p><strong>&nbsp;</strong></p>\r\n\r\n<p><strong>&nbsp;</strong></p>\r\n\r\n<p><br />\r\n&nbsp;</p>",
      "date": "2020-08-07"
    }
  },
  {
    "model": "documents.createnewdocuments",
    "pk": 111,
    "fields": {
      "created_by": 16,
      "title": "July1_31_2020",
      "content": "<h2><strong>Weekly Report</strong></h2>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p><strong>Kafka Django CMS</strong></p>\r\n\r\n<ul>\r\n\t<li>\r\n\t<p>Add connectors to Kafka platform:</p>\r\n\r\n\t<ul>\r\n\t\t<li>\r\n\t\t<p>Elasticsearch sink connector</p>\r\n\t\t</li>\r\n\t\t<li>\r\n\t\t<p>Mongo sink connector</p>\r\n\t\t</li>\r\n\t\t<li>\r\n\t\t<p>Rest source connector</p>\r\n\t\t</li>\r\n\t\t<li>\r\n\t\t<p>Cassandra sink connector</p>\r\n\t\t</li>\r\n\t</ul>\r\n\t</li>\r\n\t<li>\r\n\t<p>Write JQUERY and AJAX to get data and call APIs.</p>\r\n\t</li>\r\n\t<li>\r\n\t<p>Create custom template tags in Django to manipulate data fetch from AJAX calls and display in UI.</p>\r\n\t</li>\r\n</ul>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p><strong>Kafka connect:</strong></p>\r\n\r\n<ul>\r\n\t<li>\r\n\t<p>Implement custom query connectors</p>\r\n\t</li>\r\n\t<li>\r\n\t<p>Implement key in each message of Kafka</p>\r\n\t</li>\r\n\t<li>\r\n\t<p>Produce and consume both AVRO and JSON data in Kafka.</p>\r\n\t</li>\r\n</ul>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p><strong>KSQL</strong></p>\r\n\r\n<ul>\r\n\t<li>\r\n\t<ul>\r\n\t\t<li>\r\n\t\t<p>Create a table and stream using KSQL commands.</p>\r\n\t\t</li>\r\n\t\t<li>\r\n\t\t<p>Join the stream and table in KSQL.</p>\r\n\t\t</li>\r\n\t\t<li>\r\n\t\t<p>Fetch data from stream and table into the database.</p>\r\n\t\t</li>\r\n\t\t<li>\r\n\t\t<p>Extract nested JSON from a stream.</p>\r\n\t\t</li>\r\n\t</ul>\r\n\t</li>\r\n</ul>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p><strong>Research and Study</strong></p>\r\n\r\n<ul>\r\n\t<li>Study Kafka architecture and partition mechanism.</li>\r\n\t<li>Research on different keys used in Kafka connector and their importance.</li>\r\n\t<li>Revise KSQL commands with stream and table.</li>\r\n\t<li>Research on world trade dataset where products are categorized with Hscode.</li>\r\n</ul>",
      "date": "2020-08-10"
    }
  },
  {
    "model": "documents.createnewdocuments",
    "pk": 113,
    "fields": {
      "created_by": 24,
      "title": "REPORT JULY",
      "content": "<p><samp><span style=\"background-color:transparent; color:#000000\">JOURNEY</span></samp></p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<ol>\r\n\t<li style=\"list-style-type:decimal\">\r\n\t<p><samp><span style=\"background-color:transparent; color:#000000\">SCRIPT DATABASE CREATION</span></samp></p>\r\n\t</li>\r\n</ol>\r\n\r\n<p style=\"margin-left:36pt\"><samp><span style=\"background-color:transparent; color:#000000\">State tracking and memorization for each reply bot give was implemented as a row and columns.</span></samp></p>\r\n\r\n<ol start=\"2\">\r\n\t<li style=\"list-style-type:decimal\">\r\n\t<p><samp><span style=\"background-color:transparent; color:#000000\">INITIALIZATION OF PROJECT</span></samp></p>\r\n\t</li>\r\n</ol>\r\n\r\n<p style=\"margin-left:36pt\"><samp><span style=\"background-color:transparent; color:#000000\">Setting project up in the layout of nbot.</span></samp></p>\r\n\r\n<ol start=\"3\">\r\n\t<li style=\"list-style-type:decimal\">\r\n\t<p><samp><span style=\"background-color:transparent; color:#000000\">SETTING UP BOT WITH MOBILE THROUGH SOCKET AND NODE.</span></samp></p>\r\n\t</li>\r\n</ol>\r\n\r\n<p style=\"margin-left:36pt\"><samp><span style=\"background-color:transparent; color:#000000\">Talked with prajwal dai, bibek dai and rikee dai to debug the connection. Request from mobile to socket was not being sent to bot side. Rikee dai solved that error.</span></samp></p>\r\n\r\n<ol start=\"4\">\r\n\t<li style=\"list-style-type:decimal\">\r\n\t<p><samp><span style=\"background-color:transparent; color:#000000\">CREATED ALGORITHM FLOW</span></samp></p>\r\n\t</li>\r\n</ol>\r\n\r\n<p style=\"margin-left:36pt\"><samp><span style=\"background-color:transparent; color:#000000\">Algorithm to extract appropriate response.</span></samp></p>\r\n\r\n<ol start=\"5\">\r\n\t<li style=\"list-style-type:decimal\">\r\n\t<p><samp><span style=\"background-color:transparent; color:#000000\">EXPLAINED IT TO DIPESH AND PEMA</span></samp></p>\r\n\t</li>\r\n</ol>\r\n\r\n<p style=\"margin-left:36pt\"><samp><span style=\"background-color:transparent; color:#000000\">Explained the project and codes to pema and dipesh.</span></samp></p>\r\n\r\n<ol start=\"6\">\r\n\t<li style=\"list-style-type:decimal\">\r\n\t<p><samp><span style=\"background-color:transparent; color:#000000\">TASK DIVISION</span></samp></p>\r\n\t</li>\r\n\t<li style=\"list-style-type:decimal\">\r\n\t<p><samp><span style=\"background-color:transparent; color:#000000\">TEST SUIT AND TEST CASES CREATION</span></samp></p>\r\n\t</li>\r\n</ol>\r\n\r\n<p style=\"margin-left:36pt\"><samp><span style=\"background-color:transparent; color:#000000\">Created expected input and output for modules.</span></samp></p>\r\n\r\n<ol start=\"8\">\r\n\t<li style=\"list-style-type:decimal\">\r\n\t<p><samp><span style=\"background-color:transparent; color:#000000\">UNIT TESTING</span></samp></p>\r\n\t</li>\r\n</ol>\r\n\r\n<p style=\"margin-left:36pt\"><samp><span style=\"background-color:transparent; color:#000000\">Implemented pytest and unittest and successfully.</span></samp></p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<ol start=\"9\">\r\n\t<li style=\"list-style-type:decimal\">\r\n\t<p><samp><span style=\"background-color:transparent; color:#000000\">DISCUSSION WITH MOBILE TEAM REGARDING JSON STRUCTURE FOR COMMUNICATION.</span></samp></p>\r\n\t</li>\r\n</ol>\r\n\r\n<p style=\"margin-left:36pt\"><samp><span style=\"background-color:transparent; color:#000000\">Created documentation for response elements and made few changes in order to implement button response, text response and multiple button response.</span></samp></p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<ol start=\"10\">\r\n\t<li style=\"list-style-type:decimal\">\r\n\t<p><samp><span style=\"background-color:transparent; color:#000000\">FIRST UAT RELEASE COMPLETE.</span></samp></p>\r\n\t</li>\r\n\t<li style=\"list-style-type:decimal\">\r\n\t<p><samp><span style=\"background-color:transparent; color:#000000\">IMPLEMENTED SENTIMENT ANALYSIS AND TOXICITY OF USER&rsquo;S RESPONSE</span></samp></p>\r\n\t</li>\r\n</ol>\r\n\r\n<p><br />\r\n&nbsp;</p>\r\n\r\n<p><samp><span style=\"background-color:transparent; color:#000000\">TWITTER ANALYSIS</span></samp></p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<ol>\r\n\t<li style=\"list-style-type:decimal\">\r\n\t<p><samp><span style=\"background-color:transparent; color:#000000\">KEYPHRASE EXTRACTION FOR NEPALI TWEETS USING RAKE, PKE, AND CHUNK GRAM</span></samp></p>\r\n\t</li>\r\n\t<li style=\"list-style-type:decimal\">\r\n\t<p><samp><span style=\"background-color:transparent; color:#000000\">optimizing code to calculate retweet count of each root tweet.</span></samp></p>\r\n\t</li>\r\n\t<li style=\"list-style-type:decimal\">\r\n\t<p><samp>talked to faraz for few changes and merging of apis.</samp></p>\r\n\t</li>\r\n</ol>",
      "date": "2020-08-12"
    }
  },
  {
    "model": "documents.createnewdocuments",
    "pk": 114,
    "fields": {
      "created_by": 23,
      "title": "2020 - (03-07 to 07-08)",
      "content": "<h2><strong>Works&nbsp;Done:</strong></h2>\r\n\r\n<p><strong>Kafka:&nbsp;</strong></p>\r\n\r\n<p><strong>Log Notification:</strong></p>\r\n\r\n<p>-&nbsp;Create Kafka Consumer for Log Notification and Send notification using WebHook (71 server).&nbsp;</p>\r\n\r\n<p>- KSQL: Create streams and send multiple streams to one topic and that topic would be subscribed and for log notificaiton.</p>\r\n\r\n<p>-&nbsp;Kafka Log Notification fetch userdetails from api provided and send notification.</p>\r\n\r\n<p>-&nbsp;Demo with Bibek dai and Gaurav Sir</p>\r\n\r\n<p>-&nbsp;Change notification to tabular format</p>\r\n\r\n<p>-&nbsp;Move log streaming into kafka to production server with Bibek daju (SSL enabled production)</p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p><strong>Kafka-connect-rest-plugin (Bigmart):</strong></p>\r\n\r\n<p>-<strong>&nbsp;</strong>Install Kafka Confluent in Local</p>\r\n\r\n<p>-&nbsp;Look into Kafka-connect-rest project</p>\r\n\r\n<p>- Modifications to scehma, lpcardno partition by 500</p>\r\n\r\n<p>Issue: Database usage (db took too much time to fetch records)</p>\r\n\r\n<p>-&nbsp;Try and replicate 1mb problem.&nbsp;Initially read from&nbsp;json&nbsp;file, then tried api with encrypted data. (Unable to replicate issue)</p>\r\n\r\n<p>- Started taking&nbsp;Database-data-Exchange Project through&nbsp;Kafka connect rest. Tested 1mb issue again using sales enpoint.</p>\r\n\r\n<p>Issue:&nbsp;Schema issue, default null values</p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p><strong>Kafka Cassadra Sink Connector:</strong></p>\r\n\r\n<p>- Confluent provided a sink connector but it was only for 30-day trial.</p>\r\n\r\n<p>- Looked into connector Provided by Datastax</p>\r\n\r\n<p>-&nbsp;Kafka-cassandra-sink topic-table mapping (topic ko keys and table ko columns mapping) needs to be provided? (Yes mandatory)</p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p><strong>Elasticsearch, Logstash:</strong></p>\r\n\r\n<p>-&nbsp;ES reindex logstash data (Reason: Indexes were created back in the days and their shards were set as only 1. So to increase shards, had to reindex)</p>\r\n\r\n<p>- Logtash changes to espo.conf. (Had to communicate with Binaya but sys admins were busy&nbsp;with server migrations and were not properly communicating. Later bibek dai took over.)</p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p><strong>WireShark:</strong></p>\r\n\r\n<p>-&nbsp;Check if Request Body can be seen in packets of data using wireshark</p>\r\n\r\n<p>- Tested by deploying an app in 32 server and accessing using wireshark</p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p><strong>Cassandra:&nbsp;</strong></p>\r\n\r\n<p>-&nbsp;Cassandra Keyspace Metrics</p>\r\n\r\n<p>- Error and Overruns Metrics</p>\r\n\r\n<p>- Look into Cassandra-Stress tool (already pre-installed with cassandra out of the box)</p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p><strong>Other:</strong></p>\r\n\r\n<p>-<strong>&nbsp;</strong>Create a API template&nbsp;that can connect to&nbsp;db&#39;s based on different endpoints and fetch data. DB&#39;s inlcuded: Cassandra, MongoDB, any JDBC compatible db</p>\r\n\r\n<p>Issue: Spring Auto Configuration was trying to connect to localhost Mongo</p>\r\n\r\n<p>-&nbsp;Setup mongo in 32 server</p>\r\n\r\n<p>-&nbsp;Study about Hive</p>\r\n\r\n<p>-&nbsp;Install Hadoop and&nbsp;Hive in local</p>\r\n\r\n<p>Issue:&nbsp;Version Compatibilty issues, installed the correct version and it worked.</p>",
      "date": "2020-08-12"
    }
  },
  {
    "model": "documents.createnewdocuments",
    "pk": 117,
    "fields": {
      "created_by": 34,
      "title": "Report1",
      "content": "<p>report1</p>",
      "date": "2020-08-23"
    }
  },
  {
    "model": "documents.createnewdocuments",
    "pk": 118,
    "fields": {
      "created_by": 13,
      "title": "2020-09-18",
      "content": "<h1>Task 1</h1>\r\n\r\n<h2><strong>Api Management System</strong></h2>\r\n\r\n<p><strong>Description</strong></p>\r\n\r\n<p>A system to limit api request and keep track of it&#39;s usage</p>\r\n\r\n<p><strong>Problem Faced </strong>: Not much</p>\r\n\r\n<p><strong>Time Taken</strong> : 1 month&nbsp;</p>\r\n\r\n<h1>Task 2</h1>\r\n\r\n<h2>Visualization dashboard and SDK</h2>\r\n\r\n<p><strong>Description</strong></p>\r\n\r\n<p>Dashboard to create charts from ES data</p>\r\n\r\n<p><strong>Problem faced</strong> : Not much</p>\r\n\r\n<p><strong>Time taken</strong> : 1 month</p>\r\n\r\n<h1>Task 3</h1>\r\n\r\n<h2>API Authentication System</h2>\r\n\r\n<p><strong>Description&nbsp;</strong></p>\r\n\r\n<p>System to validate api request</p>\r\n\r\n<p><strong>Problem Faced</strong> : Choosing right packages and understanding nginx phases</p>\r\n\r\n<p>Time Taken : 1 month</p>",
      "date": "2020-09-18"
    }
  },
  {
    "model": "documents.createnewdocuments",
    "pk": 120,
    "fields": {
      "created_by": 35,
      "title": "Aug/Sep Report",
      "content": "<p><strong>8th Aug - 14th Sep</strong></p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p><strong>Elastic search</strong></p>\r\n\r\n<p>Created index, shards and inserted data.</p>\r\n\r\n<p>Defined mappings and settings of index in kibana.</p>\r\n\r\n<p>Queries to access, filter and search.</p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p><strong>Aggregations</strong></p>\r\n\r\n<p>Learned the different types of aggregations<strong>&nbsp;</strong></p>\r\n\r\n<p>Implemented Nested aggregation, Bucket aggregation, Metric aggregation, Term aggregation.</p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p><strong>Elastic search task&nbsp;</strong></p>\r\n\r\n<p>Accessed the json file/data of oec_export_destinations index from&nbsp;development server es.</p>\r\n\r\n<p>Host and port were accessed through curl requests of the index from kibana and connected in the jupyter notebook.&nbsp;</p>\r\n\r\n<p>Parsed json file using pandas/python.</p>\r\n\r\n<p>-Listed the total unique countries presented in each continent&nbsp;using nested aggregation.</p>\r\n\r\n<p>-Listed the top 10 contributing countries in total export from each continent for the year 2074/75.&nbsp;</p>\r\n\r\n<p>Explored data using pandas.</p>\r\n\r\n<p>- Created a bar chart showing the percent&nbsp;contribution of each country present in the continent &#39;Asia&#39; using pandas plot.</p>\r\n\r\n<p>- Converted the result into csv format.</p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p><strong>Learned about the architecture of the nodes in es.</strong></p>\r\n\r\n<p>- Master node, Data node, Transform node, Ingest node, Coordinating node, Machine learning node</p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p><strong>Scripting</strong></p>\r\n\r\n<p>Parameters can be used to prevent recompilation.</p>\r\n\r\n<p>Prevents circuit breaking exception error that might occur as compilation of max 75 scripts can be done in 5 minutes for most contexts.</p>\r\n\r\n<p>Except for ingest contexts, the default compilation rate is unlimited. Although the setting can <strong>be changed dynamically.</strong></p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p><strong>Parsed json file using elastic search and kibana.</strong></p>\r\n\r\n<p>Used rp_full_analysis index from development server and parse ipDetail field.</p>\r\n\r\n<p>Used ingest nodes and reindexing.</p>\r\n\r\n<p>Learned about the different processors in pipelines that can be used.</p>\r\n\r\n<p>Used a json processor and gave the field name ipDetail.</p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p><strong>Pipeline Implementation</strong></p>\r\n\r\n<p>Simulate</p>\r\n\r\n<p>Create pipeline</p>\r\n\r\n<p>Define and add processors as per the requirement.</p>\r\n\r\n<p>Enrich data by making specific changes in the processor of the incoming documents. After all the processors are done in the pipeline, the finished document is added to the target index.</p>\r\n\r\n<p>Incoming documents ---&gt; ingest pipeline ---&gt; target index</p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p><strong>Pipeline processors&nbsp;</strong></p>\r\n\r\n<p>Accessing data in pipelines using set,field,value statement.</p>\r\n\r\n<p>Implemented concatenation, append and split processors.</p>\r\n\r\n<p>Conditional execution using if.</p>\r\n\r\n<p>Handling Failures in pipeline using on_failure statement and ignore_failure statement.</p>\r\n\r\n<p>To retrieve the actual error message, metadata fields such as on_failure_message, on_failure_processor_type, on_failure_pipeline can be accessed within the on_failure block.</p>\r\n\r\n<p>&nbsp;</p>",
      "date": "2020-09-22"
    }
  },
  {
    "model": "documents.createnewdocuments",
    "pk": 121,
    "fields": {
      "created_by": 23,
      "title": "2020 - (10-08 to 30-08)",
      "content": "<h2><strong>Works&nbsp;Done:&nbsp;</strong></h2>\r\n\r\n<p><strong>Database-data-exchange:</strong></p>\r\n\r\n<p><strong>Kafka:</strong></p>\r\n\r\n<p>- Some time spent in understanding the endpoints again.</p>\r\n\r\n<p>- Rewrite project code to implement and use kafka as&nbsp;a message broker to sink data to cassandra and elasticsearch.</p>\r\n\r\n<p>- Added a new column savings. (lt + td + pa)</p>\r\n\r\n<p>Issue | Fix :</p>\r\n\r\n<p>(Multiple Endpoints Polling)</p>\r\n\r\n<p>Elasticseach:</p>\r\n\r\n<p>i) Date Parse Exceptions | Provided an acceptable mapping format while creating index for date field&nbsp;&nbsp;</p>\r\n\r\n<p>ii) Data Sink Speed (connector issue) | *No fix yet&nbsp;</p>\r\n\r\n<p>iii) ES no space (Data direc&nbsp;pointed at root direc)&nbsp;| Changed direc to mount point and restarted cluster</p>\r\n\r\n<p>Bigmart-server-api:</p>\r\n\r\n<p>i) Data count mismatch (anamoly)</p>\r\n\r\n<p>Cassandra-connector:</p>\r\n\r\n<p>i) removing and creating&nbsp;the sink&nbsp;connector again&nbsp;did not reset the topic offset</p>\r\n\r\n<p>[ Due to time constraint, botched the work and implemented chages in existing working project with Bibek dai. ]</p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p><strong>Bill-Tagging:</strong></p>\r\n\r\n<p>- Some bills have missing lpcardno&#39;s (unique customer code) due to which their purchases are missed and&nbsp;not dispalyed in the app.</p>\r\n\r\n<p>- Created an api to tag lpcardno to bills and sink missed purchase record to mongo.</p>\r\n\r\n<p>- Look into ES update by query. Updates the results returned by a query in a single request.</p>\r\n\r\n<p>- Added new column tagged.</p>\r\n\r\n<p>- Logging bill-tagged.</p>\r\n\r\n<p>Issue | Fix:</p>\r\n\r\n<p>i) Mutiple bills with same billno | Added sitecode to distinguish between them.&nbsp;</p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p><strong>Cassandra:&nbsp;</strong></p>\r\n\r\n<p>- Look into Cassandra Stress Tool</p>\r\n\r\n<p>-&nbsp;Read Cassandra Data Modeling White Paper</p>\r\n\r\n<p>( https://office.ekbana.info/products/files/doceditor.aspx?fileid=4936&amp;doc=UTNkTkR6NzdqYzBZQmw5OXpDQjVKaTFuNU5JRXpnRU5yd2lTK1ZiN2tmMD0_IjQ5MzYi0 )</p>",
      "date": "2020-09-23"
    }
  }
]